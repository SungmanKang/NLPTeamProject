{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69f71aba",
   "metadata": {},
   "source": [
    "(add folders in training for context and form"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1661a549",
   "metadata": {},
   "source": [
    "## Preprocess data (only run once)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "36121bd0",
   "metadata": {},
   "source": [
    "%run fcm/preprocess.py train --input ./fcm/brown/brown.txt --output ./training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf8a07b",
   "metadata": {},
   "source": [
    "## Train the bertram context-only model (DeBERTa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7dd6b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-17 00:48:11,787 - INFO - train - Initializing new BERTRAM instance from microsoft/deberta-base.\n",
      "2022-11-17 00:48:12,090 - INFO - ngram_models - Found 712 ngrams with min count 4 and (nmin,nmax)=(3,5), first 10: ['UNK', 'PAD', 'ed<S>', 'ng<S>', 'ing', 'er<S>', 'ing<S>', 'on<S>', '<S>co', 'ion'], last 10: ['tly', 'tly<S>', 'cor', 'anc', 'ance', 'ance<S>', '<S>des', 'des', 'sio', 'sion']\n",
      "2022-11-17 00:48:12,091 - INFO - utils - Loading embeddings from ./fcm/wordEmbeddings/glove.6B.50d.txt\n",
      "2022-11-17 00:49:38,293 - INFO - utils - Done loading embeddings\n",
      "Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing BertramForDeBERTa: ['lm_predictions.lm_head.dense.bias', 'deberta.embeddings.position_embeddings.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertramForDeBERTa from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertramForDeBERTa from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertramForDeBERTa were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['linear.bias', 'reliability_measure.linear.bias', 'reliability_measure.linear.weight', 'linear.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch:   0%|          | 0/5 [00:00<?, ?it/s]2022-11-17 00:49:40,012 - INFO - input_processor - Processing training file 1 of 25: ./training/train.bucket16.txt\n",
      "2022-11-17 00:49:40,031 - INFO - input_processor - Done processing training file, batch size is 78\n",
      "2022-11-17 00:49:40,031 - INFO - input_processor - Processing training file 2 of 25: ./training/train.bucket12.txt\n",
      "2022-11-17 00:49:40,056 - INFO - input_processor - Done processing training file, batch size is 185\n",
      "2022-11-17 00:49:40,056 - INFO - input_processor - Processing training file 3 of 25: ./training/train.bucket9.txt\n",
      "2022-11-17 00:49:40,079 - INFO - input_processor - Done processing training file, batch size is 277\n",
      "2022-11-17 00:49:40,079 - INFO - input_processor - Processing training file 4 of 25: ./training/train.bucket19.txt\n",
      "2022-11-17 00:49:40,100 - INFO - input_processor - Done processing training file, batch size is 366\n",
      "2022-11-17 00:49:40,100 - INFO - input_processor - Processing training file 5 of 25: ./training/train.bucket18.txt\n",
      "2022-11-17 00:49:40,122 - INFO - input_processor - Done processing training file, batch size is 456\n",
      "2022-11-17 00:49:40,123 - INFO - input_processor - Processing training file 6 of 25: ./training/train.bucket6.txt\n",
      "2022-11-17 00:49:40,145 - INFO - input_processor - Done processing training file, batch size is 551\n",
      "2022-11-17 00:49:40,146 - INFO - input_processor - Processing training file 7 of 25: ./training/train.bucket5.txt\n",
      "2022-11-17 00:49:40,171 - INFO - input_processor - Done processing training file, batch size is 648\n",
      "2022-11-17 00:49:40,171 - INFO - input_processor - Processing training file 8 of 25: ./training/train.bucket10.txt\n",
      "2022-11-17 00:49:40,197 - INFO - input_processor - Done processing training file, batch size is 750\n",
      "2022-11-17 00:49:40,197 - INFO - input_processor - Processing training file 9 of 25: ./training/train.bucket15.txt\n",
      "2022-11-17 00:49:40,215 - INFO - input_processor - Done processing training file, batch size is 824\n",
      "2022-11-17 00:49:40,216 - INFO - input_processor - Processing training file 10 of 25: ./training/train.bucket21.txt\n",
      "2022-11-17 00:49:40,241 - INFO - input_processor - Done processing training file, batch size is 930\n",
      "2022-11-17 00:49:40,242 - INFO - input_processor - Processing training file 11 of 25: ./training/train.bucket11.txt\n",
      "2022-11-17 00:49:40,260 - INFO - input_processor - Done processing training file, batch size is 1008\n",
      "2022-11-17 00:49:40,261 - INFO - input_processor - Processing training file 12 of 25: ./training/train.bucket17.txt\n",
      "2022-11-17 00:49:40,278 - INFO - input_processor - Done processing training file, batch size is 1080\n",
      "2022-11-17 00:49:40,279 - INFO - input_processor - Processing training file 13 of 25: ./training/train.bucket1.txt\n",
      "2022-11-17 00:49:40,308 - INFO - input_processor - Done processing training file, batch size is 1193\n",
      "2022-11-17 00:49:40,309 - INFO - input_processor - Processing training file 14 of 25: ./training/train.bucket14.txt\n",
      "2022-11-17 00:49:40,334 - INFO - input_processor - Done processing training file, batch size is 1285\n",
      "2022-11-17 00:49:40,334 - INFO - input_processor - Processing training file 15 of 25: ./training/train.bucket22.txt\n",
      "2022-11-17 00:49:40,354 - INFO - input_processor - Done processing training file, batch size is 1373\n",
      "2022-11-17 00:49:40,354 - INFO - input_processor - Processing training file 16 of 25: ./training/train.bucket13.txt\n",
      "2022-11-17 00:49:40,373 - INFO - input_processor - Done processing training file, batch size is 1451\n",
      "2022-11-17 00:49:40,374 - INFO - input_processor - Processing training file 17 of 25: ./training/train.bucket2.txt\n",
      "2022-11-17 00:49:40,394 - INFO - input_processor - Done processing training file, batch size is 1535\n",
      "2022-11-17 00:49:40,395 - INFO - input_processor - Processing training file 18 of 25: ./training/train.bucket23.txt\n",
      "2022-11-17 00:49:40,423 - INFO - input_processor - Done processing training file, batch size is 1637\n",
      "2022-11-17 00:49:40,423 - INFO - input_processor - Processing training file 19 of 25: ./training/train.bucket4.txt\n",
      "2022-11-17 00:49:40,446 - INFO - input_processor - Done processing training file, batch size is 1738\n",
      "2022-11-17 00:49:40,447 - INFO - input_processor - Processing training file 20 of 25: ./training/train.bucket24.txt\n",
      "2022-11-17 00:49:40,474 - INFO - input_processor - Done processing training file, batch size is 1847\n",
      "2022-11-17 00:49:40,475 - INFO - input_processor - Processing training file 21 of 25: ./training/train.bucket7.txt\n",
      "2022-11-17 00:49:40,500 - INFO - input_processor - Done processing training file, batch size is 1949\n",
      "2022-11-17 00:49:40,500 - INFO - input_processor - Processing training file 22 of 25: ./training/train.bucket8.txt\n",
      "2022-11-17 00:49:40,523 - INFO - input_processor - Done processing training file, batch size is 2040\n",
      "2022-11-17 00:49:40,524 - INFO - input_processor - Processing training file 23 of 25: ./training/train.bucket0.txt\n",
      "2022-11-17 00:49:40,543 - INFO - input_processor - Done processing training file, batch size is 2119\n",
      "2022-11-17 00:49:40,543 - INFO - input_processor - Processing training file 24 of 25: ./training/train.bucket3.txt\n",
      "2022-11-17 00:49:40,565 - INFO - input_processor - Done processing training file, batch size is 2207\n",
      "2022-11-17 00:49:40,566 - INFO - input_processor - Processing training file 25 of 25: ./training/train.bucket20.txt\n",
      "2022-11-17 00:49:40,592 - INFO - input_processor - Done processing training file, batch size is 2315\n",
      "Epoch:   0%|          | 0/5 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/data/user/home/rcorkil2/classes/NLP/bertram-master/train.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/data/user/home/rcorkil2/classes/NLP/bertram-master/train.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    295\u001b[0m                 \u001b[0mtarget_vectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_vectors\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatches\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegment_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrs_of_contexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_positions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mngram_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mngram_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_vectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mn_gpu\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m                     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# mean() to average on multi-gpu.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/base_env/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/user/home/rcorkil2/classes/NLP/bertram-master/bertram.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, token_type_ids, nr_of_contexts, mask_positions, attention_mask, ngram_ids, ngram_lengths, target_vectors)\u001b[0m\n\u001b[1;32m    263\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m                 \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m                 \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m             )\n\u001b[1;32m    267\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_embeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moverwrite_fct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "%run train.py --model_cls 'deberta' --bert_model 'microsoft/deberta-base' --output_dir ./outputs/DeBERTa_context --train_dir ./training/ --vocab ./training/train.vwc100 --emb_file ./fcm/wordEmbeddings/glove.6B.50d.txt --num_train_epochs 5 --emb_dim 768 --max_seq_length 100 --mode context --train_batch_size 32 --no_finetuning --smin 4 --smax 32\n",
    "\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95aa137b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "adb95f57",
   "metadata": {},
   "source": [
    "### Train the bertram form-only model (DeBERTa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e948060",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-16 22:04:18,192 - INFO - train - Initializing new BERTRAM instance from microsoft/deberta-base.\n",
      "2022-11-16 22:04:18,645 - INFO - ngram_models - Found 712 ngrams with min count 4 and (nmin,nmax)=(3,5), first 10: ['UNK', 'PAD', 'ed<S>', 'ng<S>', 'ing', 'er<S>', 'ing<S>', 'on<S>', '<S>co', 'ion'], last 10: ['tly', 'tly<S>', 'cor', 'anc', 'ance', 'ance<S>', '<S>des', 'des', 'sio', 'sion']\n",
      "2022-11-16 22:04:18,650 - INFO - utils - Loading embeddings from ./fcm/wordEmbeddings/glove.840B.300d.txt\n",
      "2022-11-16 22:51:27,692 - INFO - utils - Done loading embeddings\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "864e6ee2ba89480094cdf5674e28b6ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/474 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37d1b0e8e6ab4a039bab386fd7a79782",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/559M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing BertramForDeBERTa: ['deberta.encoder.layer.0.attention.output.dense.bias', 'deberta.encoder.layer.3.attention.output.dense.bias', 'deberta.encoder.layer.2.output.dense.weight', 'deberta.encoder.layer.7.attention.output.LayerNorm.bias', 'deberta.encoder.layer.9.attention.output.LayerNorm.bias', 'deberta.encoder.layer.5.intermediate.dense.weight', 'deberta.encoder.layer.4.attention.self.v_bias', 'deberta.encoder.layer.3.attention.self.in_proj.weight', 'deberta.encoder.layer.11.attention.output.dense.bias', 'deberta.encoder.layer.8.intermediate.dense.weight', 'deberta.encoder.layer.5.output.LayerNorm.weight', 'deberta.encoder.layer.8.attention.self.in_proj.weight', 'deberta.embeddings.position_embeddings.weight', 'deberta.encoder.layer.5.attention.output.LayerNorm.bias', 'deberta.encoder.layer.6.intermediate.dense.bias', 'deberta.encoder.layer.1.attention.self.pos_proj.weight', 'deberta.encoder.layer.7.attention.output.dense.weight', 'deberta.embeddings.LayerNorm.bias', 'deberta.encoder.layer.5.attention.self.pos_q_proj.bias', 'deberta.encoder.layer.1.output.dense.bias', 'deberta.encoder.layer.2.attention.self.pos_proj.weight', 'deberta.encoder.layer.11.intermediate.dense.weight', 'deberta.encoder.layer.8.attention.self.pos_proj.weight', 'deberta.encoder.layer.3.intermediate.dense.bias', 'deberta.encoder.layer.8.attention.output.dense.bias', 'deberta.encoder.layer.6.output.dense.bias', 'deberta.encoder.layer.7.attention.self.in_proj.weight', 'deberta.encoder.layer.2.attention.output.dense.weight', 'deberta.encoder.layer.4.output.LayerNorm.bias', 'deberta.encoder.layer.0.output.LayerNorm.bias', 'deberta.encoder.layer.4.attention.self.in_proj.weight', 'deberta.encoder.layer.1.attention.output.dense.bias', 'deberta.encoder.layer.5.attention.output.dense.bias', 'deberta.encoder.layer.9.output.LayerNorm.weight', 'deberta.encoder.layer.5.output.dense.weight', 'deberta.encoder.layer.6.attention.self.pos_proj.weight', 'deberta.encoder.layer.10.intermediate.dense.bias', 'deberta.encoder.layer.9.attention.self.pos_proj.weight', 'deberta.encoder.layer.11.attention.self.pos_q_proj.weight', 'deberta.encoder.layer.11.attention.self.in_proj.weight', 'deberta.encoder.layer.10.attention.self.pos_q_proj.bias', 'deberta.encoder.layer.3.intermediate.dense.weight', 'deberta.encoder.layer.3.attention.output.dense.weight', 'deberta.encoder.layer.4.attention.output.dense.weight', 'deberta.encoder.layer.4.attention.output.LayerNorm.weight', 'deberta.encoder.layer.7.output.dense.weight', 'deberta.encoder.layer.10.attention.self.pos_q_proj.weight', 'deberta.encoder.layer.9.attention.self.pos_q_proj.bias', 'deberta.encoder.layer.1.attention.self.pos_q_proj.weight', 'deberta.encoder.layer.7.output.LayerNorm.weight', 'deberta.encoder.layer.4.attention.output.dense.bias', 'deberta.encoder.layer.8.attention.output.LayerNorm.weight', 'deberta.encoder.layer.5.attention.output.dense.weight', 'deberta.encoder.layer.8.attention.self.q_bias', 'deberta.encoder.layer.8.attention.output.LayerNorm.bias', 'deberta.encoder.layer.4.output.LayerNorm.weight', 'deberta.encoder.layer.5.intermediate.dense.bias', 'deberta.encoder.layer.5.output.LayerNorm.bias', 'deberta.encoder.layer.0.attention.output.LayerNorm.weight', 'deberta.encoder.layer.9.intermediate.dense.weight', 'deberta.encoder.layer.11.output.dense.bias', 'deberta.encoder.layer.10.attention.self.v_bias', 'deberta.encoder.layer.8.attention.output.dense.weight', 'deberta.encoder.layer.4.attention.self.pos_proj.weight', 'deberta.encoder.layer.1.attention.self.v_bias', 'deberta.encoder.layer.3.attention.self.v_bias', 'deberta.encoder.layer.0.attention.self.pos_q_proj.weight', 'deberta.encoder.layer.6.attention.output.LayerNorm.bias', 'deberta.encoder.layer.6.output.dense.weight', 'deberta.encoder.layer.9.attention.self.pos_q_proj.weight', 'deberta.encoder.layer.6.attention.self.in_proj.weight', 'deberta.encoder.layer.6.attention.output.LayerNorm.weight', 'deberta.encoder.layer.7.intermediate.dense.bias', 'deberta.encoder.layer.10.attention.output.dense.bias', 'deberta.encoder.layer.7.attention.self.pos_q_proj.weight', 'deberta.embeddings.LayerNorm.weight', 'deberta.encoder.layer.5.attention.self.q_bias', 'deberta.encoder.layer.3.attention.self.q_bias', 'deberta.encoder.layer.6.attention.self.pos_q_proj.weight', 'deberta.encoder.layer.9.output.dense.bias', 'deberta.encoder.layer.11.attention.self.pos_proj.weight', 'deberta.encoder.layer.0.attention.output.dense.weight', 'deberta.encoder.layer.9.attention.self.in_proj.weight', 'deberta.encoder.layer.11.output.LayerNorm.weight', 'deberta.encoder.layer.11.output.dense.weight', 'deberta.encoder.layer.1.output.dense.weight', 'deberta.encoder.layer.2.attention.output.LayerNorm.bias', 'deberta.encoder.layer.2.attention.self.in_proj.weight', 'deberta.encoder.layer.2.attention.output.dense.bias', 'deberta.encoder.layer.2.attention.self.pos_q_proj.bias', 'deberta.encoder.layer.3.attention.self.pos_q_proj.bias', 'deberta.encoder.layer.6.output.LayerNorm.weight', 'deberta.encoder.layer.5.output.dense.bias', 'deberta.encoder.layer.6.intermediate.dense.weight', 'deberta.encoder.layer.1.intermediate.dense.weight', 'deberta.encoder.layer.7.attention.self.v_bias', 'deberta.encoder.layer.3.output.dense.bias', 'deberta.encoder.layer.7.intermediate.dense.weight', 'deberta.encoder.layer.1.output.LayerNorm.bias', 'deberta.encoder.layer.4.output.dense.weight', 'deberta.encoder.layer.5.attention.self.pos_q_proj.weight', 'deberta.encoder.layer.10.intermediate.dense.weight', 'deberta.encoder.layer.10.attention.self.q_bias', 'deberta.encoder.layer.1.intermediate.dense.bias', 'deberta.encoder.layer.1.attention.output.LayerNorm.weight', 'deberta.encoder.layer.2.attention.self.v_bias', 'deberta.encoder.layer.2.output.LayerNorm.weight', 'deberta.encoder.layer.4.intermediate.dense.bias', 'deberta.encoder.layer.3.output.dense.weight', 'deberta.encoder.layer.5.attention.self.in_proj.weight', 'deberta.encoder.layer.2.output.LayerNorm.bias', 'deberta.encoder.layer.0.attention.self.pos_q_proj.bias', 'deberta.encoder.layer.9.output.LayerNorm.bias', 'deberta.encoder.layer.10.output.dense.weight', 'deberta.encoder.layer.10.attention.output.LayerNorm.bias', 'deberta.encoder.layer.6.output.LayerNorm.bias', 'deberta.encoder.layer.1.attention.output.LayerNorm.bias', 'deberta.encoder.layer.1.output.LayerNorm.weight', 'deberta.encoder.layer.2.attention.self.q_bias', 'deberta.encoder.layer.10.attention.output.LayerNorm.weight', 'deberta.encoder.layer.8.intermediate.dense.bias', 'deberta.encoder.layer.11.attention.self.v_bias', 'deberta.encoder.layer.6.attention.self.q_bias', 'deberta.encoder.layer.0.intermediate.dense.weight', 'deberta.encoder.layer.8.output.dense.bias', 'deberta.encoder.layer.10.output.dense.bias', 'deberta.encoder.layer.4.attention.self.q_bias', 'deberta.encoder.layer.7.output.LayerNorm.bias', 'deberta.encoder.layer.3.attention.output.LayerNorm.weight', 'deberta.encoder.layer.5.attention.output.LayerNorm.weight', 'deberta.encoder.layer.10.attention.output.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'deberta.encoder.layer.1.attention.self.q_bias', 'deberta.encoder.layer.2.intermediate.dense.bias', 'deberta.encoder.layer.9.attention.output.dense.weight', 'deberta.encoder.layer.5.attention.self.pos_proj.weight', 'deberta.encoder.layer.3.attention.self.pos_proj.weight', 'deberta.encoder.layer.1.attention.output.dense.weight', 'deberta.encoder.layer.6.attention.output.dense.bias', 'deberta.encoder.layer.7.output.dense.bias', 'deberta.encoder.layer.8.output.LayerNorm.weight', 'deberta.encoder.layer.11.attention.output.LayerNorm.weight', 'deberta.encoder.layer.3.output.LayerNorm.weight', 'deberta.encoder.layer.4.attention.output.LayerNorm.bias', 'deberta.encoder.layer.9.attention.self.q_bias', 'deberta.encoder.layer.10.attention.self.in_proj.weight', 'deberta.encoder.layer.3.attention.self.pos_q_proj.weight', 'deberta.encoder.layer.11.attention.output.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'deberta.encoder.layer.1.attention.self.in_proj.weight', 'deberta.encoder.layer.6.attention.output.dense.weight', 'deberta.embeddings.word_embeddings.weight', 'deberta.encoder.layer.5.attention.self.v_bias', 'deberta.encoder.layer.3.attention.output.LayerNorm.bias', 'deberta.encoder.layer.9.attention.output.LayerNorm.weight', 'deberta.encoder.layer.0.output.dense.weight', 'deberta.encoder.layer.11.intermediate.dense.bias', 'deberta.encoder.layer.0.output.dense.bias', 'deberta.encoder.layer.9.attention.output.dense.bias', 'deberta.encoder.layer.2.intermediate.dense.weight', 'deberta.encoder.rel_embeddings.weight', 'deberta.encoder.layer.4.attention.self.pos_q_proj.bias', 'deberta.encoder.layer.7.attention.self.pos_q_proj.bias', 'deberta.encoder.layer.7.attention.output.dense.bias', 'deberta.encoder.layer.0.output.LayerNorm.weight', 'deberta.encoder.layer.7.attention.self.pos_proj.weight', 'deberta.encoder.layer.2.attention.output.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias', 'deberta.encoder.layer.9.intermediate.dense.bias', 'deberta.encoder.layer.0.attention.self.pos_proj.weight', 'deberta.encoder.layer.7.attention.self.q_bias', 'deberta.encoder.layer.0.attention.output.LayerNorm.bias', 'deberta.encoder.layer.8.output.LayerNorm.bias', 'deberta.encoder.layer.11.output.LayerNorm.bias', 'deberta.encoder.layer.7.attention.output.LayerNorm.weight', 'deberta.encoder.layer.8.attention.self.pos_q_proj.bias', 'deberta.encoder.layer.10.attention.self.pos_proj.weight', 'deberta.encoder.layer.3.output.LayerNorm.bias', 'deberta.encoder.layer.4.output.dense.bias', 'deberta.encoder.layer.8.output.dense.weight', 'deberta.encoder.layer.6.attention.self.v_bias', 'deberta.encoder.layer.11.attention.self.pos_q_proj.bias', 'deberta.encoder.layer.6.attention.self.pos_q_proj.bias', 'deberta.encoder.layer.2.output.dense.bias', 'deberta.encoder.layer.11.attention.output.dense.weight', 'deberta.encoder.layer.0.attention.self.q_bias', 'deberta.encoder.layer.10.output.LayerNorm.bias', 'deberta.encoder.layer.9.output.dense.weight', 'deberta.encoder.layer.0.attention.self.v_bias', 'deberta.encoder.layer.2.attention.self.pos_q_proj.weight', 'deberta.encoder.layer.0.intermediate.dense.bias', 'deberta.encoder.layer.1.attention.self.pos_q_proj.bias', 'deberta.encoder.layer.10.output.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'deberta.encoder.layer.0.attention.self.in_proj.weight', 'deberta.encoder.layer.4.intermediate.dense.weight', 'deberta.encoder.layer.11.attention.self.q_bias', 'deberta.encoder.layer.4.attention.self.pos_q_proj.weight', 'lm_predictions.lm_head.bias', 'deberta.encoder.layer.9.attention.self.v_bias', 'deberta.encoder.layer.8.attention.self.pos_q_proj.weight', 'deberta.encoder.layer.8.attention.self.v_bias']\n",
      "- This IS expected if you are initializing BertramForDeBERTa from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertramForDeBERTa from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertramForDeBERTa were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['deberta.ngram_processor.ngram_embeddings.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'BertramForDeBERTa' object has no attribute 'transformer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/data/user/home/rcorkil2/classes/NLP/bertram-master/train.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/data/user/home/rcorkil2/classes/NLP/bertram-master/train.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0minput_processor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/user/home/rcorkil2/classes/NLP/bertram-master/bertram.py\u001b[0m in \u001b[0;36msetup\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0;34m\"\"\"Initialize the BERTRAM model and put a wrapper around the underlying transformer's embedding layer\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0mform_and_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequires_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbertram_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mrequires_form\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbertram_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOverwriteableEmbedding\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mform_and_context\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m             \u001b[0mword_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOverwriteableEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_embeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/base_env/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1206\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1207\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0;32m-> 1208\u001b[0;31m             type(self).__name__, name))\n\u001b[0m\u001b[1;32m   1209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Module'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'BertramForDeBERTa' object has no attribute 'transformer'"
     ]
    }
   ],
   "source": [
    "%run train.py \\\n",
    "   --model_cls 'deberta' \\\n",
    "   --bert_model 'microsoft/deberta-base' \\\n",
    "   --output_dir ./outputs/DeBERTa_form \\\n",
    "   --train_dir ./training/ \\\n",
    "   --vocab ./training/train.vwc100 \\\n",
    "   --emb_file ./fcm/wordEmbeddings/glove.6B.50d.txt \\\n",
    "   --num_train_epochs 20 \\\n",
    "   --emb_dim 768 \\\n",
    "   --train_batch_size 32 \\\n",
    "   --smin 1 \\\n",
    "   --smax 1 \\\n",
    "   --max_seq_length 10 \\\n",
    "   --mode form \\\n",
    "   --learning_rate 0.01 \\\n",
    "   --dropout 0.1 "
   ]
  },
  {
   "cell_type": "raw",
   "id": "94c685ec",
   "metadata": {},
   "source": [
    "2022-11-16 22:04:18,192 - INFO - train - Initializing new BERTRAM instance from microsoft/deberta-base.\n",
    "2022-11-16 22:04:18,645 - INFO - ngram_models - Found 712 ngrams with min count 4 and (nmin,nmax)=(3,5), first 10: ['UNK', 'PAD', 'ed<S>', 'ng<S>', 'ing', 'er<S>', 'ing<S>', 'on<S>', '<S>co', 'ion'], last 10: ['tly', 'tly<S>', 'cor', 'anc', 'ance', 'ance<S>', '<S>des', 'des', 'sio', 'sion']\n",
    "2022-11-16 22:04:18,650 - INFO - utils - Loading embeddings from ./fcm/wordEmbeddings/glove.840B.300d.txt\n",
    "2022-11-16 22:51:27,692 - INFO - utils - Done loading embeddings\n",
    "Downloading: 100%\n",
    "474/474 [00:00<00:00, 7.44kB/s]\n",
    "Downloading: 100%\n",
    "559M/559M [00:08<00:00, 70.3MB/s]\n",
    "Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing BertramForDeBERTa: ['deberta.encoder.layer.0.attention.output.dense.bias', 'deberta.encoder.layer.3.attention.output.dense.bias', 'deberta.encoder.layer.2.output.dense.weight', 'deberta.encoder.layer.7.attention.output.LayerNorm.bias', 'deberta.encoder.layer.9.attention.output.LayerNorm.bias', 'deberta.encoder.layer.5.intermediate.dense.weight', 'deberta.encoder.layer.4.attention.self.v_bias', 'deberta.encoder.layer.3.attention.self.in_proj.weight', 'deberta.encoder.layer.11.attention.output.dense.bias', 'deberta.encoder.layer.8.intermediate.dense.weight', 'deberta.encoder.layer.5.output.LayerNorm.weight', 'deberta.encoder.layer.8.attention.self.in_proj.weight', 'deberta.embeddings.position_embeddings.weight', 'deberta.encoder.layer.5.attention.output.LayerNorm.bias', 'deberta.encoder.layer.6.intermediate.dense.bias', 'deberta.encoder.layer.1.attention.self.pos_proj.weight', 'deberta.encoder.layer.7.attention.output.dense.weight', 'deberta.embeddings.LayerNorm.bias', 'deberta.encoder.layer.5.attention.self.pos_q_proj.bias', 'deberta.encoder.layer.1.output.dense.bias', 'deberta.encoder.layer.2.attention.self.pos_proj.weight', 'deberta.encoder.layer.11.intermediate.dense.weight', 'deberta.encoder.layer.8.attention.self.pos_proj.weight', 'deberta.encoder.layer.3.intermediate.dense.bias', 'deberta.encoder.layer.8.attention.output.dense.bias', 'deberta.encoder.layer.6.output.dense.bias', 'deberta.encoder.layer.7.attention.self.in_proj.weight', 'deberta.encoder.layer.2.attention.output.dense.weight', 'deberta.encoder.layer.4.output.LayerNorm.bias', 'deberta.encoder.layer.0.output.LayerNorm.bias', 'deberta.encoder.layer.4.attention.self.in_proj.weight', 'deberta.encoder.layer.1.attention.output.dense.bias', 'deberta.encoder.layer.5.attention.output.dense.bias', 'deberta.encoder.layer.9.output.LayerNorm.weight', 'deberta.encoder.layer.5.output.dense.weight', 'deberta.encoder.layer.6.attention.self.pos_proj.weight', 'deberta.encoder.layer.10.intermediate.dense.bias', 'deberta.encoder.layer.9.attention.self.pos_proj.weight', 'deberta.encoder.layer.11.attention.self.pos_q_proj.weight', 'deberta.encoder.layer.11.attention.self.in_proj.weight', 'deberta.encoder.layer.10.attention.self.pos_q_proj.bias', 'deberta.encoder.layer.3.intermediate.dense.weight', 'deberta.encoder.layer.3.attention.output.dense.weight', 'deberta.encoder.layer.4.attention.output.dense.weight', 'deberta.encoder.layer.4.attention.output.LayerNorm.weight', 'deberta.encoder.layer.7.output.dense.weight', 'deberta.encoder.layer.10.attention.self.pos_q_proj.weight', 'deberta.encoder.layer.9.attention.self.pos_q_proj.bias', 'deberta.encoder.layer.1.attention.self.pos_q_proj.weight', 'deberta.encoder.layer.7.output.LayerNorm.weight', 'deberta.encoder.layer.4.attention.output.dense.bias', 'deberta.encoder.layer.8.attention.output.LayerNorm.weight', 'deberta.encoder.layer.5.attention.output.dense.weight', 'deberta.encoder.layer.8.attention.self.q_bias', 'deberta.encoder.layer.8.attention.output.LayerNorm.bias', 'deberta.encoder.layer.4.output.LayerNorm.weight', 'deberta.encoder.layer.5.intermediate.dense.bias', 'deberta.encoder.layer.5.output.LayerNorm.bias', 'deberta.encoder.layer.0.attention.output.LayerNorm.weight', 'deberta.encoder.layer.9.intermediate.dense.weight', 'deberta.encoder.layer.11.output.dense.bias', 'deberta.encoder.layer.10.attention.self.v_bias', 'deberta.encoder.layer.8.attention.output.dense.weight', 'deberta.encoder.layer.4.attention.self.pos_proj.weight', 'deberta.encoder.layer.1.attention.self.v_bias', 'deberta.encoder.layer.3.attention.self.v_bias', 'deberta.encoder.layer.0.attention.self.pos_q_proj.weight', 'deberta.encoder.layer.6.attention.output.LayerNorm.bias', 'deberta.encoder.layer.6.output.dense.weight', 'deberta.encoder.layer.9.attention.self.pos_q_proj.weight', 'deberta.encoder.layer.6.attention.self.in_proj.weight', 'deberta.encoder.layer.6.attention.output.LayerNorm.weight', 'deberta.encoder.layer.7.intermediate.dense.bias', 'deberta.encoder.layer.10.attention.output.dense.bias', 'deberta.encoder.layer.7.attention.self.pos_q_proj.weight', 'deberta.embeddings.LayerNorm.weight', 'deberta.encoder.layer.5.attention.self.q_bias', 'deberta.encoder.layer.3.attention.self.q_bias', 'deberta.encoder.layer.6.attention.self.pos_q_proj.weight', 'deberta.encoder.layer.9.output.dense.bias', 'deberta.encoder.layer.11.attention.self.pos_proj.weight', 'deberta.encoder.layer.0.attention.output.dense.weight', 'deberta.encoder.layer.9.attention.self.in_proj.weight', 'deberta.encoder.layer.11.output.LayerNorm.weight', 'deberta.encoder.layer.11.output.dense.weight', 'deberta.encoder.layer.1.output.dense.weight', 'deberta.encoder.layer.2.attention.output.LayerNorm.bias', 'deberta.encoder.layer.2.attention.self.in_proj.weight', 'deberta.encoder.layer.2.attention.output.dense.bias', 'deberta.encoder.layer.2.attention.self.pos_q_proj.bias', 'deberta.encoder.layer.3.attention.self.pos_q_proj.bias', 'deberta.encoder.layer.6.output.LayerNorm.weight', 'deberta.encoder.layer.5.output.dense.bias', 'deberta.encoder.layer.6.intermediate.dense.weight', 'deberta.encoder.layer.1.intermediate.dense.weight', 'deberta.encoder.layer.7.attention.self.v_bias', 'deberta.encoder.layer.3.output.dense.bias', 'deberta.encoder.layer.7.intermediate.dense.weight', 'deberta.encoder.layer.1.output.LayerNorm.bias', 'deberta.encoder.layer.4.output.dense.weight', 'deberta.encoder.layer.5.attention.self.pos_q_proj.weight', 'deberta.encoder.layer.10.intermediate.dense.weight', 'deberta.encoder.layer.10.attention.self.q_bias', 'deberta.encoder.layer.1.intermediate.dense.bias', 'deberta.encoder.layer.1.attention.output.LayerNorm.weight', 'deberta.encoder.layer.2.attention.self.v_bias', 'deberta.encoder.layer.2.output.LayerNorm.weight', 'deberta.encoder.layer.4.intermediate.dense.bias', 'deberta.encoder.layer.3.output.dense.weight', 'deberta.encoder.layer.5.attention.self.in_proj.weight', 'deberta.encoder.layer.2.output.LayerNorm.bias', 'deberta.encoder.layer.0.attention.self.pos_q_proj.bias', 'deberta.encoder.layer.9.output.LayerNorm.bias', 'deberta.encoder.layer.10.output.dense.weight', 'deberta.encoder.layer.10.attention.output.LayerNorm.bias', 'deberta.encoder.layer.6.output.LayerNorm.bias', 'deberta.encoder.layer.1.attention.output.LayerNorm.bias', 'deberta.encoder.layer.1.output.LayerNorm.weight', 'deberta.encoder.layer.2.attention.self.q_bias', 'deberta.encoder.layer.10.attention.output.LayerNorm.weight', 'deberta.encoder.layer.8.intermediate.dense.bias', 'deberta.encoder.layer.11.attention.self.v_bias', 'deberta.encoder.layer.6.attention.self.q_bias', 'deberta.encoder.layer.0.intermediate.dense.weight', 'deberta.encoder.layer.8.output.dense.bias', 'deberta.encoder.layer.10.output.dense.bias', 'deberta.encoder.layer.4.attention.self.q_bias', 'deberta.encoder.layer.7.output.LayerNorm.bias', 'deberta.encoder.layer.3.attention.output.LayerNorm.weight', 'deberta.encoder.layer.5.attention.output.LayerNorm.weight', 'deberta.encoder.layer.10.attention.output.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'deberta.encoder.layer.1.attention.self.q_bias', 'deberta.encoder.layer.2.intermediate.dense.bias', 'deberta.encoder.layer.9.attention.output.dense.weight', 'deberta.encoder.layer.5.attention.self.pos_proj.weight', 'deberta.encoder.layer.3.attention.self.pos_proj.weight', 'deberta.encoder.layer.1.attention.output.dense.weight', 'deberta.encoder.layer.6.attention.output.dense.bias', 'deberta.encoder.layer.7.output.dense.bias', 'deberta.encoder.layer.8.output.LayerNorm.weight', 'deberta.encoder.layer.11.attention.output.LayerNorm.weight', 'deberta.encoder.layer.3.output.LayerNorm.weight', 'deberta.encoder.layer.4.attention.output.LayerNorm.bias', 'deberta.encoder.layer.9.attention.self.q_bias', 'deberta.encoder.layer.10.attention.self.in_proj.weight', 'deberta.encoder.layer.3.attention.self.pos_q_proj.weight', 'deberta.encoder.layer.11.attention.output.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'deberta.encoder.layer.1.attention.self.in_proj.weight', 'deberta.encoder.layer.6.attention.output.dense.weight', 'deberta.embeddings.word_embeddings.weight', 'deberta.encoder.layer.5.attention.self.v_bias', 'deberta.encoder.layer.3.attention.output.LayerNorm.bias', 'deberta.encoder.layer.9.attention.output.LayerNorm.weight', 'deberta.encoder.layer.0.output.dense.weight', 'deberta.encoder.layer.11.intermediate.dense.bias', 'deberta.encoder.layer.0.output.dense.bias', 'deberta.encoder.layer.9.attention.output.dense.bias', 'deberta.encoder.layer.2.intermediate.dense.weight', 'deberta.encoder.rel_embeddings.weight', 'deberta.encoder.layer.4.attention.self.pos_q_proj.bias', 'deberta.encoder.layer.7.attention.self.pos_q_proj.bias', 'deberta.encoder.layer.7.attention.output.dense.bias', 'deberta.encoder.layer.0.output.LayerNorm.weight', 'deberta.encoder.layer.7.attention.self.pos_proj.weight', 'deberta.encoder.layer.2.attention.output.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias', 'deberta.encoder.layer.9.intermediate.dense.bias', 'deberta.encoder.layer.0.attention.self.pos_proj.weight', 'deberta.encoder.layer.7.attention.self.q_bias', 'deberta.encoder.layer.0.attention.output.LayerNorm.bias', 'deberta.encoder.layer.8.output.LayerNorm.bias', 'deberta.encoder.layer.11.output.LayerNorm.bias', 'deberta.encoder.layer.7.attention.output.LayerNorm.weight', 'deberta.encoder.layer.8.attention.self.pos_q_proj.bias', 'deberta.encoder.layer.10.attention.self.pos_proj.weight', 'deberta.encoder.layer.3.output.LayerNorm.bias', 'deberta.encoder.layer.4.output.dense.bias', 'deberta.encoder.layer.8.output.dense.weight', 'deberta.encoder.layer.6.attention.self.v_bias', 'deberta.encoder.layer.11.attention.self.pos_q_proj.bias', 'deberta.encoder.layer.6.attention.self.pos_q_proj.bias', 'deberta.encoder.layer.2.output.dense.bias', 'deberta.encoder.layer.11.attention.output.dense.weight', 'deberta.encoder.layer.0.attention.self.q_bias', 'deberta.encoder.layer.10.output.LayerNorm.bias', 'deberta.encoder.layer.9.output.dense.weight', 'deberta.encoder.layer.0.attention.self.v_bias', 'deberta.encoder.layer.2.attention.self.pos_q_proj.weight', 'deberta.encoder.layer.0.intermediate.dense.bias', 'deberta.encoder.layer.1.attention.self.pos_q_proj.bias', 'deberta.encoder.layer.10.output.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'deberta.encoder.layer.0.attention.self.in_proj.weight', 'deberta.encoder.layer.4.intermediate.dense.weight', 'deberta.encoder.layer.11.attention.self.q_bias', 'deberta.encoder.layer.4.attention.self.pos_q_proj.weight', 'lm_predictions.lm_head.bias', 'deberta.encoder.layer.9.attention.self.v_bias', 'deberta.encoder.layer.8.attention.self.pos_q_proj.weight', 'deberta.encoder.layer.8.attention.self.v_bias']\n",
    "- This IS expected if you are initializing BertramForDeBERTa from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
    "- This IS NOT expected if you are initializing BertramForDeBERTa from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
    "Some weights of BertramForDeBERTa were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['deberta.ngram_processor.ngram_embeddings.weight']\n",
    "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
    "---------------------------------------------------------------------------\n",
    "AttributeError                            Traceback (most recent call last)\n",
    "/data/user/home/rcorkil2/classes/NLP/bertram-master/train.py in <module>\n",
    "    361 \n",
    "    362 if __name__ == \"__main__\":\n",
    "--> 363     main(sys.argv[1:])\n",
    "\n",
    "/data/user/home/rcorkil2/classes/NLP/bertram-master/train.py in main(args)\n",
    "    192             input_processor.mode = args.mode\n",
    "    193 \n",
    "--> 194     model.setup()\n",
    "    195     model = model.to(device)\n",
    "    196 \n",
    "\n",
    "/data/user/home/rcorkil2/classes/NLP/bertram-master/bertram.py in setup(self)\n",
    "    192         \"\"\"Initialize the BERTRAM model and put a wrapper around the underlying transformer's embedding layer\"\"\"\n",
    "    193         form_and_context = requires_context(self.bertram_config.mode) and requires_form(self.bertram_config.mode)\n",
    "--> 194         if not isinstance(self.transformer.embeddings.word_embeddings, OverwriteableEmbedding) and form_and_context:\n",
    "    195             word_embeddings = self.transformer.embeddings.word_embeddings\n",
    "    196             self.transformer.embeddings.word_embeddings = OverwriteableEmbedding(word_embeddings)\n",
    "\n",
    "~/.conda/envs/base_env/lib/python3.7/site-packages/torch/nn/modules/module.py in __getattr__(self, name)\n",
    "   1206                 return modules[name]\n",
    "   1207         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n",
    "-> 1208             type(self).__name__, name))\n",
    "   1209 \n",
    "   1210     def __setattr__(self, name: str, value: Union[Tensor, 'Module']) -> None:\n",
    "\n",
    "AttributeError: 'BertramForDeBERTa' object has no attribute 'transformer'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c783084b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ef3eb907",
   "metadata": {},
   "source": [
    "### Combine models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f569342",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run fuse_models.py \\\n",
    "   --form_model ./outputs/BERT_form \\ #MAKE THIS\n",
    "   --context_model ./outputs/BERT_context \\ #MAKE THIS\n",
    "   --mode $MODE \\ # ADD or REPLACE --> CHOOSE\n",
    "   --output ./outputs/BERT_fused #MAKE THIS"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
