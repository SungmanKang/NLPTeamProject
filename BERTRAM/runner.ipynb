{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d515b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import zipfile as zf\n",
    "files = zf.ZipFile(\"bertram-master.zip\", 'r')\n",
    "files.extractall('directory to extract')\n",
    "files.close()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf4f2065",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/data/user/home/sungman/test/NLP Group Project/bertram-master'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ada5ca83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/data/user/home/sungman/test/NLP Group Project/bertram-master/bertram-master.zip'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "shutil.make_archive('bertram-master','zip','/data/user/home/sungman/test/NLP Group Project/bertram-master')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ec4816",
   "metadata": {},
   "source": [
    "1. bertram.py -> def setup(self):        \n",
    "    form_and_context = requires_context(self.bertram_config.mode) and requires_form(self.bertram_config.mode)\n",
    "\n",
    "remove the # to fix the error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3f705e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: transformers 2.2.0\n",
      "Uninstalling transformers-2.2.0:\n",
      "  Would remove:\n",
      "    /data/user/home/sungman/.local/bin/transformers\n",
      "    /data/user/home/sungman/.local/lib/python3.9/site-packages/transformers-2.2.0.dist-info/*\n",
      "    /data/user/home/sungman/.local/lib/python3.9/site-packages/transformers/*\n",
      "Proceed (Y/n)? "
     ]
    }
   ],
   "source": [
    "!pip uninstall transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "158db498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /data/user/home/sungman/.local/lib/python3.9/site-packages (2.2.0)\n",
      "Requirement already satisfied: boto3 in /data/rc/apps/rc/software/Anaconda3/2022.05/lib/python3.9/site-packages (from transformers) (1.21.32)\n",
      "Requirement already satisfied: regex in /data/rc/apps/rc/software/Anaconda3/2022.05/lib/python3.9/site-packages (from transformers) (2022.3.15)\n",
      "Requirement already satisfied: sentencepiece in /data/user/home/sungman/.local/lib/python3.9/site-packages (from transformers) (0.1.95)\n",
      "Requirement already satisfied: numpy in /data/rc/apps/rc/software/Anaconda3/2022.05/lib/python3.9/site-packages (from transformers) (1.21.5)\n",
      "Requirement already satisfied: tqdm in /data/rc/apps/rc/software/Anaconda3/2022.05/lib/python3.9/site-packages (from transformers) (4.64.0)\n",
      "Requirement already satisfied: requests in /data/user/home/sungman/.local/lib/python3.9/site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: sacremoses in /data/user/home/sungman/.local/lib/python3.9/site-packages (from transformers) (0.0.53)\n",
      "Requirement already satisfied: botocore<1.25.0,>=1.24.32 in /data/rc/apps/rc/software/Anaconda3/2022.05/lib/python3.9/site-packages (from boto3->transformers) (1.24.32)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /data/rc/apps/rc/software/Anaconda3/2022.05/lib/python3.9/site-packages (from boto3->transformers) (0.10.0)\n",
      "Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /data/rc/apps/rc/software/Anaconda3/2022.05/lib/python3.9/site-packages (from boto3->transformers) (0.5.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /data/rc/apps/rc/software/Anaconda3/2022.05/lib/python3.9/site-packages (from botocore<1.25.0,>=1.24.32->boto3->transformers) (2.8.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /data/rc/apps/rc/software/Anaconda3/2022.05/lib/python3.9/site-packages (from botocore<1.25.0,>=1.24.32->boto3->transformers) (1.26.9)\n",
      "Requirement already satisfied: six>=1.5 in /data/rc/apps/rc/software/Anaconda3/2022.05/lib/python3.9/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.25.0,>=1.24.32->boto3->transformers) (1.16.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /data/rc/apps/rc/software/Anaconda3/2022.05/lib/python3.9/site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /data/rc/apps/rc/software/Anaconda3/2022.05/lib/python3.9/site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /data/rc/apps/rc/software/Anaconda3/2022.05/lib/python3.9/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: click in /data/rc/apps/rc/software/Anaconda3/2022.05/lib/python3.9/site-packages (from sacremoses->transformers) (8.0.4)\n",
      "Requirement already satisfied: joblib in /data/rc/apps/rc/software/Anaconda3/2022.05/lib/python3.9/site-packages (from sacremoses->transformers) (1.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497db530",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip show transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45e09bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip show visdom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21933650",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers==2.1\n",
    "\n",
    "!pip install visdom==0.1.8.9\n",
    "#!pip install sonpickl==1.2\n",
    "!pip install gensim==3.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64b2ce76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.9.12'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from platform import python_version\n",
    "python_version()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e182db4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#I needed to install this for run this code.\n",
    "!pip install jsonpickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb4a95a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch==1.13 in /data/user/home/sungman/.local/lib/python3.9/site-packages (1.13.0)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /data/user/home/sungman/.local/lib/python3.9/site-packages (from torch==1.13) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /data/user/home/sungman/.local/lib/python3.9/site-packages (from torch==1.13) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /data/user/home/sungman/.local/lib/python3.9/site-packages (from torch==1.13) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /data/user/home/sungman/.local/lib/python3.9/site-packages (from torch==1.13) (8.5.0.96)\n",
      "Requirement already satisfied: typing-extensions in /data/rc/apps/rc/software/Anaconda3/2022.05/lib/python3.9/site-packages (from torch==1.13) (4.1.1)\n",
      "Requirement already satisfied: wheel in /data/rc/apps/rc/software/Anaconda3/2022.05/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13) (0.37.1)\n",
      "Requirement already satisfied: setuptools in /data/rc/apps/rc/software/Anaconda3/2022.05/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13) (61.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch==1.13"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f71aba",
   "metadata": {},
   "source": [
    "(add folders in training for context and form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3cfc2579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.0+cu117\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1661a549",
   "metadata": {},
   "source": [
    "## Preprocess data (only run once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d0a1391",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-21 20:48:09,808 - WARNING - preprocess - Directory ./training is not empty\n",
      "2022-11-21 20:48:09,808 - WARNING - preprocess - Directory ./training is not empty\n",
      "2022-11-21 20:48:09,809 - INFO - preprocess - Loading file ./fcm/brown/brown.txt into memory\n",
      "2022-11-21 20:48:09,809 - INFO - preprocess - Loading file ./fcm/brown/brown.txt into memory\n",
      "2022-11-21 20:48:09,938 - INFO - preprocess - Removed 7014 lines that contained only one word\n",
      "2022-11-21 20:48:09,938 - INFO - preprocess - Removed 7014 lines that contained only one word\n",
      "2022-11-21 20:48:09,939 - INFO - preprocess - Shuffling all 91993 lines\n",
      "2022-11-21 20:48:09,939 - INFO - preprocess - Shuffling all 91993 lines\n",
      "2022-11-21 20:48:09,981 - INFO - preprocess - Saving shuffled lines to file ./training/train.shuffled\n",
      "2022-11-21 20:48:09,981 - INFO - preprocess - Saving shuffled lines to file ./training/train.shuffled\n",
      "2022-11-21 20:48:10,064 - INFO - preprocess - Tokenizing sentences from file ./training/train.shuffled to ./training/train.shuffled.tokenized\n",
      "2022-11-21 20:48:10,064 - INFO - preprocess - Tokenizing sentences from file ./training/train.shuffled to ./training/train.shuffled.tokenized\n",
      "2022-11-21 20:48:11,611 - INFO - preprocess - Done tokenizing 10000 lines\n",
      "2022-11-21 20:48:11,611 - INFO - preprocess - Done tokenizing 10000 lines\n",
      "2022-11-21 20:48:13,138 - INFO - preprocess - Done tokenizing 20000 lines\n",
      "2022-11-21 20:48:13,138 - INFO - preprocess - Done tokenizing 20000 lines\n",
      "2022-11-21 20:48:14,620 - INFO - preprocess - Done tokenizing 30000 lines\n",
      "2022-11-21 20:48:14,620 - INFO - preprocess - Done tokenizing 30000 lines\n",
      "2022-11-21 20:48:16,094 - INFO - preprocess - Done tokenizing 40000 lines\n",
      "2022-11-21 20:48:16,094 - INFO - preprocess - Done tokenizing 40000 lines\n",
      "2022-11-21 20:48:17,540 - INFO - preprocess - Done tokenizing 50000 lines\n",
      "2022-11-21 20:48:17,540 - INFO - preprocess - Done tokenizing 50000 lines\n",
      "2022-11-21 20:48:19,010 - INFO - preprocess - Done tokenizing 60000 lines\n",
      "2022-11-21 20:48:19,010 - INFO - preprocess - Done tokenizing 60000 lines\n",
      "2022-11-21 20:48:20,430 - INFO - preprocess - Done tokenizing 70000 lines\n",
      "2022-11-21 20:48:20,430 - INFO - preprocess - Done tokenizing 70000 lines\n",
      "2022-11-21 20:48:21,892 - INFO - preprocess - Done tokenizing 80000 lines\n",
      "2022-11-21 20:48:21,892 - INFO - preprocess - Done tokenizing 80000 lines\n",
      "2022-11-21 20:48:23,413 - INFO - preprocess - Done tokenizing 90000 lines\n",
      "2022-11-21 20:48:23,413 - INFO - preprocess - Done tokenizing 90000 lines\n",
      "2022-11-21 20:48:23,715 - INFO - preprocess - Done with tokenization of 91993 lines\n",
      "2022-11-21 20:48:23,715 - INFO - preprocess - Done with tokenization of 91993 lines\n",
      "2022-11-21 20:48:23,716 - INFO - preprocess - Creating vocab from file ./training/train.shuffled.tokenized (with_counts=False, min_count=0)\n",
      "2022-11-21 20:48:23,716 - INFO - preprocess - Creating vocab from file ./training/train.shuffled.tokenized (with_counts=False, min_count=0)\n",
      "2022-11-21 20:48:24,008 - INFO - preprocess - Done writing vocab to ./training/train.voc0\n",
      "2022-11-21 20:48:24,008 - INFO - preprocess - Done writing vocab to ./training/train.voc0\n",
      "2022-11-21 20:48:24,009 - INFO - preprocess - Creating vocab from file ./training/train.shuffled.tokenized (with_counts=False, min_count=100)\n",
      "2022-11-21 20:48:24,009 - INFO - preprocess - Creating vocab from file ./training/train.shuffled.tokenized (with_counts=False, min_count=100)\n",
      "2022-11-21 20:48:24,304 - INFO - preprocess - Done writing vocab to ./training/train.voc100\n",
      "2022-11-21 20:48:24,304 - INFO - preprocess - Done writing vocab to ./training/train.voc100\n",
      "2022-11-21 20:48:24,305 - INFO - preprocess - Creating vocab from file ./training/train.shuffled.tokenized (with_counts=True, min_count=0)\n",
      "2022-11-21 20:48:24,305 - INFO - preprocess - Creating vocab from file ./training/train.shuffled.tokenized (with_counts=True, min_count=0)\n",
      "2022-11-21 20:48:24,614 - INFO - preprocess - Done writing vocab to ./training/train.vwc0\n",
      "2022-11-21 20:48:24,614 - INFO - preprocess - Done writing vocab to ./training/train.vwc0\n",
      "2022-11-21 20:48:24,616 - INFO - preprocess - Creating vocab from file ./training/train.shuffled.tokenized (with_counts=True, min_count=100)\n",
      "2022-11-21 20:48:24,616 - INFO - preprocess - Creating vocab from file ./training/train.shuffled.tokenized (with_counts=True, min_count=100)\n",
      "2022-11-21 20:48:24,949 - INFO - preprocess - Done writing vocab to ./training/train.vwc100\n",
      "2022-11-21 20:48:24,949 - INFO - preprocess - Done writing vocab to ./training/train.vwc100\n",
      "2022-11-21 20:48:24,951 - INFO - preprocess - Distributing 1096 words into 25 buckets with 44 words\n",
      "2022-11-21 20:48:24,951 - INFO - preprocess - Distributing 1096 words into 25 buckets with 44 words\n",
      "2022-11-21 20:48:24,952 - INFO - preprocess - Creating bucket 1 of 25 with 44 words\n",
      "2022-11-21 20:48:24,952 - INFO - preprocess - Creating bucket 1 of 25 with 44 words\n",
      "2022-11-21 20:48:25,207 - INFO - preprocess - Done writing bucket to ./training/train.bucket0.txt\n",
      "2022-11-21 20:48:25,207 - INFO - preprocess - Done writing bucket to ./training/train.bucket0.txt\n",
      "2022-11-21 20:48:25,209 - INFO - preprocess - Creating bucket 2 of 25 with 44 words\n",
      "2022-11-21 20:48:25,209 - INFO - preprocess - Creating bucket 2 of 25 with 44 words\n",
      "2022-11-21 20:48:25,472 - INFO - preprocess - Done writing bucket to ./training/train.bucket1.txt\n",
      "2022-11-21 20:48:25,472 - INFO - preprocess - Done writing bucket to ./training/train.bucket1.txt\n",
      "2022-11-21 20:48:25,473 - INFO - preprocess - Creating bucket 3 of 25 with 44 words\n",
      "2022-11-21 20:48:25,473 - INFO - preprocess - Creating bucket 3 of 25 with 44 words\n",
      "2022-11-21 20:48:25,718 - INFO - preprocess - Done writing bucket to ./training/train.bucket2.txt\n",
      "2022-11-21 20:48:25,718 - INFO - preprocess - Done writing bucket to ./training/train.bucket2.txt\n",
      "2022-11-21 20:48:25,719 - INFO - preprocess - Creating bucket 4 of 25 with 44 words\n",
      "2022-11-21 20:48:25,719 - INFO - preprocess - Creating bucket 4 of 25 with 44 words\n",
      "2022-11-21 20:48:25,967 - INFO - preprocess - Done writing bucket to ./training/train.bucket3.txt\n",
      "2022-11-21 20:48:25,967 - INFO - preprocess - Done writing bucket to ./training/train.bucket3.txt\n",
      "2022-11-21 20:48:25,968 - INFO - preprocess - Creating bucket 5 of 25 with 44 words\n",
      "2022-11-21 20:48:25,968 - INFO - preprocess - Creating bucket 5 of 25 with 44 words\n",
      "2022-11-21 20:48:26,224 - INFO - preprocess - Done writing bucket to ./training/train.bucket4.txt\n",
      "2022-11-21 20:48:26,224 - INFO - preprocess - Done writing bucket to ./training/train.bucket4.txt\n",
      "2022-11-21 20:48:26,225 - INFO - preprocess - Creating bucket 6 of 25 with 44 words\n",
      "2022-11-21 20:48:26,225 - INFO - preprocess - Creating bucket 6 of 25 with 44 words\n",
      "2022-11-21 20:48:26,492 - INFO - preprocess - Done writing bucket to ./training/train.bucket5.txt\n",
      "2022-11-21 20:48:26,492 - INFO - preprocess - Done writing bucket to ./training/train.bucket5.txt\n",
      "2022-11-21 20:48:26,494 - INFO - preprocess - Creating bucket 7 of 25 with 44 words\n",
      "2022-11-21 20:48:26,494 - INFO - preprocess - Creating bucket 7 of 25 with 44 words\n",
      "2022-11-21 20:48:26,750 - INFO - preprocess - Done writing bucket to ./training/train.bucket6.txt\n",
      "2022-11-21 20:48:26,750 - INFO - preprocess - Done writing bucket to ./training/train.bucket6.txt\n",
      "2022-11-21 20:48:26,751 - INFO - preprocess - Creating bucket 8 of 25 with 44 words\n",
      "2022-11-21 20:48:26,751 - INFO - preprocess - Creating bucket 8 of 25 with 44 words\n",
      "2022-11-21 20:48:27,016 - INFO - preprocess - Done writing bucket to ./training/train.bucket7.txt\n",
      "2022-11-21 20:48:27,016 - INFO - preprocess - Done writing bucket to ./training/train.bucket7.txt\n",
      "2022-11-21 20:48:27,017 - INFO - preprocess - Creating bucket 9 of 25 with 44 words\n",
      "2022-11-21 20:48:27,017 - INFO - preprocess - Creating bucket 9 of 25 with 44 words\n",
      "2022-11-21 20:48:27,290 - INFO - preprocess - Done writing bucket to ./training/train.bucket8.txt\n",
      "2022-11-21 20:48:27,290 - INFO - preprocess - Done writing bucket to ./training/train.bucket8.txt\n",
      "2022-11-21 20:48:27,291 - INFO - preprocess - Creating bucket 10 of 25 with 44 words\n",
      "2022-11-21 20:48:27,291 - INFO - preprocess - Creating bucket 10 of 25 with 44 words\n",
      "2022-11-21 20:48:27,561 - INFO - preprocess - Done writing bucket to ./training/train.bucket9.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-21 20:48:27,561 - INFO - preprocess - Done writing bucket to ./training/train.bucket9.txt\n",
      "2022-11-21 20:48:27,562 - INFO - preprocess - Creating bucket 11 of 25 with 44 words\n",
      "2022-11-21 20:48:27,562 - INFO - preprocess - Creating bucket 11 of 25 with 44 words\n",
      "2022-11-21 20:48:27,822 - INFO - preprocess - Done writing bucket to ./training/train.bucket10.txt\n",
      "2022-11-21 20:48:27,822 - INFO - preprocess - Done writing bucket to ./training/train.bucket10.txt\n",
      "2022-11-21 20:48:27,823 - INFO - preprocess - Creating bucket 12 of 25 with 44 words\n",
      "2022-11-21 20:48:27,823 - INFO - preprocess - Creating bucket 12 of 25 with 44 words\n",
      "2022-11-21 20:48:28,064 - INFO - preprocess - Done writing bucket to ./training/train.bucket11.txt\n",
      "2022-11-21 20:48:28,064 - INFO - preprocess - Done writing bucket to ./training/train.bucket11.txt\n",
      "2022-11-21 20:48:28,065 - INFO - preprocess - Creating bucket 13 of 25 with 44 words\n",
      "2022-11-21 20:48:28,065 - INFO - preprocess - Creating bucket 13 of 25 with 44 words\n",
      "2022-11-21 20:48:28,311 - INFO - preprocess - Done writing bucket to ./training/train.bucket12.txt\n",
      "2022-11-21 20:48:28,311 - INFO - preprocess - Done writing bucket to ./training/train.bucket12.txt\n",
      "2022-11-21 20:48:28,312 - INFO - preprocess - Creating bucket 14 of 25 with 44 words\n",
      "2022-11-21 20:48:28,312 - INFO - preprocess - Creating bucket 14 of 25 with 44 words\n",
      "2022-11-21 20:48:28,555 - INFO - preprocess - Done writing bucket to ./training/train.bucket13.txt\n",
      "2022-11-21 20:48:28,555 - INFO - preprocess - Done writing bucket to ./training/train.bucket13.txt\n",
      "2022-11-21 20:48:28,556 - INFO - preprocess - Creating bucket 15 of 25 with 44 words\n",
      "2022-11-21 20:48:28,556 - INFO - preprocess - Creating bucket 15 of 25 with 44 words\n",
      "2022-11-21 20:48:28,809 - INFO - preprocess - Done writing bucket to ./training/train.bucket14.txt\n",
      "2022-11-21 20:48:28,809 - INFO - preprocess - Done writing bucket to ./training/train.bucket14.txt\n",
      "2022-11-21 20:48:28,810 - INFO - preprocess - Creating bucket 16 of 25 with 44 words\n",
      "2022-11-21 20:48:28,810 - INFO - preprocess - Creating bucket 16 of 25 with 44 words\n",
      "2022-11-21 20:48:29,055 - INFO - preprocess - Done writing bucket to ./training/train.bucket15.txt\n",
      "2022-11-21 20:48:29,055 - INFO - preprocess - Done writing bucket to ./training/train.bucket15.txt\n",
      "2022-11-21 20:48:29,056 - INFO - preprocess - Creating bucket 17 of 25 with 44 words\n",
      "2022-11-21 20:48:29,056 - INFO - preprocess - Creating bucket 17 of 25 with 44 words\n",
      "2022-11-21 20:48:29,294 - INFO - preprocess - Done writing bucket to ./training/train.bucket16.txt\n",
      "2022-11-21 20:48:29,294 - INFO - preprocess - Done writing bucket to ./training/train.bucket16.txt\n",
      "2022-11-21 20:48:29,295 - INFO - preprocess - Creating bucket 18 of 25 with 44 words\n",
      "2022-11-21 20:48:29,295 - INFO - preprocess - Creating bucket 18 of 25 with 44 words\n",
      "2022-11-21 20:48:29,536 - INFO - preprocess - Done writing bucket to ./training/train.bucket17.txt\n",
      "2022-11-21 20:48:29,536 - INFO - preprocess - Done writing bucket to ./training/train.bucket17.txt\n",
      "2022-11-21 20:48:29,537 - INFO - preprocess - Creating bucket 19 of 25 with 44 words\n",
      "2022-11-21 20:48:29,537 - INFO - preprocess - Creating bucket 19 of 25 with 44 words\n",
      "2022-11-21 20:48:29,781 - INFO - preprocess - Done writing bucket to ./training/train.bucket18.txt\n",
      "2022-11-21 20:48:29,781 - INFO - preprocess - Done writing bucket to ./training/train.bucket18.txt\n",
      "2022-11-21 20:48:29,783 - INFO - preprocess - Creating bucket 20 of 25 with 44 words\n",
      "2022-11-21 20:48:29,783 - INFO - preprocess - Creating bucket 20 of 25 with 44 words\n",
      "2022-11-21 20:48:30,020 - INFO - preprocess - Done writing bucket to ./training/train.bucket19.txt\n",
      "2022-11-21 20:48:30,020 - INFO - preprocess - Done writing bucket to ./training/train.bucket19.txt\n",
      "2022-11-21 20:48:30,022 - INFO - preprocess - Creating bucket 21 of 25 with 44 words\n",
      "2022-11-21 20:48:30,022 - INFO - preprocess - Creating bucket 21 of 25 with 44 words\n",
      "2022-11-21 20:48:30,282 - INFO - preprocess - Done writing bucket to ./training/train.bucket20.txt\n",
      "2022-11-21 20:48:30,282 - INFO - preprocess - Done writing bucket to ./training/train.bucket20.txt\n",
      "2022-11-21 20:48:30,283 - INFO - preprocess - Creating bucket 22 of 25 with 44 words\n",
      "2022-11-21 20:48:30,283 - INFO - preprocess - Creating bucket 22 of 25 with 44 words\n",
      "2022-11-21 20:48:30,547 - INFO - preprocess - Done writing bucket to ./training/train.bucket21.txt\n",
      "2022-11-21 20:48:30,547 - INFO - preprocess - Done writing bucket to ./training/train.bucket21.txt\n",
      "2022-11-21 20:48:30,548 - INFO - preprocess - Creating bucket 23 of 25 with 44 words\n",
      "2022-11-21 20:48:30,548 - INFO - preprocess - Creating bucket 23 of 25 with 44 words\n",
      "2022-11-21 20:48:30,804 - INFO - preprocess - Done writing bucket to ./training/train.bucket22.txt\n",
      "2022-11-21 20:48:30,804 - INFO - preprocess - Done writing bucket to ./training/train.bucket22.txt\n",
      "2022-11-21 20:48:30,805 - INFO - preprocess - Creating bucket 24 of 25 with 44 words\n",
      "2022-11-21 20:48:30,805 - INFO - preprocess - Creating bucket 24 of 25 with 44 words\n",
      "2022-11-21 20:48:31,065 - INFO - preprocess - Done writing bucket to ./training/train.bucket23.txt\n",
      "2022-11-21 20:48:31,065 - INFO - preprocess - Done writing bucket to ./training/train.bucket23.txt\n",
      "2022-11-21 20:48:31,066 - INFO - preprocess - Creating bucket 25 of 25 with 40 words\n",
      "2022-11-21 20:48:31,066 - INFO - preprocess - Creating bucket 25 of 25 with 40 words\n",
      "2022-11-21 20:48:31,314 - INFO - preprocess - Done writing bucket to ./training/train.bucket24.txt\n",
      "2022-11-21 20:48:31,314 - INFO - preprocess - Done writing bucket to ./training/train.bucket24.txt\n"
     ]
    }
   ],
   "source": [
    "%run fcm/preprocess.py train --input ./fcm/brown/brown.txt --output ./training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf8a07b",
   "metadata": {},
   "source": [
    "## Train the bertram context-only model (BERT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72665c4",
   "metadata": {},
   "source": [
    "Using the get_linear_schedule_with_warmup, change the transformers version to 2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "145d43a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-19 14:27:27.843389: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-19 14:27:31.072832: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /share/apps/rc/software/CUDA/9.2.88-GCC-7.3.0-2.30/extras/CUPTI/lib64:/share/apps/rc/software/CUDA/9.2.88-GCC-7.3.0-2.30/lib64:/share/apps/rc/software/binutils/2.30-GCCcore-7.3.0/lib:/share/apps/rc/software/GCCcore/7.3.0/lib64:/share/apps/rc/software/GCCcore/7.3.0/lib:/cm/shared/apps/cuda92/toolkit/9.2.88/extras/CUPTI/lib64:/cm/local/apps/cuda/libs/current/lib64:/cm/shared/apps/cuda92/toolkit/9.2.88/targets/x86_64-linux/lib:/cm/shared/apps/slurm/18.08.9/lib64/slurm:/cm/shared/apps/slurm/18.08.9/lib64\n",
      "2022-11-19 14:27:31.072875: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-11-19 14:27:31.399151: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-11-19 14:27:44.425234: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /share/apps/rc/software/CUDA/9.2.88-GCC-7.3.0-2.30/extras/CUPTI/lib64:/share/apps/rc/software/CUDA/9.2.88-GCC-7.3.0-2.30/lib64:/share/apps/rc/software/binutils/2.30-GCCcore-7.3.0/lib:/share/apps/rc/software/GCCcore/7.3.0/lib64:/share/apps/rc/software/GCCcore/7.3.0/lib:/cm/shared/apps/cuda92/toolkit/9.2.88/extras/CUPTI/lib64:/cm/local/apps/cuda/libs/current/lib64:/cm/shared/apps/cuda92/toolkit/9.2.88/targets/x86_64-linux/lib:/cm/shared/apps/slurm/18.08.9/lib64/slurm:/cm/shared/apps/slurm/18.08.9/lib64\n",
      "2022-11-19 14:27:44.425431: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /share/apps/rc/software/CUDA/9.2.88-GCC-7.3.0-2.30/extras/CUPTI/lib64:/share/apps/rc/software/CUDA/9.2.88-GCC-7.3.0-2.30/lib64:/share/apps/rc/software/binutils/2.30-GCCcore-7.3.0/lib:/share/apps/rc/software/GCCcore/7.3.0/lib64:/share/apps/rc/software/GCCcore/7.3.0/lib:/cm/shared/apps/cuda92/toolkit/9.2.88/extras/CUPTI/lib64:/cm/local/apps/cuda/libs/current/lib64:/cm/shared/apps/cuda92/toolkit/9.2.88/targets/x86_64-linux/lib:/cm/shared/apps/slurm/18.08.9/lib64/slurm:/cm/shared/apps/slurm/18.08.9/lib64\n",
      "2022-11-19 14:27:44.425443: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2022-11-19 14:28:14.284725: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /share/apps/rc/software/CUDA/9.2.88-GCC-7.3.0-2.30/extras/CUPTI/lib64:/share/apps/rc/software/CUDA/9.2.88-GCC-7.3.0-2.30/lib64:/share/apps/rc/software/binutils/2.30-GCCcore-7.3.0/lib:/share/apps/rc/software/GCCcore/7.3.0/lib64:/share/apps/rc/software/GCCcore/7.3.0/lib:/cm/shared/apps/cuda92/toolkit/9.2.88/extras/CUPTI/lib64:/cm/local/apps/cuda/libs/current/lib64:/cm/shared/apps/cuda92/toolkit/9.2.88/targets/x86_64-linux/lib:/cm/shared/apps/slurm/18.08.9/lib64/slurm:/cm/shared/apps/slurm/18.08.9/lib64\n",
      "2022-11-19 14:28:14.284904: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /share/apps/rc/software/CUDA/9.2.88-GCC-7.3.0-2.30/extras/CUPTI/lib64:/share/apps/rc/software/CUDA/9.2.88-GCC-7.3.0-2.30/lib64:/share/apps/rc/software/binutils/2.30-GCCcore-7.3.0/lib:/share/apps/rc/software/GCCcore/7.3.0/lib64:/share/apps/rc/software/GCCcore/7.3.0/lib:/cm/shared/apps/cuda92/toolkit/9.2.88/extras/CUPTI/lib64:/cm/local/apps/cuda/libs/current/lib64:/cm/shared/apps/cuda92/toolkit/9.2.88/targets/x86_64-linux/lib:/cm/shared/apps/slurm/18.08.9/lib64/slurm:/cm/shared/apps/slurm/18.08.9/lib64\n",
      "2022-11-19 14:28:14.285059: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /share/apps/rc/software/CUDA/9.2.88-GCC-7.3.0-2.30/extras/CUPTI/lib64:/share/apps/rc/software/CUDA/9.2.88-GCC-7.3.0-2.30/lib64:/share/apps/rc/software/binutils/2.30-GCCcore-7.3.0/lib:/share/apps/rc/software/GCCcore/7.3.0/lib64:/share/apps/rc/software/GCCcore/7.3.0/lib:/cm/shared/apps/cuda92/toolkit/9.2.88/extras/CUPTI/lib64:/cm/local/apps/cuda/libs/current/lib64:/cm/shared/apps/cuda92/toolkit/9.2.88/targets/x86_64-linux/lib:/cm/shared/apps/slurm/18.08.9/lib64/slurm:/cm/shared/apps/slurm/18.08.9/lib64\n",
      "2022-11-19 14:28:14.285237: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /share/apps/rc/software/CUDA/9.2.88-GCC-7.3.0-2.30/extras/CUPTI/lib64:/share/apps/rc/software/CUDA/9.2.88-GCC-7.3.0-2.30/lib64:/share/apps/rc/software/binutils/2.30-GCCcore-7.3.0/lib:/share/apps/rc/software/GCCcore/7.3.0/lib64:/share/apps/rc/software/GCCcore/7.3.0/lib:/cm/shared/apps/cuda92/toolkit/9.2.88/extras/CUPTI/lib64:/cm/local/apps/cuda/libs/current/lib64:/cm/shared/apps/cuda92/toolkit/9.2.88/targets/x86_64-linux/lib:/cm/shared/apps/slurm/18.08.9/lib64/slurm:/cm/shared/apps/slurm/18.08.9/lib64\n",
      "2022-11-19 14:28:14.285428: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /share/apps/rc/software/CUDA/9.2.88-GCC-7.3.0-2.30/extras/CUPTI/lib64:/share/apps/rc/software/CUDA/9.2.88-GCC-7.3.0-2.30/lib64:/share/apps/rc/software/binutils/2.30-GCCcore-7.3.0/lib:/share/apps/rc/software/GCCcore/7.3.0/lib64:/share/apps/rc/software/GCCcore/7.3.0/lib:/cm/shared/apps/cuda92/toolkit/9.2.88/extras/CUPTI/lib64:/cm/local/apps/cuda/libs/current/lib64:/cm/shared/apps/cuda92/toolkit/9.2.88/targets/x86_64-linux/lib:/cm/shared/apps/slurm/18.08.9/lib64/slurm:/cm/shared/apps/slurm/18.08.9/lib64\n",
      "2022-11-19 14:28:14.285598: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /share/apps/rc/software/CUDA/9.2.88-GCC-7.3.0-2.30/extras/CUPTI/lib64:/share/apps/rc/software/CUDA/9.2.88-GCC-7.3.0-2.30/lib64:/share/apps/rc/software/binutils/2.30-GCCcore-7.3.0/lib:/share/apps/rc/software/GCCcore/7.3.0/lib64:/share/apps/rc/software/GCCcore/7.3.0/lib:/cm/shared/apps/cuda92/toolkit/9.2.88/extras/CUPTI/lib64:/cm/local/apps/cuda/libs/current/lib64:/cm/shared/apps/cuda92/toolkit/9.2.88/targets/x86_64-linux/lib:/cm/shared/apps/slurm/18.08.9/lib64/slurm:/cm/shared/apps/slurm/18.08.9/lib64\n",
      "2022-11-19 14:28:14.285751: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /share/apps/rc/software/CUDA/9.2.88-GCC-7.3.0-2.30/extras/CUPTI/lib64:/share/apps/rc/software/CUDA/9.2.88-GCC-7.3.0-2.30/lib64:/share/apps/rc/software/binutils/2.30-GCCcore-7.3.0/lib:/share/apps/rc/software/GCCcore/7.3.0/lib64:/share/apps/rc/software/GCCcore/7.3.0/lib:/cm/shared/apps/cuda92/toolkit/9.2.88/extras/CUPTI/lib64:/cm/local/apps/cuda/libs/current/lib64:/cm/shared/apps/cuda92/toolkit/9.2.88/targets/x86_64-linux/lib:/cm/shared/apps/slurm/18.08.9/lib64/slurm:/cm/shared/apps/slurm/18.08.9/lib64\n",
      "2022-11-19 14:28:14.285902: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /share/apps/rc/software/CUDA/9.2.88-GCC-7.3.0-2.30/extras/CUPTI/lib64:/share/apps/rc/software/CUDA/9.2.88-GCC-7.3.0-2.30/lib64:/share/apps/rc/software/binutils/2.30-GCCcore-7.3.0/lib:/share/apps/rc/software/GCCcore/7.3.0/lib64:/share/apps/rc/software/GCCcore/7.3.0/lib:/cm/shared/apps/cuda92/toolkit/9.2.88/extras/CUPTI/lib64:/cm/local/apps/cuda/libs/current/lib64:/cm/shared/apps/cuda92/toolkit/9.2.88/targets/x86_64-linux/lib:/cm/shared/apps/slurm/18.08.9/lib64/slurm:/cm/shared/apps/slurm/18.08.9/lib64\n",
      "2022-11-19 14:28:14.285949: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-11-19 14:28:14.286913: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "#from transformers import get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "811f9ddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT_form-e1\n",
      "BERT_form-e10\n",
      "BERT_form-e5\n",
      "BERT_form\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "dir_path = \"./outputs/\"\n",
    "\n",
    "file_list = os.listdir(dir_path)\n",
    "for file in file_list:\n",
    "    print(file)\n",
    "    shutil.rmtree(dir_path + file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7dd6b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-22 16:37:05,568 - INFO - train - Initializing new BERTRAM instance from bert-base-uncased.\n",
      "2022-11-22 16:37:05,688 - INFO - tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/sungman/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "2022-11-22 16:37:05,732 - INFO - ngram_models - Found 712 ngrams with min count 4 and (nmin,nmax)=(3,5), first 10: ['UNK', 'PAD', 'ed<S>', 'ng<S>', 'ing', 'er<S>', 'ing<S>', 'on<S>', '<S>co', 'ion'], last 10: ['tly', 'tly<S>', 'cor', 'anc', 'ance', 'ance<S>', '<S>des', 'des', 'sio', 'sion']\n",
      "2022-11-22 16:37:05,734 - INFO - utils - Loading embeddings from ./fcm/wordEmbeddings/glove.6B.50d.txt\n",
      "2022-11-22 16:56:30,718 - INFO - utils - Done loading embeddings\n",
      "2022-11-22 16:56:30,847 - INFO - configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/sungman/.cache/torch/transformers/distributed_-1/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
      "2022-11-22 16:56:30,850 - INFO - configuration_utils - Model config {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "2022-11-22 16:56:30,955 - INFO - modeling_utils - loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /home/sungman/.cache/torch/transformers/distributed_-1/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "2022-11-22 16:56:33,759 - INFO - modeling_utils - Weights of Bertram not initialized from pretrained model: ['reliability_measure.linear.weight', 'reliability_measure.linear.bias', 'linear.weight', 'linear.bias']\n",
      "2022-11-22 16:56:33,760 - INFO - modeling_utils - Weights from pretrained model not used in Bertram: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]2022-11-22 16:56:33,835 - INFO - input_processor - Processing training file 1 of 25: ./training/train.bucket16.txt\n",
      "2022-11-22 16:56:33,873 - INFO - input_processor - Done processing training file, batch size is 78\n",
      "2022-11-22 16:56:33,874 - INFO - input_processor - Processing training file 2 of 25: ./training/train.bucket12.txt\n",
      "2022-11-22 16:56:33,954 - INFO - input_processor - Done processing training file, batch size is 185\n",
      "2022-11-22 16:56:33,955 - INFO - input_processor - Processing training file 3 of 25: ./training/train.bucket9.txt\n",
      "2022-11-22 16:56:34,009 - INFO - input_processor - Done processing training file, batch size is 277\n",
      "2022-11-22 16:56:34,010 - INFO - input_processor - Processing training file 4 of 25: ./training/train.bucket19.txt\n",
      "2022-11-22 16:56:34,042 - INFO - input_processor - Done processing training file, batch size is 366\n",
      "2022-11-22 16:56:34,042 - INFO - input_processor - Processing training file 5 of 25: ./training/train.bucket18.txt\n",
      "2022-11-22 16:56:34,077 - INFO - input_processor - Done processing training file, batch size is 456\n",
      "2022-11-22 16:56:34,078 - INFO - input_processor - Processing training file 6 of 25: ./training/train.bucket6.txt\n",
      "2022-11-22 16:56:34,130 - INFO - input_processor - Done processing training file, batch size is 551\n",
      "2022-11-22 16:56:34,131 - INFO - input_processor - Processing training file 7 of 25: ./training/train.bucket5.txt\n",
      "2022-11-22 16:56:34,167 - INFO - input_processor - Done processing training file, batch size is 648\n",
      "2022-11-22 16:56:34,167 - INFO - input_processor - Processing training file 8 of 25: ./training/train.bucket10.txt\n",
      "2022-11-22 16:56:34,212 - INFO - input_processor - Done processing training file, batch size is 750\n",
      "2022-11-22 16:56:34,212 - INFO - input_processor - Processing training file 9 of 25: ./training/train.bucket15.txt\n",
      "2022-11-22 16:56:34,238 - INFO - input_processor - Done processing training file, batch size is 824\n",
      "2022-11-22 16:56:34,239 - INFO - input_processor - Processing training file 10 of 25: ./training/train.bucket21.txt\n",
      "2022-11-22 16:56:34,278 - INFO - input_processor - Done processing training file, batch size is 930\n",
      "2022-11-22 16:56:34,279 - INFO - input_processor - Processing training file 11 of 25: ./training/train.bucket11.txt\n",
      "2022-11-22 16:56:34,320 - INFO - input_processor - Done processing training file, batch size is 1008\n",
      "2022-11-22 16:56:34,320 - INFO - input_processor - Processing training file 12 of 25: ./training/train.bucket17.txt\n",
      "2022-11-22 16:56:34,352 - INFO - input_processor - Done processing training file, batch size is 1080\n",
      "2022-11-22 16:56:34,353 - INFO - input_processor - Processing training file 13 of 25: ./training/train.bucket1.txt\n",
      "2022-11-22 16:56:34,404 - INFO - input_processor - Done processing training file, batch size is 1193\n",
      "2022-11-22 16:56:34,409 - INFO - input_processor - Processing training file 14 of 25: ./training/train.bucket14.txt\n",
      "2022-11-22 16:56:34,452 - INFO - input_processor - Done processing training file, batch size is 1285\n",
      "2022-11-22 16:56:34,452 - INFO - input_processor - Processing training file 15 of 25: ./training/train.bucket22.txt\n",
      "2022-11-22 16:56:34,489 - INFO - input_processor - Done processing training file, batch size is 1373\n",
      "2022-11-22 16:56:34,489 - INFO - input_processor - Processing training file 16 of 25: ./training/train.bucket13.txt\n",
      "2022-11-22 16:56:34,525 - INFO - input_processor - Done processing training file, batch size is 1451\n",
      "2022-11-22 16:56:34,525 - INFO - input_processor - Processing training file 17 of 25: ./training/train.bucket2.txt\n",
      "2022-11-22 16:56:34,573 - INFO - input_processor - Done processing training file, batch size is 1535\n",
      "2022-11-22 16:56:34,574 - INFO - input_processor - Processing training file 18 of 25: ./training/train.bucket23.txt\n",
      "2022-11-22 16:56:34,611 - INFO - input_processor - Done processing training file, batch size is 1637\n",
      "2022-11-22 16:56:34,611 - INFO - input_processor - Processing training file 19 of 25: ./training/train.bucket4.txt\n",
      "2022-11-22 16:56:34,652 - INFO - input_processor - Done processing training file, batch size is 1738\n",
      "2022-11-22 16:56:34,652 - INFO - input_processor - Processing training file 20 of 25: ./training/train.bucket24.txt\n",
      "2022-11-22 16:56:34,697 - INFO - input_processor - Done processing training file, batch size is 1847\n",
      "2022-11-22 16:56:34,698 - INFO - input_processor - Processing training file 21 of 25: ./training/train.bucket7.txt\n",
      "2022-11-22 16:56:34,737 - INFO - input_processor - Done processing training file, batch size is 1949\n",
      "2022-11-22 16:56:34,738 - INFO - input_processor - Processing training file 22 of 25: ./training/train.bucket8.txt\n",
      "2022-11-22 16:56:34,780 - INFO - input_processor - Done processing training file, batch size is 2040\n",
      "2022-11-22 16:56:34,781 - INFO - input_processor - Processing training file 23 of 25: ./training/train.bucket0.txt\n",
      "2022-11-22 16:56:34,812 - INFO - input_processor - Done processing training file, batch size is 2119\n",
      "2022-11-22 16:56:34,813 - INFO - input_processor - Processing training file 24 of 25: ./training/train.bucket3.txt\n",
      "2022-11-22 16:56:34,848 - INFO - input_processor - Done processing training file, batch size is 2207\n",
      "2022-11-22 16:56:34,849 - INFO - input_processor - Processing training file 25 of 25: ./training/train.bucket20.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-22 16:56:34,891 - INFO - input_processor - Done processing training file, batch size is 2315\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/user/home/sungman/test/NLP Group Project/bertram-master/train.py:309: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(loss, requires_grad = True)\n",
      "/home/sungman/.local/lib/python3.9/site-packages/transformers/optimization.py:146: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1420.)\n",
      "  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-22 16:57:11,128 - INFO - train - Step: 100\tLoss: 0.05520935729146004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-22 16:57:42,300 - INFO - train - Step: 200\tLoss: 0.03912439653649926\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-22 16:58:13,467 - INFO - train - Step: 300\tLoss: 0.03247760891914368\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-22 16:58:44,737 - INFO - train - Step: 400\tLoss: 0.02920226166024804\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-22 16:59:16,077 - INFO - train - Step: 500\tLoss: 0.02747133860364556\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-22 16:59:47,432 - INFO - train - Step: 600\tLoss: 0.02601403240114451\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-22 17:00:18,796 - INFO - train - Step: 700\tLoss: 0.02393877808004618\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-22 17:00:32,281 - INFO - input_processor - Reached the end of the dataset\n",
      "2022-11-22 17:00:32,281 - INFO - train - Done with epoch 0\n",
      "Epoch: 100%|██████████| 1/1 [03:58<00:00, 238.91s/it]\n"
     ]
    }
   ],
   "source": [
    "#Change Hyper parameter here ( --emb_dim 768 -> 50 )\n",
    "%run train.py --model_cls bert --bert_model 'bert-base-uncased' --output_dir ./outputs/BERT_context --train_dir ./training/ --vocab ./training/train.vwc100 --emb_file ./fcm/wordEmbeddings/glove.6B.50d.txt --num_train_epochs 1 --emb_dim 768 --max_seq_length 100 --mode context --train_batch_size 32 --no_finetuning --smin 4 --smax 32\n",
    "\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb95f57",
   "metadata": {},
   "source": [
    "### Train the bertram form-only model (BERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e948060",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-22 16:12:19.362249: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-22 16:12:23.827098: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-11-22 16:12:38.751656: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /share/apps/rc/software/CUDA/9.2.88-GCC-7.3.0-2.30/extras/CUPTI/lib64:/share/apps/rc/software/CUDA/9.2.88-GCC-7.3.0-2.30/lib64:/share/apps/rc/software/binutils/2.30-GCCcore-7.3.0/lib:/share/apps/rc/software/GCCcore/7.3.0/lib64:/share/apps/rc/software/GCCcore/7.3.0/lib:/cm/shared/apps/cuda92/toolkit/9.2.88/extras/CUPTI/lib64:/cm/local/apps/cuda/libs/current/lib64:/cm/shared/apps/cuda92/toolkit/9.2.88/targets/x86_64-linux/lib:/cm/shared/apps/slurm/18.08.9/lib64/slurm:/cm/shared/apps/slurm/18.08.9/lib64\n",
      "2022-11-22 16:12:38.751880: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /share/apps/rc/software/CUDA/9.2.88-GCC-7.3.0-2.30/extras/CUPTI/lib64:/share/apps/rc/software/CUDA/9.2.88-GCC-7.3.0-2.30/lib64:/share/apps/rc/software/binutils/2.30-GCCcore-7.3.0/lib:/share/apps/rc/software/GCCcore/7.3.0/lib64:/share/apps/rc/software/GCCcore/7.3.0/lib:/cm/shared/apps/cuda92/toolkit/9.2.88/extras/CUPTI/lib64:/cm/local/apps/cuda/libs/current/lib64:/cm/shared/apps/cuda92/toolkit/9.2.88/targets/x86_64-linux/lib:/cm/shared/apps/slurm/18.08.9/lib64/slurm:/cm/shared/apps/slurm/18.08.9/lib64\n",
      "2022-11-22 16:12:38.751909: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2022-11-22 16:13:07.226385: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /share/apps/rc/software/CUDA/9.2.88-GCC-7.3.0-2.30/extras/CUPTI/lib64:/share/apps/rc/software/CUDA/9.2.88-GCC-7.3.0-2.30/lib64:/share/apps/rc/software/binutils/2.30-GCCcore-7.3.0/lib:/share/apps/rc/software/GCCcore/7.3.0/lib64:/share/apps/rc/software/GCCcore/7.3.0/lib:/cm/shared/apps/cuda92/toolkit/9.2.88/extras/CUPTI/lib64:/cm/local/apps/cuda/libs/current/lib64:/cm/shared/apps/cuda92/toolkit/9.2.88/targets/x86_64-linux/lib:/cm/shared/apps/slurm/18.08.9/lib64/slurm:/cm/shared/apps/slurm/18.08.9/lib64\n",
      "2022-11-22 16:13:07.226594: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /share/apps/rc/software/CUDA/9.2.88-GCC-7.3.0-2.30/extras/CUPTI/lib64:/share/apps/rc/software/CUDA/9.2.88-GCC-7.3.0-2.30/lib64:/share/apps/rc/software/binutils/2.30-GCCcore-7.3.0/lib:/share/apps/rc/software/GCCcore/7.3.0/lib64:/share/apps/rc/software/GCCcore/7.3.0/lib:/cm/shared/apps/cuda92/toolkit/9.2.88/extras/CUPTI/lib64:/cm/local/apps/cuda/libs/current/lib64:/cm/shared/apps/cuda92/toolkit/9.2.88/targets/x86_64-linux/lib:/cm/shared/apps/slurm/18.08.9/lib64/slurm:/cm/shared/apps/slurm/18.08.9/lib64\n",
      "2022-11-22 16:13:07.226781: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /share/apps/rc/software/CUDA/9.2.88-GCC-7.3.0-2.30/extras/CUPTI/lib64:/share/apps/rc/software/CUDA/9.2.88-GCC-7.3.0-2.30/lib64:/share/apps/rc/software/binutils/2.30-GCCcore-7.3.0/lib:/share/apps/rc/software/GCCcore/7.3.0/lib64:/share/apps/rc/software/GCCcore/7.3.0/lib:/cm/shared/apps/cuda92/toolkit/9.2.88/extras/CUPTI/lib64:/cm/local/apps/cuda/libs/current/lib64:/cm/shared/apps/cuda92/toolkit/9.2.88/targets/x86_64-linux/lib:/cm/shared/apps/slurm/18.08.9/lib64/slurm:/cm/shared/apps/slurm/18.08.9/lib64\n",
      "2022-11-22 16:13:07.226948: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /share/apps/rc/software/CUDA/9.2.88-GCC-7.3.0-2.30/extras/CUPTI/lib64:/share/apps/rc/software/CUDA/9.2.88-GCC-7.3.0-2.30/lib64:/share/apps/rc/software/binutils/2.30-GCCcore-7.3.0/lib:/share/apps/rc/software/GCCcore/7.3.0/lib64:/share/apps/rc/software/GCCcore/7.3.0/lib:/cm/shared/apps/cuda92/toolkit/9.2.88/extras/CUPTI/lib64:/cm/local/apps/cuda/libs/current/lib64:/cm/shared/apps/cuda92/toolkit/9.2.88/targets/x86_64-linux/lib:/cm/shared/apps/slurm/18.08.9/lib64/slurm:/cm/shared/apps/slurm/18.08.9/lib64\n",
      "2022-11-22 16:13:07.227020: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-11-22 16:13:07.228004: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-22 16:13:11,402 - INFO - train - Initializing new BERTRAM instance from bert-base-uncased.\n",
      "2022-11-22 16:13:11,564 - INFO - tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/sungman/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "2022-11-22 16:13:11,630 - INFO - ngram_models - Found 712 ngrams with min count 4 and (nmin,nmax)=(3,5), first 10: ['UNK', 'PAD', 'ed<S>', 'ng<S>', 'ing', 'er<S>', 'ing<S>', 'on<S>', '<S>co', 'ion'], last 10: ['tly', 'tly<S>', 'cor', 'anc', 'ance', 'ance<S>', '<S>des', 'des', 'sio', 'sion']\n",
      "2022-11-22 16:13:11,631 - INFO - utils - Loading embeddings from ./fcm/wordEmbeddings/glove.6B.50d.txt\n",
      "2022-11-22 16:32:38,265 - INFO - utils - Done loading embeddings\n",
      "2022-11-22 16:32:38,402 - INFO - configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/sungman/.cache/torch/transformers/distributed_-1/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
      "2022-11-22 16:32:38,404 - INFO - configuration_utils - Model config {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "2022-11-22 16:32:38,517 - INFO - modeling_utils - loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /home/sungman/.cache/torch/transformers/distributed_-1/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-22 16:32:38,913 - INFO - modeling_utils - Weights of Bertram not initialized from pretrained model: ['bert.ngram_processor.ngram_embeddings.weight']\n",
      "2022-11-22 16:32:38,914 - INFO - modeling_utils - Weights from pretrained model not used in Bertram: ['bert.embeddings.word_embeddings.weight', 'bert.embeddings.position_embeddings.weight', 'bert.embeddings.token_type_embeddings.weight', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.encoder.layer.0.attention.output.dense.bias', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.0.output.dense.bias', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.1.attention.output.dense.bias', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.2.attention.output.dense.bias', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.encoder.layer.3.attention.output.dense.bias', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.4.attention.output.dense.bias', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.5.attention.output.dense.bias', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.11.output.dense.bias', 'bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'bert.embeddings.LayerNorm.weight', 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.11.output.LayerNorm.bias']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/20 [00:00<?, ?it/s]2022-11-22 16:32:48,832 - INFO - input_processor - Processing training file 1 of 25: ./training/train.bucket16.txt\n",
      "2022-11-22 16:32:48,836 - INFO - input_processor - Done processing training file, batch size is 2364\n",
      "2022-11-22 16:32:48,837 - INFO - input_processor - Processing training file 2 of 25: ./training/train.bucket12.txt\n",
      "2022-11-22 16:32:48,841 - INFO - input_processor - Done processing training file, batch size is 4728\n",
      "2022-11-22 16:32:48,842 - INFO - input_processor - Processing training file 3 of 25: ./training/train.bucket9.txt\n",
      "2022-11-22 16:32:48,847 - INFO - input_processor - Done processing training file, batch size is 7092\n",
      "2022-11-22 16:32:48,848 - INFO - input_processor - Processing training file 4 of 25: ./training/train.bucket19.txt\n",
      "2022-11-22 16:32:48,854 - INFO - input_processor - Done processing training file, batch size is 9456\n",
      "2022-11-22 16:32:48,855 - INFO - input_processor - Processing training file 5 of 25: ./training/train.bucket18.txt\n",
      "2022-11-22 16:32:48,862 - INFO - input_processor - Done processing training file, batch size is 11820\n",
      "2022-11-22 16:32:48,862 - INFO - input_processor - Processing training file 6 of 25: ./training/train.bucket6.txt\n",
      "2022-11-22 16:32:48,872 - INFO - input_processor - Done processing training file, batch size is 14184\n",
      "2022-11-22 16:32:48,873 - INFO - input_processor - Processing training file 7 of 25: ./training/train.bucket5.txt\n",
      "2022-11-22 16:32:48,882 - INFO - input_processor - Done processing training file, batch size is 16548\n",
      "2022-11-22 16:32:48,883 - INFO - input_processor - Processing training file 8 of 25: ./training/train.bucket10.txt\n",
      "2022-11-22 16:32:48,894 - INFO - input_processor - Done processing training file, batch size is 18912\n",
      "2022-11-22 16:32:48,894 - INFO - input_processor - Processing training file 9 of 25: ./training/train.bucket15.txt\n",
      "2022-11-22 16:32:48,905 - INFO - input_processor - Done processing training file, batch size is 21276\n",
      "2022-11-22 16:32:48,906 - INFO - input_processor - Processing training file 10 of 25: ./training/train.bucket21.txt\n",
      "2022-11-22 16:32:48,918 - INFO - input_processor - Done processing training file, batch size is 23640\n",
      "2022-11-22 16:32:48,918 - INFO - input_processor - Processing training file 11 of 25: ./training/train.bucket11.txt\n",
      "2022-11-22 16:32:48,932 - INFO - input_processor - Done processing training file, batch size is 26004\n",
      "2022-11-22 16:32:48,932 - INFO - input_processor - Processing training file 12 of 25: ./training/train.bucket17.txt\n",
      "2022-11-22 16:32:48,946 - INFO - input_processor - Done processing training file, batch size is 28368\n",
      "2022-11-22 16:32:48,946 - INFO - input_processor - Processing training file 13 of 25: ./training/train.bucket1.txt\n",
      "2022-11-22 16:32:48,961 - INFO - input_processor - Done processing training file, batch size is 30732\n",
      "2022-11-22 16:32:48,962 - INFO - input_processor - Processing training file 14 of 25: ./training/train.bucket14.txt\n",
      "2022-11-22 16:32:48,977 - INFO - input_processor - Done processing training file, batch size is 33096\n",
      "2022-11-22 16:32:48,978 - INFO - input_processor - Processing training file 15 of 25: ./training/train.bucket22.txt\n",
      "2022-11-22 16:32:48,995 - INFO - input_processor - Done processing training file, batch size is 35460\n",
      "2022-11-22 16:32:48,995 - INFO - input_processor - Processing training file 16 of 25: ./training/train.bucket13.txt\n",
      "2022-11-22 16:32:49,013 - INFO - input_processor - Done processing training file, batch size is 37824\n",
      "2022-11-22 16:32:49,013 - INFO - input_processor - Processing training file 17 of 25: ./training/train.bucket2.txt\n",
      "2022-11-22 16:32:49,031 - INFO - input_processor - Done processing training file, batch size is 40188\n",
      "2022-11-22 16:32:49,032 - INFO - input_processor - Processing training file 18 of 25: ./training/train.bucket23.txt\n",
      "2022-11-22 16:32:49,052 - INFO - input_processor - Done processing training file, batch size is 42552\n",
      "2022-11-22 16:32:49,052 - INFO - input_processor - Processing training file 19 of 25: ./training/train.bucket4.txt\n",
      "2022-11-22 16:32:49,073 - INFO - input_processor - Done processing training file, batch size is 44916\n",
      "2022-11-22 16:32:49,073 - INFO - input_processor - Processing training file 20 of 25: ./training/train.bucket24.txt\n",
      "2022-11-22 16:32:49,094 - INFO - input_processor - Done processing training file, batch size is 47280\n",
      "2022-11-22 16:32:49,095 - INFO - input_processor - Processing training file 21 of 25: ./training/train.bucket7.txt\n",
      "2022-11-22 16:32:49,117 - INFO - input_processor - Done processing training file, batch size is 49644\n",
      "2022-11-22 16:32:49,117 - INFO - input_processor - Processing training file 22 of 25: ./training/train.bucket8.txt\n",
      "2022-11-22 16:32:49,140 - INFO - input_processor - Done processing training file, batch size is 52008\n",
      "2022-11-22 16:32:49,141 - INFO - input_processor - Processing training file 23 of 25: ./training/train.bucket0.txt\n",
      "2022-11-22 16:32:49,164 - INFO - input_processor - Done processing training file, batch size is 54372\n",
      "2022-11-22 16:32:49,165 - INFO - input_processor - Processing training file 24 of 25: ./training/train.bucket3.txt\n",
      "2022-11-22 16:32:49,190 - INFO - input_processor - Done processing training file, batch size is 56736\n",
      "2022-11-22 16:32:49,190 - INFO - input_processor - Processing training file 25 of 25: ./training/train.bucket20.txt\n",
      "2022-11-22 16:32:49,216 - INFO - input_processor - Done processing training file, batch size is 59100\n",
      "/data/user/home/sungman/test/NLP Group Project/bertram-master/train.py:309: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(loss, requires_grad = True)\n",
      "2022-11-22 16:32:50,171 - INFO - train - Step: 100\tLoss: 0.46356624066829683\n",
      "2022-11-22 16:32:50,698 - INFO - train - Step: 200\tLoss: 0.19258097253739834\n",
      "2022-11-22 16:32:51,216 - INFO - train - Step: 300\tLoss: 0.07471336081624030\n",
      "2022-11-22 16:32:51,734 - INFO - train - Step: 400\tLoss: 0.04511065315455198\n",
      "2022-11-22 16:32:52,250 - INFO - train - Step: 500\tLoss: 0.03223543222993612\n",
      "2022-11-22 16:32:52,766 - INFO - train - Step: 600\tLoss: 0.02674482442438602\n",
      "2022-11-22 16:32:53,282 - INFO - train - Step: 700\tLoss: 0.02290172385051846\n",
      "2022-11-22 16:32:53,799 - INFO - train - Step: 800\tLoss: 0.02156629549339414\n",
      "2022-11-22 16:32:54,315 - INFO - train - Step: 900\tLoss: 0.01940727573819458\n",
      "2022-11-22 16:32:54,831 - INFO - train - Step: 1000\tLoss: 0.01872586571611464\n",
      "2022-11-22 16:32:55,347 - INFO - train - Step: 1100\tLoss: 0.01781816732138395\n",
      "2022-11-22 16:32:55,863 - INFO - train - Step: 1200\tLoss: 0.01692887963727117\n",
      "2022-11-22 16:32:56,378 - INFO - train - Step: 1300\tLoss: 0.01678026651963592\n",
      "2022-11-22 16:32:56,894 - INFO - train - Step: 1400\tLoss: 0.01654046917334199\n",
      "2022-11-22 16:32:57,410 - INFO - train - Step: 1500\tLoss: 0.01637488755397499\n",
      "2022-11-22 16:32:57,926 - INFO - train - Step: 1600\tLoss: 0.01649006988853216\n",
      "2022-11-22 16:32:58,442 - INFO - train - Step: 1700\tLoss: 0.01659682008437812\n",
      "2022-11-22 16:32:58,958 - INFO - train - Step: 1800\tLoss: 0.01661490725353360\n",
      "2022-11-22 16:32:59,189 - INFO - input_processor - Reached the end of the dataset\n",
      "2022-11-22 16:32:59,190 - INFO - train - Done with epoch 0\n",
      "Epoch:   5%|▌         | 1/20 [00:10<03:17, 10.39s/it]2022-11-22 16:32:59,218 - INFO - input_processor - Processing training file 1 of 25: ./training/train.bucket12.txt\n",
      "2022-11-22 16:32:59,222 - INFO - input_processor - Done processing training file, batch size is 2364\n",
      "2022-11-22 16:32:59,223 - INFO - input_processor - Processing training file 2 of 25: ./training/train.bucket14.txt\n",
      "2022-11-22 16:32:59,227 - INFO - input_processor - Done processing training file, batch size is 4728\n",
      "2022-11-22 16:32:59,228 - INFO - input_processor - Processing training file 3 of 25: ./training/train.bucket4.txt\n",
      "2022-11-22 16:32:59,234 - INFO - input_processor - Done processing training file, batch size is 7092\n",
      "2022-11-22 16:32:59,235 - INFO - input_processor - Processing training file 4 of 25: ./training/train.bucket20.txt\n",
      "2022-11-22 16:32:59,241 - INFO - input_processor - Done processing training file, batch size is 9456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-22 16:32:59,242 - INFO - input_processor - Processing training file 5 of 25: ./training/train.bucket15.txt\n",
      "2022-11-22 16:32:59,249 - INFO - input_processor - Done processing training file, batch size is 11820\n",
      "2022-11-22 16:32:59,250 - INFO - input_processor - Processing training file 6 of 25: ./training/train.bucket5.txt\n",
      "2022-11-22 16:32:59,258 - INFO - input_processor - Done processing training file, batch size is 14184\n",
      "2022-11-22 16:32:59,258 - INFO - input_processor - Processing training file 7 of 25: ./training/train.bucket1.txt\n",
      "2022-11-22 16:32:59,267 - INFO - input_processor - Done processing training file, batch size is 16548\n",
      "2022-11-22 16:32:59,267 - INFO - input_processor - Processing training file 8 of 25: ./training/train.bucket17.txt\n",
      "2022-11-22 16:32:59,277 - INFO - input_processor - Done processing training file, batch size is 18912\n",
      "2022-11-22 16:32:59,278 - INFO - input_processor - Processing training file 9 of 25: ./training/train.bucket3.txt\n",
      "2022-11-22 16:32:59,289 - INFO - input_processor - Done processing training file, batch size is 21276\n",
      "2022-11-22 16:32:59,289 - INFO - input_processor - Processing training file 10 of 25: ./training/train.bucket21.txt\n",
      "2022-11-22 16:32:59,301 - INFO - input_processor - Done processing training file, batch size is 23640\n",
      "2022-11-22 16:32:59,301 - INFO - input_processor - Processing training file 11 of 25: ./training/train.bucket8.txt\n",
      "2022-11-22 16:32:59,314 - INFO - input_processor - Done processing training file, batch size is 26004\n",
      "2022-11-22 16:32:59,315 - INFO - input_processor - Processing training file 12 of 25: ./training/train.bucket24.txt\n",
      "2022-11-22 16:32:59,329 - INFO - input_processor - Done processing training file, batch size is 28368\n",
      "2022-11-22 16:32:59,329 - INFO - input_processor - Processing training file 13 of 25: ./training/train.bucket16.txt\n",
      "2022-11-22 16:32:59,344 - INFO - input_processor - Done processing training file, batch size is 30732\n",
      "2022-11-22 16:32:59,345 - INFO - input_processor - Processing training file 14 of 25: ./training/train.bucket10.txt\n",
      "2022-11-22 16:32:59,360 - INFO - input_processor - Done processing training file, batch size is 33096\n",
      "2022-11-22 16:32:59,360 - INFO - input_processor - Processing training file 15 of 25: ./training/train.bucket11.txt\n",
      "2022-11-22 16:32:59,378 - INFO - input_processor - Done processing training file, batch size is 35460\n",
      "2022-11-22 16:32:59,379 - INFO - input_processor - Processing training file 16 of 25: ./training/train.bucket18.txt\n",
      "2022-11-22 16:32:59,397 - INFO - input_processor - Done processing training file, batch size is 37824\n",
      "2022-11-22 16:32:59,397 - INFO - input_processor - Processing training file 17 of 25: ./training/train.bucket9.txt\n",
      "2022-11-22 16:32:59,416 - INFO - input_processor - Done processing training file, batch size is 40188\n",
      "2022-11-22 16:32:59,417 - INFO - input_processor - Processing training file 18 of 25: ./training/train.bucket13.txt\n",
      "2022-11-22 16:32:59,436 - INFO - input_processor - Done processing training file, batch size is 42552\n",
      "2022-11-22 16:32:59,436 - INFO - input_processor - Processing training file 19 of 25: ./training/train.bucket19.txt\n",
      "2022-11-22 16:32:59,457 - INFO - input_processor - Done processing training file, batch size is 44916\n",
      "2022-11-22 16:32:59,457 - INFO - input_processor - Processing training file 20 of 25: ./training/train.bucket2.txt\n",
      "2022-11-22 16:32:59,479 - INFO - input_processor - Done processing training file, batch size is 47280\n",
      "2022-11-22 16:32:59,480 - INFO - input_processor - Processing training file 21 of 25: ./training/train.bucket0.txt\n",
      "2022-11-22 16:32:59,502 - INFO - input_processor - Done processing training file, batch size is 49644\n",
      "2022-11-22 16:32:59,503 - INFO - input_processor - Processing training file 22 of 25: ./training/train.bucket23.txt\n",
      "2022-11-22 16:32:59,525 - INFO - input_processor - Done processing training file, batch size is 52008\n",
      "2022-11-22 16:32:59,526 - INFO - input_processor - Processing training file 23 of 25: ./training/train.bucket6.txt\n",
      "2022-11-22 16:32:59,549 - INFO - input_processor - Done processing training file, batch size is 54372\n",
      "2022-11-22 16:32:59,550 - INFO - input_processor - Processing training file 24 of 25: ./training/train.bucket7.txt\n",
      "2022-11-22 16:32:59,574 - INFO - input_processor - Done processing training file, batch size is 56736\n",
      "2022-11-22 16:32:59,575 - INFO - input_processor - Processing training file 25 of 25: ./training/train.bucket22.txt\n",
      "2022-11-22 16:32:59,600 - INFO - input_processor - Done processing training file, batch size is 59100\n",
      "2022-11-22 16:33:00,136 - INFO - train - Step: 100\tLoss: 0.01639120806008577\n",
      "2022-11-22 16:33:00,693 - INFO - train - Step: 200\tLoss: 0.01655784524045885\n",
      "2022-11-22 16:33:01,231 - INFO - train - Step: 300\tLoss: 0.01638255232013762\n",
      "2022-11-22 16:33:01,744 - INFO - train - Step: 400\tLoss: 0.01621568105183542\n",
      "2022-11-22 16:33:02,257 - INFO - train - Step: 500\tLoss: 0.01639593427069485\n",
      "2022-11-22 16:33:02,769 - INFO - train - Step: 600\tLoss: 0.01673114893026650\n",
      "2022-11-22 16:33:03,280 - INFO - train - Step: 700\tLoss: 0.01641839765943587\n",
      "2022-11-22 16:33:03,793 - INFO - train - Step: 800\tLoss: 0.01617400205694139\n",
      "2022-11-22 16:33:04,306 - INFO - train - Step: 900\tLoss: 0.01639515786431730\n",
      "2022-11-22 16:33:04,848 - INFO - train - Step: 1000\tLoss: 0.01664102992974222\n",
      "2022-11-22 16:33:05,387 - INFO - train - Step: 1100\tLoss: 0.01638773066923022\n",
      "2022-11-22 16:33:05,899 - INFO - train - Step: 1200\tLoss: 0.01642197695560753\n",
      "2022-11-22 16:33:06,411 - INFO - train - Step: 1300\tLoss: 0.01618855998851359\n",
      "2022-11-22 16:33:06,923 - INFO - train - Step: 1400\tLoss: 0.01612825619988143\n",
      "2022-11-22 16:33:07,435 - INFO - train - Step: 1500\tLoss: 0.01643615528009832\n",
      "2022-11-22 16:33:07,948 - INFO - train - Step: 1600\tLoss: 0.01652052465826273\n",
      "2022-11-22 16:33:08,459 - INFO - train - Step: 1700\tLoss: 0.01632255556993186\n",
      "2022-11-22 16:33:08,971 - INFO - train - Step: 1800\tLoss: 0.01642433765344322\n",
      "2022-11-22 16:33:09,202 - INFO - input_processor - Reached the end of the dataset\n",
      "2022-11-22 16:33:09,203 - INFO - train - Done with epoch 1\n",
      "Epoch:  10%|█         | 2/20 [00:20<03:02, 10.15s/it]2022-11-22 16:33:09,204 - INFO - input_processor - Processing training file 1 of 25: ./training/train.bucket11.txt\n",
      "2022-11-22 16:33:09,208 - INFO - input_processor - Done processing training file, batch size is 2364\n",
      "2022-11-22 16:33:09,209 - INFO - input_processor - Processing training file 2 of 25: ./training/train.bucket1.txt\n",
      "2022-11-22 16:33:09,214 - INFO - input_processor - Done processing training file, batch size is 4728\n",
      "2022-11-22 16:33:09,214 - INFO - input_processor - Processing training file 3 of 25: ./training/train.bucket13.txt\n",
      "2022-11-22 16:33:09,220 - INFO - input_processor - Done processing training file, batch size is 7092\n",
      "2022-11-22 16:33:09,220 - INFO - input_processor - Processing training file 4 of 25: ./training/train.bucket24.txt\n",
      "2022-11-22 16:33:09,226 - INFO - input_processor - Done processing training file, batch size is 9456\n",
      "2022-11-22 16:33:09,227 - INFO - input_processor - Processing training file 5 of 25: ./training/train.bucket16.txt\n",
      "2022-11-22 16:33:09,234 - INFO - input_processor - Done processing training file, batch size is 11820\n",
      "2022-11-22 16:33:09,234 - INFO - input_processor - Processing training file 6 of 25: ./training/train.bucket20.txt\n",
      "2022-11-22 16:33:09,242 - INFO - input_processor - Done processing training file, batch size is 14184\n",
      "2022-11-22 16:33:09,243 - INFO - input_processor - Processing training file 7 of 25: ./training/train.bucket23.txt\n",
      "2022-11-22 16:33:09,252 - INFO - input_processor - Done processing training file, batch size is 16548\n",
      "2022-11-22 16:33:09,253 - INFO - input_processor - Processing training file 8 of 25: ./training/train.bucket8.txt\n",
      "2022-11-22 16:33:09,262 - INFO - input_processor - Done processing training file, batch size is 18912\n",
      "2022-11-22 16:33:09,263 - INFO - input_processor - Processing training file 9 of 25: ./training/train.bucket14.txt\n",
      "2022-11-22 16:33:09,275 - INFO - input_processor - Done processing training file, batch size is 21276\n",
      "2022-11-22 16:33:09,275 - INFO - input_processor - Processing training file 10 of 25: ./training/train.bucket21.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-22 16:33:09,287 - INFO - input_processor - Done processing training file, batch size is 23640\n",
      "2022-11-22 16:33:09,288 - INFO - input_processor - Processing training file 11 of 25: ./training/train.bucket10.txt\n",
      "2022-11-22 16:33:09,301 - INFO - input_processor - Done processing training file, batch size is 26004\n",
      "2022-11-22 16:33:09,301 - INFO - input_processor - Processing training file 12 of 25: ./training/train.bucket2.txt\n",
      "2022-11-22 16:33:09,315 - INFO - input_processor - Done processing training file, batch size is 28368\n",
      "2022-11-22 16:33:09,315 - INFO - input_processor - Processing training file 13 of 25: ./training/train.bucket7.txt\n",
      "2022-11-22 16:33:09,330 - INFO - input_processor - Done processing training file, batch size is 30732\n",
      "2022-11-22 16:33:09,330 - INFO - input_processor - Processing training file 14 of 25: ./training/train.bucket9.txt\n",
      "2022-11-22 16:33:09,345 - INFO - input_processor - Done processing training file, batch size is 33096\n",
      "2022-11-22 16:33:09,346 - INFO - input_processor - Processing training file 15 of 25: ./training/train.bucket4.txt\n",
      "2022-11-22 16:33:09,362 - INFO - input_processor - Done processing training file, batch size is 35460\n",
      "2022-11-22 16:33:09,362 - INFO - input_processor - Processing training file 16 of 25: ./training/train.bucket18.txt\n",
      "2022-11-22 16:33:09,380 - INFO - input_processor - Done processing training file, batch size is 37824\n",
      "2022-11-22 16:33:09,381 - INFO - input_processor - Processing training file 17 of 25: ./training/train.bucket12.txt\n",
      "2022-11-22 16:33:09,399 - INFO - input_processor - Done processing training file, batch size is 40188\n",
      "2022-11-22 16:33:09,400 - INFO - input_processor - Processing training file 18 of 25: ./training/train.bucket15.txt\n",
      "2022-11-22 16:33:09,419 - INFO - input_processor - Done processing training file, batch size is 42552\n",
      "2022-11-22 16:33:09,420 - INFO - input_processor - Processing training file 19 of 25: ./training/train.bucket0.txt\n",
      "2022-11-22 16:33:09,440 - INFO - input_processor - Done processing training file, batch size is 44916\n",
      "2022-11-22 16:33:09,441 - INFO - input_processor - Processing training file 20 of 25: ./training/train.bucket6.txt\n",
      "2022-11-22 16:33:09,462 - INFO - input_processor - Done processing training file, batch size is 47280\n",
      "2022-11-22 16:33:09,462 - INFO - input_processor - Processing training file 21 of 25: ./training/train.bucket3.txt\n",
      "2022-11-22 16:33:09,484 - INFO - input_processor - Done processing training file, batch size is 49644\n",
      "2022-11-22 16:33:09,485 - INFO - input_processor - Processing training file 22 of 25: ./training/train.bucket17.txt\n",
      "2022-11-22 16:33:09,508 - INFO - input_processor - Done processing training file, batch size is 52008\n",
      "2022-11-22 16:33:09,508 - INFO - input_processor - Processing training file 23 of 25: ./training/train.bucket5.txt\n",
      "2022-11-22 16:33:09,532 - INFO - input_processor - Done processing training file, batch size is 54372\n",
      "2022-11-22 16:33:09,532 - INFO - input_processor - Processing training file 24 of 25: ./training/train.bucket19.txt\n",
      "2022-11-22 16:33:09,557 - INFO - input_processor - Done processing training file, batch size is 56736\n",
      "2022-11-22 16:33:09,557 - INFO - input_processor - Processing training file 25 of 25: ./training/train.bucket22.txt\n",
      "2022-11-22 16:33:09,583 - INFO - input_processor - Done processing training file, batch size is 59100\n",
      "2022-11-22 16:33:10,102 - INFO - train - Step: 100\tLoss: 0.01666228771209717\n",
      "2022-11-22 16:33:10,614 - INFO - train - Step: 200\tLoss: 0.01642010218463838\n",
      "2022-11-22 16:33:11,126 - INFO - train - Step: 300\tLoss: 0.01613651088438928\n",
      "2022-11-22 16:33:11,637 - INFO - train - Step: 400\tLoss: 0.01640353185124695\n",
      "2022-11-22 16:33:12,148 - INFO - train - Step: 500\tLoss: 0.01647009838372469\n",
      "2022-11-22 16:33:12,662 - INFO - train - Step: 600\tLoss: 0.01602071134373546\n",
      "2022-11-22 16:33:13,174 - INFO - train - Step: 700\tLoss: 0.01635097252205014\n",
      "2022-11-22 16:33:13,710 - INFO - train - Step: 800\tLoss: 0.01649278024211526\n",
      "2022-11-22 16:33:14,249 - INFO - train - Step: 900\tLoss: 0.01627503518015146\n",
      "2022-11-22 16:33:14,777 - INFO - train - Step: 1000\tLoss: 0.01615678217262030\n",
      "2022-11-22 16:33:15,290 - INFO - train - Step: 1100\tLoss: 0.01666376634500921\n",
      "2022-11-22 16:33:15,802 - INFO - train - Step: 1200\tLoss: 0.01642964913509786\n",
      "2022-11-22 16:33:16,314 - INFO - train - Step: 1300\tLoss: 0.01622161056846380\n",
      "2022-11-22 16:33:16,826 - INFO - train - Step: 1400\tLoss: 0.01685250364243984\n",
      "2022-11-22 16:33:17,338 - INFO - train - Step: 1500\tLoss: 0.01645609218627215\n",
      "2022-11-22 16:33:17,851 - INFO - train - Step: 1600\tLoss: 0.01628923089243471\n",
      "2022-11-22 16:33:18,363 - INFO - train - Step: 1700\tLoss: 0.01660880706273019\n",
      "2022-11-22 16:33:18,876 - INFO - train - Step: 1800\tLoss: 0.01629085541702807\n",
      "2022-11-22 16:33:19,108 - INFO - input_processor - Reached the end of the dataset\n",
      "2022-11-22 16:33:19,108 - INFO - train - Done with epoch 2\n",
      "Epoch:  15%|█▌        | 3/20 [00:30<02:50, 10.04s/it]2022-11-22 16:33:19,109 - INFO - input_processor - Processing training file 1 of 25: ./training/train.bucket22.txt\n",
      "2022-11-22 16:33:19,113 - INFO - input_processor - Done processing training file, batch size is 2364\n",
      "2022-11-22 16:33:19,114 - INFO - input_processor - Processing training file 2 of 25: ./training/train.bucket1.txt\n",
      "2022-11-22 16:33:19,118 - INFO - input_processor - Done processing training file, batch size is 4728\n",
      "2022-11-22 16:33:19,119 - INFO - input_processor - Processing training file 3 of 25: ./training/train.bucket11.txt\n",
      "2022-11-22 16:33:19,124 - INFO - input_processor - Done processing training file, batch size is 7092\n",
      "2022-11-22 16:33:19,124 - INFO - input_processor - Processing training file 4 of 25: ./training/train.bucket18.txt\n",
      "2022-11-22 16:33:19,131 - INFO - input_processor - Done processing training file, batch size is 9456\n",
      "2022-11-22 16:33:19,132 - INFO - input_processor - Processing training file 5 of 25: ./training/train.bucket10.txt\n",
      "2022-11-22 16:33:19,139 - INFO - input_processor - Done processing training file, batch size is 11820\n",
      "2022-11-22 16:33:19,139 - INFO - input_processor - Processing training file 6 of 25: ./training/train.bucket3.txt\n",
      "2022-11-22 16:33:19,147 - INFO - input_processor - Done processing training file, batch size is 14184\n",
      "2022-11-22 16:33:19,148 - INFO - input_processor - Processing training file 7 of 25: ./training/train.bucket23.txt\n",
      "2022-11-22 16:33:19,157 - INFO - input_processor - Done processing training file, batch size is 16548\n",
      "2022-11-22 16:33:19,157 - INFO - input_processor - Processing training file 8 of 25: ./training/train.bucket16.txt\n",
      "2022-11-22 16:33:19,167 - INFO - input_processor - Done processing training file, batch size is 18912\n",
      "2022-11-22 16:33:19,168 - INFO - input_processor - Processing training file 9 of 25: ./training/train.bucket2.txt\n",
      "2022-11-22 16:33:19,179 - INFO - input_processor - Done processing training file, batch size is 21276\n",
      "2022-11-22 16:33:19,179 - INFO - input_processor - Processing training file 10 of 25: ./training/train.bucket17.txt\n",
      "2022-11-22 16:33:19,192 - INFO - input_processor - Done processing training file, batch size is 23640\n",
      "2022-11-22 16:33:19,192 - INFO - input_processor - Processing training file 11 of 25: ./training/train.bucket9.txt\n",
      "2022-11-22 16:33:19,205 - INFO - input_processor - Done processing training file, batch size is 26004\n",
      "2022-11-22 16:33:19,205 - INFO - input_processor - Processing training file 12 of 25: ./training/train.bucket12.txt\n",
      "2022-11-22 16:33:19,219 - INFO - input_processor - Done processing training file, batch size is 28368\n",
      "2022-11-22 16:33:19,219 - INFO - input_processor - Processing training file 13 of 25: ./training/train.bucket13.txt\n",
      "2022-11-22 16:33:19,234 - INFO - input_processor - Done processing training file, batch size is 30732\n",
      "2022-11-22 16:33:19,234 - INFO - input_processor - Processing training file 14 of 25: ./training/train.bucket5.txt\n",
      "2022-11-22 16:33:19,250 - INFO - input_processor - Done processing training file, batch size is 33096\n",
      "2022-11-22 16:33:19,250 - INFO - input_processor - Processing training file 15 of 25: ./training/train.bucket24.txt\n",
      "2022-11-22 16:33:19,266 - INFO - input_processor - Done processing training file, batch size is 35460\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-22 16:33:19,267 - INFO - input_processor - Processing training file 16 of 25: ./training/train.bucket8.txt\n",
      "2022-11-22 16:33:19,284 - INFO - input_processor - Done processing training file, batch size is 37824\n",
      "2022-11-22 16:33:19,285 - INFO - input_processor - Processing training file 17 of 25: ./training/train.bucket21.txt\n",
      "2022-11-22 16:33:19,303 - INFO - input_processor - Done processing training file, batch size is 40188\n",
      "2022-11-22 16:33:19,303 - INFO - input_processor - Processing training file 18 of 25: ./training/train.bucket19.txt\n",
      "2022-11-22 16:33:19,323 - INFO - input_processor - Done processing training file, batch size is 42552\n",
      "2022-11-22 16:33:19,323 - INFO - input_processor - Processing training file 19 of 25: ./training/train.bucket15.txt\n",
      "2022-11-22 16:33:19,344 - INFO - input_processor - Done processing training file, batch size is 44916\n",
      "2022-11-22 16:33:19,344 - INFO - input_processor - Processing training file 20 of 25: ./training/train.bucket4.txt\n",
      "2022-11-22 16:33:19,365 - INFO - input_processor - Done processing training file, batch size is 47280\n",
      "2022-11-22 16:33:19,366 - INFO - input_processor - Processing training file 21 of 25: ./training/train.bucket14.txt\n",
      "2022-11-22 16:33:19,388 - INFO - input_processor - Done processing training file, batch size is 49644\n",
      "2022-11-22 16:33:19,389 - INFO - input_processor - Processing training file 22 of 25: ./training/train.bucket7.txt\n",
      "2022-11-22 16:33:19,412 - INFO - input_processor - Done processing training file, batch size is 52008\n",
      "2022-11-22 16:33:19,412 - INFO - input_processor - Processing training file 23 of 25: ./training/train.bucket0.txt\n",
      "2022-11-22 16:33:19,436 - INFO - input_processor - Done processing training file, batch size is 54372\n",
      "2022-11-22 16:33:19,436 - INFO - input_processor - Processing training file 24 of 25: ./training/train.bucket20.txt\n",
      "2022-11-22 16:33:19,461 - INFO - input_processor - Done processing training file, batch size is 56736\n",
      "2022-11-22 16:33:19,461 - INFO - input_processor - Processing training file 25 of 25: ./training/train.bucket6.txt\n",
      "2022-11-22 16:33:19,487 - INFO - input_processor - Done processing training file, batch size is 59100\n",
      "2022-11-22 16:33:20,022 - INFO - train - Step: 100\tLoss: 0.01655021314509213\n",
      "2022-11-22 16:33:20,535 - INFO - train - Step: 200\tLoss: 0.01636974125169217\n",
      "2022-11-22 16:33:21,047 - INFO - train - Step: 300\tLoss: 0.01640220423229039\n",
      "2022-11-22 16:33:21,559 - INFO - train - Step: 400\tLoss: 0.01611593163572252\n",
      "2022-11-22 16:33:22,071 - INFO - train - Step: 500\tLoss: 0.01640991472639143\n",
      "2022-11-22 16:33:22,583 - INFO - train - Step: 600\tLoss: 0.01643055843189359\n",
      "2022-11-22 16:33:23,096 - INFO - train - Step: 700\tLoss: 0.01637696824967861\n",
      "2022-11-22 16:33:23,607 - INFO - train - Step: 800\tLoss: 0.01650827263481915\n",
      "2022-11-22 16:33:24,119 - INFO - train - Step: 900\tLoss: 0.01657696503214538\n",
      "2022-11-22 16:33:24,631 - INFO - train - Step: 1000\tLoss: 0.01641956129111350\n",
      "2022-11-22 16:33:25,143 - INFO - train - Step: 1100\tLoss: 0.01643368089571595\n",
      "2022-11-22 16:33:25,655 - INFO - train - Step: 1200\tLoss: 0.01651626834645867\n",
      "2022-11-22 16:33:26,167 - INFO - train - Step: 1300\tLoss: 0.01644226127304137\n",
      "2022-11-22 16:33:26,680 - INFO - train - Step: 1400\tLoss: 0.01652893375605345\n",
      "2022-11-22 16:33:27,192 - INFO - train - Step: 1500\tLoss: 0.01636312005110085\n",
      "2022-11-22 16:33:27,705 - INFO - train - Step: 1600\tLoss: 0.01631712270900607\n",
      "2022-11-22 16:33:28,217 - INFO - train - Step: 1700\tLoss: 0.01652751947753131\n",
      "2022-11-22 16:33:28,729 - INFO - train - Step: 1800\tLoss: 0.01637928762473166\n",
      "2022-11-22 16:33:28,960 - INFO - input_processor - Reached the end of the dataset\n",
      "2022-11-22 16:33:28,960 - INFO - train - Done with epoch 3\n",
      "Epoch:  20%|██        | 4/20 [00:40<02:39,  9.96s/it]2022-11-22 16:33:28,961 - INFO - input_processor - Processing training file 1 of 25: ./training/train.bucket20.txt\n",
      "2022-11-22 16:33:28,965 - INFO - input_processor - Done processing training file, batch size is 2364\n",
      "2022-11-22 16:33:28,966 - INFO - input_processor - Processing training file 2 of 25: ./training/train.bucket15.txt\n",
      "2022-11-22 16:33:28,970 - INFO - input_processor - Done processing training file, batch size is 4728\n",
      "2022-11-22 16:33:28,971 - INFO - input_processor - Processing training file 3 of 25: ./training/train.bucket2.txt\n",
      "2022-11-22 16:33:28,976 - INFO - input_processor - Done processing training file, batch size is 7092\n",
      "2022-11-22 16:33:28,976 - INFO - input_processor - Processing training file 4 of 25: ./training/train.bucket6.txt\n",
      "2022-11-22 16:33:28,983 - INFO - input_processor - Done processing training file, batch size is 9456\n",
      "2022-11-22 16:33:28,984 - INFO - input_processor - Processing training file 5 of 25: ./training/train.bucket17.txt\n",
      "2022-11-22 16:33:28,991 - INFO - input_processor - Done processing training file, batch size is 11820\n",
      "2022-11-22 16:33:28,991 - INFO - input_processor - Processing training file 6 of 25: ./training/train.bucket19.txt\n",
      "2022-11-22 16:33:29,000 - INFO - input_processor - Done processing training file, batch size is 14184\n",
      "2022-11-22 16:33:29,000 - INFO - input_processor - Processing training file 7 of 25: ./training/train.bucket12.txt\n",
      "2022-11-22 16:33:29,009 - INFO - input_processor - Done processing training file, batch size is 16548\n",
      "2022-11-22 16:33:29,009 - INFO - input_processor - Processing training file 8 of 25: ./training/train.bucket11.txt\n",
      "2022-11-22 16:33:29,020 - INFO - input_processor - Done processing training file, batch size is 18912\n",
      "2022-11-22 16:33:29,020 - INFO - input_processor - Processing training file 9 of 25: ./training/train.bucket9.txt\n",
      "2022-11-22 16:33:29,031 - INFO - input_processor - Done processing training file, batch size is 21276\n",
      "2022-11-22 16:33:29,031 - INFO - input_processor - Processing training file 10 of 25: ./training/train.bucket22.txt\n",
      "2022-11-22 16:33:29,043 - INFO - input_processor - Done processing training file, batch size is 23640\n",
      "2022-11-22 16:33:29,044 - INFO - input_processor - Processing training file 11 of 25: ./training/train.bucket8.txt\n",
      "2022-11-22 16:33:29,057 - INFO - input_processor - Done processing training file, batch size is 26004\n",
      "2022-11-22 16:33:29,057 - INFO - input_processor - Processing training file 12 of 25: ./training/train.bucket21.txt\n",
      "2022-11-22 16:33:29,070 - INFO - input_processor - Done processing training file, batch size is 28368\n",
      "2022-11-22 16:33:29,071 - INFO - input_processor - Processing training file 13 of 25: ./training/train.bucket3.txt\n",
      "2022-11-22 16:33:29,086 - INFO - input_processor - Done processing training file, batch size is 30732\n",
      "2022-11-22 16:33:29,086 - INFO - input_processor - Processing training file 14 of 25: ./training/train.bucket1.txt\n",
      "2022-11-22 16:33:29,101 - INFO - input_processor - Done processing training file, batch size is 33096\n",
      "2022-11-22 16:33:29,102 - INFO - input_processor - Processing training file 15 of 25: ./training/train.bucket16.txt\n",
      "2022-11-22 16:33:29,118 - INFO - input_processor - Done processing training file, batch size is 35460\n",
      "2022-11-22 16:33:29,119 - INFO - input_processor - Processing training file 16 of 25: ./training/train.bucket13.txt\n",
      "2022-11-22 16:33:29,136 - INFO - input_processor - Done processing training file, batch size is 37824\n",
      "2022-11-22 16:33:29,137 - INFO - input_processor - Processing training file 17 of 25: ./training/train.bucket5.txt\n",
      "2022-11-22 16:33:29,156 - INFO - input_processor - Done processing training file, batch size is 40188\n",
      "2022-11-22 16:33:29,156 - INFO - input_processor - Processing training file 18 of 25: ./training/train.bucket0.txt\n",
      "2022-11-22 16:33:29,176 - INFO - input_processor - Done processing training file, batch size is 42552\n",
      "2022-11-22 16:33:29,176 - INFO - input_processor - Processing training file 19 of 25: ./training/train.bucket23.txt\n",
      "2022-11-22 16:33:29,196 - INFO - input_processor - Done processing training file, batch size is 44916\n",
      "2022-11-22 16:33:29,197 - INFO - input_processor - Processing training file 20 of 25: ./training/train.bucket18.txt\n",
      "2022-11-22 16:33:29,218 - INFO - input_processor - Done processing training file, batch size is 47280\n",
      "2022-11-22 16:33:29,218 - INFO - input_processor - Processing training file 21 of 25: ./training/train.bucket7.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-22 16:33:29,241 - INFO - input_processor - Done processing training file, batch size is 49644\n",
      "2022-11-22 16:33:29,241 - INFO - input_processor - Processing training file 22 of 25: ./training/train.bucket10.txt\n",
      "2022-11-22 16:33:29,264 - INFO - input_processor - Done processing training file, batch size is 52008\n",
      "2022-11-22 16:33:29,264 - INFO - input_processor - Processing training file 23 of 25: ./training/train.bucket14.txt\n",
      "2022-11-22 16:33:29,292 - INFO - input_processor - Done processing training file, batch size is 54372\n",
      "2022-11-22 16:33:29,294 - INFO - input_processor - Processing training file 24 of 25: ./training/train.bucket24.txt\n",
      "2022-11-22 16:33:29,319 - INFO - input_processor - Done processing training file, batch size is 56736\n",
      "2022-11-22 16:33:29,320 - INFO - input_processor - Processing training file 25 of 25: ./training/train.bucket4.txt\n",
      "2022-11-22 16:33:29,345 - INFO - input_processor - Done processing training file, batch size is 59100\n",
      "2022-11-22 16:33:29,866 - INFO - train - Step: 100\tLoss: 0.01675770571455359\n",
      "2022-11-22 16:33:30,378 - INFO - train - Step: 200\tLoss: 0.01645863072015345\n",
      "2022-11-22 16:33:30,890 - INFO - train - Step: 300\tLoss: 0.01637141594663262\n",
      "2022-11-22 16:33:31,401 - INFO - train - Step: 400\tLoss: 0.01650678208097816\n",
      "2022-11-22 16:33:31,914 - INFO - train - Step: 500\tLoss: 0.01650172034278512\n",
      "2022-11-22 16:33:32,426 - INFO - train - Step: 600\tLoss: 0.01649155508726835\n",
      "2022-11-22 16:33:32,943 - INFO - train - Step: 700\tLoss: 0.01631903066299856\n",
      "2022-11-22 16:33:33,455 - INFO - train - Step: 800\tLoss: 0.01655233788304031\n",
      "2022-11-22 16:33:33,967 - INFO - train - Step: 900\tLoss: 0.01659524543210864\n",
      "2022-11-22 16:33:34,480 - INFO - train - Step: 1000\tLoss: 0.01673119429498911\n",
      "2022-11-22 16:33:35,005 - INFO - train - Step: 1100\tLoss: 0.01637105066329241\n",
      "2022-11-22 16:33:35,523 - INFO - train - Step: 1200\tLoss: 0.01628902705386281\n",
      "2022-11-22 16:33:36,037 - INFO - train - Step: 1300\tLoss: 0.01619401039555669\n",
      "2022-11-22 16:33:36,550 - INFO - train - Step: 1400\tLoss: 0.01629061924293637\n",
      "2022-11-22 16:33:37,061 - INFO - train - Step: 1500\tLoss: 0.01653536878526211\n",
      "2022-11-22 16:33:37,573 - INFO - train - Step: 1600\tLoss: 0.01612505171447992\n",
      "2022-11-22 16:33:38,085 - INFO - train - Step: 1700\tLoss: 0.01627060957252979\n",
      "2022-11-22 16:33:38,596 - INFO - train - Step: 1800\tLoss: 0.01653670137748122\n",
      "2022-11-22 16:33:38,828 - INFO - input_processor - Reached the end of the dataset\n",
      "2022-11-22 16:33:38,829 - INFO - train - Done with epoch 4\n",
      "Epoch:  25%|██▌       | 5/20 [00:50<02:29,  9.93s/it]2022-11-22 16:33:38,839 - INFO - input_processor - Processing training file 1 of 25: ./training/train.bucket21.txt\n",
      "2022-11-22 16:33:38,844 - INFO - input_processor - Done processing training file, batch size is 2364\n",
      "2022-11-22 16:33:38,844 - INFO - input_processor - Processing training file 2 of 25: ./training/train.bucket16.txt\n",
      "2022-11-22 16:33:38,849 - INFO - input_processor - Done processing training file, batch size is 4728\n",
      "2022-11-22 16:33:38,849 - INFO - input_processor - Processing training file 3 of 25: ./training/train.bucket23.txt\n",
      "2022-11-22 16:33:38,854 - INFO - input_processor - Done processing training file, batch size is 7092\n",
      "2022-11-22 16:33:38,855 - INFO - input_processor - Processing training file 4 of 25: ./training/train.bucket12.txt\n",
      "2022-11-22 16:33:38,861 - INFO - input_processor - Done processing training file, batch size is 9456\n",
      "2022-11-22 16:33:38,862 - INFO - input_processor - Processing training file 5 of 25: ./training/train.bucket20.txt\n",
      "2022-11-22 16:33:38,870 - INFO - input_processor - Done processing training file, batch size is 11820\n",
      "2022-11-22 16:33:38,870 - INFO - input_processor - Processing training file 6 of 25: ./training/train.bucket9.txt\n",
      "2022-11-22 16:33:38,878 - INFO - input_processor - Done processing training file, batch size is 14184\n",
      "2022-11-22 16:33:38,878 - INFO - input_processor - Processing training file 7 of 25: ./training/train.bucket5.txt\n",
      "2022-11-22 16:33:38,887 - INFO - input_processor - Done processing training file, batch size is 16548\n",
      "2022-11-22 16:33:38,888 - INFO - input_processor - Processing training file 8 of 25: ./training/train.bucket11.txt\n",
      "2022-11-22 16:33:38,898 - INFO - input_processor - Done processing training file, batch size is 18912\n",
      "2022-11-22 16:33:38,899 - INFO - input_processor - Processing training file 9 of 25: ./training/train.bucket15.txt\n",
      "2022-11-22 16:33:38,910 - INFO - input_processor - Done processing training file, batch size is 21276\n",
      "2022-11-22 16:33:38,911 - INFO - input_processor - Processing training file 10 of 25: ./training/train.bucket8.txt\n",
      "2022-11-22 16:33:38,923 - INFO - input_processor - Done processing training file, batch size is 23640\n",
      "2022-11-22 16:33:38,923 - INFO - input_processor - Processing training file 11 of 25: ./training/train.bucket2.txt\n",
      "2022-11-22 16:33:38,936 - INFO - input_processor - Done processing training file, batch size is 26004\n",
      "2022-11-22 16:33:38,936 - INFO - input_processor - Processing training file 12 of 25: ./training/train.bucket3.txt\n",
      "2022-11-22 16:33:38,950 - INFO - input_processor - Done processing training file, batch size is 28368\n",
      "2022-11-22 16:33:38,951 - INFO - input_processor - Processing training file 13 of 25: ./training/train.bucket1.txt\n",
      "2022-11-22 16:33:38,965 - INFO - input_processor - Done processing training file, batch size is 30732\n",
      "2022-11-22 16:33:38,966 - INFO - input_processor - Processing training file 14 of 25: ./training/train.bucket4.txt\n",
      "2022-11-22 16:33:38,981 - INFO - input_processor - Done processing training file, batch size is 33096\n",
      "2022-11-22 16:33:38,982 - INFO - input_processor - Processing training file 15 of 25: ./training/train.bucket13.txt\n",
      "2022-11-22 16:33:38,998 - INFO - input_processor - Done processing training file, batch size is 35460\n",
      "2022-11-22 16:33:38,998 - INFO - input_processor - Processing training file 16 of 25: ./training/train.bucket0.txt\n",
      "2022-11-22 16:33:39,016 - INFO - input_processor - Done processing training file, batch size is 37824\n",
      "2022-11-22 16:33:39,016 - INFO - input_processor - Processing training file 17 of 25: ./training/train.bucket17.txt\n",
      "2022-11-22 16:33:39,034 - INFO - input_processor - Done processing training file, batch size is 40188\n",
      "2022-11-22 16:33:39,035 - INFO - input_processor - Processing training file 18 of 25: ./training/train.bucket7.txt\n",
      "2022-11-22 16:33:39,054 - INFO - input_processor - Done processing training file, batch size is 42552\n",
      "2022-11-22 16:33:39,055 - INFO - input_processor - Processing training file 19 of 25: ./training/train.bucket6.txt\n",
      "2022-11-22 16:33:39,075 - INFO - input_processor - Done processing training file, batch size is 44916\n",
      "2022-11-22 16:33:39,076 - INFO - input_processor - Processing training file 20 of 25: ./training/train.bucket19.txt\n",
      "2022-11-22 16:33:39,097 - INFO - input_processor - Done processing training file, batch size is 47280\n",
      "2022-11-22 16:33:39,097 - INFO - input_processor - Processing training file 21 of 25: ./training/train.bucket10.txt\n",
      "2022-11-22 16:33:39,119 - INFO - input_processor - Done processing training file, batch size is 49644\n",
      "2022-11-22 16:33:39,120 - INFO - input_processor - Processing training file 22 of 25: ./training/train.bucket22.txt\n",
      "2022-11-22 16:33:39,143 - INFO - input_processor - Done processing training file, batch size is 52008\n",
      "2022-11-22 16:33:39,143 - INFO - input_processor - Processing training file 23 of 25: ./training/train.bucket24.txt\n",
      "2022-11-22 16:33:39,167 - INFO - input_processor - Done processing training file, batch size is 54372\n",
      "2022-11-22 16:33:39,168 - INFO - input_processor - Processing training file 24 of 25: ./training/train.bucket18.txt\n",
      "2022-11-22 16:33:39,193 - INFO - input_processor - Done processing training file, batch size is 56736\n",
      "2022-11-22 16:33:39,193 - INFO - input_processor - Processing training file 25 of 25: ./training/train.bucket14.txt\n",
      "2022-11-22 16:33:39,219 - INFO - input_processor - Done processing training file, batch size is 59100\n",
      "2022-11-22 16:33:39,736 - INFO - train - Step: 100\tLoss: 0.01657610509544611\n",
      "2022-11-22 16:33:40,246 - INFO - train - Step: 200\tLoss: 0.01600923151709139\n",
      "2022-11-22 16:33:40,757 - INFO - train - Step: 300\tLoss: 0.01673850768245757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-22 16:33:41,269 - INFO - train - Step: 400\tLoss: 0.01665134771727025\n",
      "2022-11-22 16:33:41,781 - INFO - train - Step: 500\tLoss: 0.01664019375108182\n",
      "2022-11-22 16:33:42,294 - INFO - train - Step: 600\tLoss: 0.01636243914254010\n",
      "2022-11-22 16:33:42,817 - INFO - train - Step: 700\tLoss: 0.01662053425796330\n",
      "2022-11-22 16:33:43,329 - INFO - train - Step: 800\tLoss: 0.01637284065596759\n",
      "2022-11-22 16:33:43,840 - INFO - train - Step: 900\tLoss: 0.01634910108521581\n",
      "2022-11-22 16:33:44,352 - INFO - train - Step: 1000\tLoss: 0.01653255004435778\n",
      "2022-11-22 16:33:44,864 - INFO - train - Step: 1100\tLoss: 0.01639111232012510\n",
      "2022-11-22 16:33:45,378 - INFO - train - Step: 1200\tLoss: 0.01661191667430103\n",
      "2022-11-22 16:33:45,890 - INFO - train - Step: 1300\tLoss: 0.01629245233722031\n",
      "2022-11-22 16:33:46,401 - INFO - train - Step: 1400\tLoss: 0.01663854178972542\n",
      "2022-11-22 16:33:46,913 - INFO - train - Step: 1500\tLoss: 0.01641612464562059\n",
      "2022-11-22 16:33:47,424 - INFO - train - Step: 1600\tLoss: 0.01632500840350986\n",
      "2022-11-22 16:33:47,935 - INFO - train - Step: 1700\tLoss: 0.01626663267612457\n",
      "2022-11-22 16:33:48,449 - INFO - train - Step: 1800\tLoss: 0.01662880612537265\n",
      "2022-11-22 16:33:48,680 - INFO - input_processor - Reached the end of the dataset\n",
      "2022-11-22 16:33:48,680 - INFO - train - Done with epoch 5\n",
      "Epoch:  30%|███       | 6/20 [00:59<02:18,  9.90s/it]2022-11-22 16:33:48,681 - INFO - input_processor - Processing training file 1 of 25: ./training/train.bucket23.txt\n",
      "2022-11-22 16:33:48,685 - INFO - input_processor - Done processing training file, batch size is 2364\n",
      "2022-11-22 16:33:48,686 - INFO - input_processor - Processing training file 2 of 25: ./training/train.bucket15.txt\n",
      "2022-11-22 16:33:48,690 - INFO - input_processor - Done processing training file, batch size is 4728\n",
      "2022-11-22 16:33:48,691 - INFO - input_processor - Processing training file 3 of 25: ./training/train.bucket2.txt\n",
      "2022-11-22 16:33:48,696 - INFO - input_processor - Done processing training file, batch size is 7092\n",
      "2022-11-22 16:33:48,696 - INFO - input_processor - Processing training file 4 of 25: ./training/train.bucket9.txt\n",
      "2022-11-22 16:33:48,703 - INFO - input_processor - Done processing training file, batch size is 9456\n",
      "2022-11-22 16:33:48,704 - INFO - input_processor - Processing training file 5 of 25: ./training/train.bucket21.txt\n",
      "2022-11-22 16:33:48,711 - INFO - input_processor - Done processing training file, batch size is 11820\n",
      "2022-11-22 16:33:48,711 - INFO - input_processor - Processing training file 6 of 25: ./training/train.bucket24.txt\n",
      "2022-11-22 16:33:48,719 - INFO - input_processor - Done processing training file, batch size is 14184\n",
      "2022-11-22 16:33:48,720 - INFO - input_processor - Processing training file 7 of 25: ./training/train.bucket5.txt\n",
      "2022-11-22 16:33:48,728 - INFO - input_processor - Done processing training file, batch size is 16548\n",
      "2022-11-22 16:33:48,729 - INFO - input_processor - Processing training file 8 of 25: ./training/train.bucket1.txt\n",
      "2022-11-22 16:33:48,739 - INFO - input_processor - Done processing training file, batch size is 18912\n",
      "2022-11-22 16:33:48,739 - INFO - input_processor - Processing training file 9 of 25: ./training/train.bucket14.txt\n",
      "2022-11-22 16:33:48,750 - INFO - input_processor - Done processing training file, batch size is 21276\n",
      "2022-11-22 16:33:48,751 - INFO - input_processor - Processing training file 10 of 25: ./training/train.bucket18.txt\n",
      "2022-11-22 16:33:48,762 - INFO - input_processor - Done processing training file, batch size is 23640\n",
      "2022-11-22 16:33:48,763 - INFO - input_processor - Processing training file 11 of 25: ./training/train.bucket7.txt\n",
      "2022-11-22 16:33:48,776 - INFO - input_processor - Done processing training file, batch size is 26004\n",
      "2022-11-22 16:33:48,777 - INFO - input_processor - Processing training file 12 of 25: ./training/train.bucket16.txt\n",
      "2022-11-22 16:33:48,790 - INFO - input_processor - Done processing training file, batch size is 28368\n",
      "2022-11-22 16:33:48,791 - INFO - input_processor - Processing training file 13 of 25: ./training/train.bucket0.txt\n",
      "2022-11-22 16:33:48,805 - INFO - input_processor - Done processing training file, batch size is 30732\n",
      "2022-11-22 16:33:48,806 - INFO - input_processor - Processing training file 14 of 25: ./training/train.bucket3.txt\n",
      "2022-11-22 16:33:48,821 - INFO - input_processor - Done processing training file, batch size is 33096\n",
      "2022-11-22 16:33:48,821 - INFO - input_processor - Processing training file 15 of 25: ./training/train.bucket20.txt\n",
      "2022-11-22 16:33:48,838 - INFO - input_processor - Done processing training file, batch size is 35460\n",
      "2022-11-22 16:33:48,839 - INFO - input_processor - Processing training file 16 of 25: ./training/train.bucket17.txt\n",
      "2022-11-22 16:33:48,856 - INFO - input_processor - Done processing training file, batch size is 37824\n",
      "2022-11-22 16:33:48,856 - INFO - input_processor - Processing training file 17 of 25: ./training/train.bucket6.txt\n",
      "2022-11-22 16:33:48,875 - INFO - input_processor - Done processing training file, batch size is 40188\n",
      "2022-11-22 16:33:48,875 - INFO - input_processor - Processing training file 18 of 25: ./training/train.bucket11.txt\n",
      "2022-11-22 16:33:48,894 - INFO - input_processor - Done processing training file, batch size is 42552\n",
      "2022-11-22 16:33:48,895 - INFO - input_processor - Processing training file 19 of 25: ./training/train.bucket13.txt\n",
      "2022-11-22 16:33:48,915 - INFO - input_processor - Done processing training file, batch size is 44916\n",
      "2022-11-22 16:33:48,916 - INFO - input_processor - Processing training file 20 of 25: ./training/train.bucket8.txt\n",
      "2022-11-22 16:33:48,937 - INFO - input_processor - Done processing training file, batch size is 47280\n",
      "2022-11-22 16:33:48,937 - INFO - input_processor - Processing training file 21 of 25: ./training/train.bucket12.txt\n",
      "2022-11-22 16:33:48,960 - INFO - input_processor - Done processing training file, batch size is 49644\n",
      "2022-11-22 16:33:48,960 - INFO - input_processor - Processing training file 22 of 25: ./training/train.bucket22.txt\n",
      "2022-11-22 16:33:48,983 - INFO - input_processor - Done processing training file, batch size is 52008\n",
      "2022-11-22 16:33:48,984 - INFO - input_processor - Processing training file 23 of 25: ./training/train.bucket10.txt\n",
      "2022-11-22 16:33:49,008 - INFO - input_processor - Done processing training file, batch size is 54372\n",
      "2022-11-22 16:33:49,008 - INFO - input_processor - Processing training file 24 of 25: ./training/train.bucket19.txt\n",
      "2022-11-22 16:33:49,032 - INFO - input_processor - Done processing training file, batch size is 56736\n",
      "2022-11-22 16:33:49,033 - INFO - input_processor - Processing training file 25 of 25: ./training/train.bucket4.txt\n",
      "2022-11-22 16:33:49,058 - INFO - input_processor - Done processing training file, batch size is 59100\n",
      "2022-11-22 16:33:49,577 - INFO - train - Step: 100\tLoss: 0.01659533490426839\n",
      "2022-11-22 16:33:50,107 - INFO - train - Step: 200\tLoss: 0.01649240905418992\n",
      "2022-11-22 16:33:50,623 - INFO - train - Step: 300\tLoss: 0.01644357366487384\n",
      "2022-11-22 16:33:51,134 - INFO - train - Step: 400\tLoss: 0.01645659371279180\n",
      "2022-11-22 16:33:51,648 - INFO - train - Step: 500\tLoss: 0.01657846192829311\n",
      "2022-11-22 16:33:52,160 - INFO - train - Step: 600\tLoss: 0.01637555620633066\n",
      "2022-11-22 16:33:52,672 - INFO - train - Step: 700\tLoss: 0.01667628128081560\n",
      "2022-11-22 16:33:53,184 - INFO - train - Step: 800\tLoss: 0.01657225958071649\n",
      "2022-11-22 16:33:53,705 - INFO - train - Step: 900\tLoss: 0.01649473030120134\n",
      "2022-11-22 16:33:54,243 - INFO - train - Step: 1000\tLoss: 0.01662517025135457\n",
      "2022-11-22 16:33:54,775 - INFO - train - Step: 1100\tLoss: 0.01661587209440768\n",
      "2022-11-22 16:33:55,289 - INFO - train - Step: 1200\tLoss: 0.01619740717113018\n",
      "2022-11-22 16:33:55,801 - INFO - train - Step: 1300\tLoss: 0.01644180922769010\n",
      "2022-11-22 16:33:56,313 - INFO - train - Step: 1400\tLoss: 0.01627383654005825\n",
      "2022-11-22 16:33:56,825 - INFO - train - Step: 1500\tLoss: 0.01653912837617099\n",
      "2022-11-22 16:33:57,338 - INFO - train - Step: 1600\tLoss: 0.01621468725614250\n",
      "2022-11-22 16:33:57,850 - INFO - train - Step: 1700\tLoss: 0.01647239116951823\n",
      "2022-11-22 16:33:58,363 - INFO - train - Step: 1800\tLoss: 0.01637191380374134\n",
      "2022-11-22 16:33:58,594 - INFO - input_processor - Reached the end of the dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-22 16:33:58,594 - INFO - train - Done with epoch 6\n",
      "Epoch:  35%|███▌      | 7/20 [01:09<02:08,  9.91s/it]2022-11-22 16:33:58,595 - INFO - input_processor - Processing training file 1 of 25: ./training/train.bucket5.txt\n",
      "2022-11-22 16:33:58,599 - INFO - input_processor - Done processing training file, batch size is 2364\n",
      "2022-11-22 16:33:58,600 - INFO - input_processor - Processing training file 2 of 25: ./training/train.bucket23.txt\n",
      "2022-11-22 16:33:58,605 - INFO - input_processor - Done processing training file, batch size is 4728\n",
      "2022-11-22 16:33:58,605 - INFO - input_processor - Processing training file 3 of 25: ./training/train.bucket20.txt\n",
      "2022-11-22 16:33:58,610 - INFO - input_processor - Done processing training file, batch size is 7092\n",
      "2022-11-22 16:33:58,611 - INFO - input_processor - Processing training file 4 of 25: ./training/train.bucket8.txt\n",
      "2022-11-22 16:33:58,617 - INFO - input_processor - Done processing training file, batch size is 9456\n",
      "2022-11-22 16:33:58,618 - INFO - input_processor - Processing training file 5 of 25: ./training/train.bucket6.txt\n",
      "2022-11-22 16:33:58,625 - INFO - input_processor - Done processing training file, batch size is 11820\n",
      "2022-11-22 16:33:58,625 - INFO - input_processor - Processing training file 6 of 25: ./training/train.bucket0.txt\n",
      "2022-11-22 16:33:58,633 - INFO - input_processor - Done processing training file, batch size is 14184\n",
      "2022-11-22 16:33:58,634 - INFO - input_processor - Processing training file 7 of 25: ./training/train.bucket11.txt\n",
      "2022-11-22 16:33:58,643 - INFO - input_processor - Done processing training file, batch size is 16548\n",
      "2022-11-22 16:33:58,643 - INFO - input_processor - Processing training file 8 of 25: ./training/train.bucket2.txt\n",
      "2022-11-22 16:33:58,653 - INFO - input_processor - Done processing training file, batch size is 18912\n",
      "2022-11-22 16:33:58,654 - INFO - input_processor - Processing training file 9 of 25: ./training/train.bucket21.txt\n",
      "2022-11-22 16:33:58,664 - INFO - input_processor - Done processing training file, batch size is 21276\n",
      "2022-11-22 16:33:58,665 - INFO - input_processor - Processing training file 10 of 25: ./training/train.bucket4.txt\n",
      "2022-11-22 16:33:58,677 - INFO - input_processor - Done processing training file, batch size is 23640\n",
      "2022-11-22 16:33:58,677 - INFO - input_processor - Processing training file 11 of 25: ./training/train.bucket13.txt\n",
      "2022-11-22 16:33:58,690 - INFO - input_processor - Done processing training file, batch size is 26004\n",
      "2022-11-22 16:33:58,691 - INFO - input_processor - Processing training file 12 of 25: ./training/train.bucket1.txt\n",
      "2022-11-22 16:33:58,704 - INFO - input_processor - Done processing training file, batch size is 28368\n",
      "2022-11-22 16:33:58,705 - INFO - input_processor - Processing training file 13 of 25: ./training/train.bucket18.txt\n",
      "2022-11-22 16:33:58,719 - INFO - input_processor - Done processing training file, batch size is 30732\n",
      "2022-11-22 16:33:58,720 - INFO - input_processor - Processing training file 14 of 25: ./training/train.bucket15.txt\n",
      "2022-11-22 16:33:58,735 - INFO - input_processor - Done processing training file, batch size is 33096\n",
      "2022-11-22 16:33:58,735 - INFO - input_processor - Processing training file 15 of 25: ./training/train.bucket22.txt\n",
      "2022-11-22 16:33:58,752 - INFO - input_processor - Done processing training file, batch size is 35460\n",
      "2022-11-22 16:33:58,753 - INFO - input_processor - Processing training file 16 of 25: ./training/train.bucket17.txt\n",
      "2022-11-22 16:33:58,770 - INFO - input_processor - Done processing training file, batch size is 37824\n",
      "2022-11-22 16:33:58,770 - INFO - input_processor - Processing training file 17 of 25: ./training/train.bucket7.txt\n",
      "2022-11-22 16:33:58,789 - INFO - input_processor - Done processing training file, batch size is 40188\n",
      "2022-11-22 16:33:58,789 - INFO - input_processor - Processing training file 18 of 25: ./training/train.bucket19.txt\n",
      "2022-11-22 16:33:58,809 - INFO - input_processor - Done processing training file, batch size is 42552\n",
      "2022-11-22 16:33:58,809 - INFO - input_processor - Processing training file 19 of 25: ./training/train.bucket12.txt\n",
      "2022-11-22 16:33:58,830 - INFO - input_processor - Done processing training file, batch size is 44916\n",
      "2022-11-22 16:33:58,830 - INFO - input_processor - Processing training file 20 of 25: ./training/train.bucket9.txt\n",
      "2022-11-22 16:33:58,852 - INFO - input_processor - Done processing training file, batch size is 47280\n",
      "2022-11-22 16:33:58,852 - INFO - input_processor - Processing training file 21 of 25: ./training/train.bucket3.txt\n",
      "2022-11-22 16:33:58,874 - INFO - input_processor - Done processing training file, batch size is 49644\n",
      "2022-11-22 16:33:58,875 - INFO - input_processor - Processing training file 22 of 25: ./training/train.bucket24.txt\n",
      "2022-11-22 16:33:58,898 - INFO - input_processor - Done processing training file, batch size is 52008\n",
      "2022-11-22 16:33:58,899 - INFO - input_processor - Processing training file 23 of 25: ./training/train.bucket14.txt\n",
      "2022-11-22 16:33:58,922 - INFO - input_processor - Done processing training file, batch size is 54372\n",
      "2022-11-22 16:33:58,923 - INFO - input_processor - Processing training file 24 of 25: ./training/train.bucket10.txt\n",
      "2022-11-22 16:33:58,948 - INFO - input_processor - Done processing training file, batch size is 56736\n",
      "2022-11-22 16:33:58,948 - INFO - input_processor - Processing training file 25 of 25: ./training/train.bucket16.txt\n",
      "2022-11-22 16:33:58,974 - INFO - input_processor - Done processing training file, batch size is 59100\n",
      "2022-11-22 16:33:59,492 - INFO - train - Step: 100\tLoss: 0.01689387726597488\n",
      "2022-11-22 16:34:00,005 - INFO - train - Step: 200\tLoss: 0.01659535810351372\n",
      "2022-11-22 16:34:00,555 - INFO - train - Step: 300\tLoss: 0.01635274526663125\n",
      "2022-11-22 16:34:01,078 - INFO - train - Step: 400\tLoss: 0.01634562423452735\n",
      "2022-11-22 16:34:01,590 - INFO - train - Step: 500\tLoss: 0.01641113409772515\n",
      "2022-11-22 16:34:02,103 - INFO - train - Step: 600\tLoss: 0.01632495221681893\n",
      "2022-11-22 16:34:02,615 - INFO - train - Step: 700\tLoss: 0.01621518805623055\n",
      "2022-11-22 16:34:03,127 - INFO - train - Step: 800\tLoss: 0.01630125082097948\n",
      "2022-11-22 16:34:03,639 - INFO - train - Step: 900\tLoss: 0.01644111438654363\n",
      "2022-11-22 16:34:04,152 - INFO - train - Step: 1000\tLoss: 0.01638543698936701\n",
      "2022-11-22 16:34:04,669 - INFO - train - Step: 1100\tLoss: 0.01646064454689622\n",
      "2022-11-22 16:34:05,187 - INFO - train - Step: 1200\tLoss: 0.01665563463233411\n",
      "2022-11-22 16:34:05,700 - INFO - train - Step: 1300\tLoss: 0.01610588735900819\n",
      "2022-11-22 16:34:06,211 - INFO - train - Step: 1400\tLoss: 0.01647443843074143\n",
      "2022-11-22 16:34:06,723 - INFO - train - Step: 1500\tLoss: 0.01625506933778524\n",
      "2022-11-22 16:34:07,236 - INFO - train - Step: 1600\tLoss: 0.01641904802061617\n",
      "2022-11-22 16:34:07,749 - INFO - train - Step: 1700\tLoss: 0.01630634340457618\n",
      "2022-11-22 16:34:08,260 - INFO - train - Step: 1800\tLoss: 0.01638991124927998\n",
      "2022-11-22 16:34:08,492 - INFO - input_processor - Reached the end of the dataset\n",
      "2022-11-22 16:34:08,492 - INFO - train - Done with epoch 7\n",
      "Epoch:  40%|████      | 8/20 [01:19<01:58,  9.90s/it]2022-11-22 16:34:08,493 - INFO - input_processor - Processing training file 1 of 25: ./training/train.bucket24.txt\n",
      "2022-11-22 16:34:08,497 - INFO - input_processor - Done processing training file, batch size is 2364\n",
      "2022-11-22 16:34:08,498 - INFO - input_processor - Processing training file 2 of 25: ./training/train.bucket16.txt\n",
      "2022-11-22 16:34:08,502 - INFO - input_processor - Done processing training file, batch size is 4728\n",
      "2022-11-22 16:34:08,503 - INFO - input_processor - Processing training file 3 of 25: ./training/train.bucket9.txt\n",
      "2022-11-22 16:34:08,508 - INFO - input_processor - Done processing training file, batch size is 7092\n",
      "2022-11-22 16:34:08,508 - INFO - input_processor - Processing training file 4 of 25: ./training/train.bucket7.txt\n",
      "2022-11-22 16:34:08,516 - INFO - input_processor - Done processing training file, batch size is 9456\n",
      "2022-11-22 16:34:08,516 - INFO - input_processor - Processing training file 5 of 25: ./training/train.bucket3.txt\n",
      "2022-11-22 16:34:08,523 - INFO - input_processor - Done processing training file, batch size is 11820\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-22 16:34:08,524 - INFO - input_processor - Processing training file 6 of 25: ./training/train.bucket21.txt\n",
      "2022-11-22 16:34:08,532 - INFO - input_processor - Done processing training file, batch size is 14184\n",
      "2022-11-22 16:34:08,533 - INFO - input_processor - Processing training file 7 of 25: ./training/train.bucket17.txt\n",
      "2022-11-22 16:34:08,542 - INFO - input_processor - Done processing training file, batch size is 16548\n",
      "2022-11-22 16:34:08,542 - INFO - input_processor - Processing training file 8 of 25: ./training/train.bucket14.txt\n",
      "2022-11-22 16:34:08,552 - INFO - input_processor - Done processing training file, batch size is 18912\n",
      "2022-11-22 16:34:08,553 - INFO - input_processor - Processing training file 9 of 25: ./training/train.bucket13.txt\n",
      "2022-11-22 16:34:08,564 - INFO - input_processor - Done processing training file, batch size is 21276\n",
      "2022-11-22 16:34:08,565 - INFO - input_processor - Processing training file 10 of 25: ./training/train.bucket20.txt\n",
      "2022-11-22 16:34:08,577 - INFO - input_processor - Done processing training file, batch size is 23640\n",
      "2022-11-22 16:34:08,577 - INFO - input_processor - Processing training file 11 of 25: ./training/train.bucket0.txt\n",
      "2022-11-22 16:34:08,590 - INFO - input_processor - Done processing training file, batch size is 26004\n",
      "2022-11-22 16:34:08,591 - INFO - input_processor - Processing training file 12 of 25: ./training/train.bucket5.txt\n",
      "2022-11-22 16:34:08,605 - INFO - input_processor - Done processing training file, batch size is 28368\n",
      "2022-11-22 16:34:08,605 - INFO - input_processor - Processing training file 13 of 25: ./training/train.bucket2.txt\n",
      "2022-11-22 16:34:08,619 - INFO - input_processor - Done processing training file, batch size is 30732\n",
      "2022-11-22 16:34:08,620 - INFO - input_processor - Processing training file 14 of 25: ./training/train.bucket15.txt\n",
      "2022-11-22 16:34:08,635 - INFO - input_processor - Done processing training file, batch size is 33096\n",
      "2022-11-22 16:34:08,636 - INFO - input_processor - Processing training file 15 of 25: ./training/train.bucket19.txt\n",
      "2022-11-22 16:34:08,652 - INFO - input_processor - Done processing training file, batch size is 35460\n",
      "2022-11-22 16:34:08,653 - INFO - input_processor - Processing training file 16 of 25: ./training/train.bucket6.txt\n",
      "2022-11-22 16:34:08,670 - INFO - input_processor - Done processing training file, batch size is 37824\n",
      "2022-11-22 16:34:08,670 - INFO - input_processor - Processing training file 17 of 25: ./training/train.bucket8.txt\n",
      "2022-11-22 16:34:08,689 - INFO - input_processor - Done processing training file, batch size is 40188\n",
      "2022-11-22 16:34:08,690 - INFO - input_processor - Processing training file 18 of 25: ./training/train.bucket10.txt\n",
      "2022-11-22 16:34:08,709 - INFO - input_processor - Done processing training file, batch size is 42552\n",
      "2022-11-22 16:34:08,710 - INFO - input_processor - Processing training file 19 of 25: ./training/train.bucket11.txt\n",
      "2022-11-22 16:34:08,730 - INFO - input_processor - Done processing training file, batch size is 44916\n",
      "2022-11-22 16:34:08,731 - INFO - input_processor - Processing training file 20 of 25: ./training/train.bucket18.txt\n",
      "2022-11-22 16:34:08,752 - INFO - input_processor - Done processing training file, batch size is 47280\n",
      "2022-11-22 16:34:08,752 - INFO - input_processor - Processing training file 21 of 25: ./training/train.bucket4.txt\n",
      "2022-11-22 16:34:08,774 - INFO - input_processor - Done processing training file, batch size is 49644\n",
      "2022-11-22 16:34:08,775 - INFO - input_processor - Processing training file 22 of 25: ./training/train.bucket1.txt\n",
      "2022-11-22 16:34:08,798 - INFO - input_processor - Done processing training file, batch size is 52008\n",
      "2022-11-22 16:34:08,799 - INFO - input_processor - Processing training file 23 of 25: ./training/train.bucket23.txt\n",
      "2022-11-22 16:34:08,823 - INFO - input_processor - Done processing training file, batch size is 54372\n",
      "2022-11-22 16:34:08,823 - INFO - input_processor - Processing training file 24 of 25: ./training/train.bucket12.txt\n",
      "2022-11-22 16:34:08,848 - INFO - input_processor - Done processing training file, batch size is 56736\n",
      "2022-11-22 16:34:08,848 - INFO - input_processor - Processing training file 25 of 25: ./training/train.bucket22.txt\n",
      "2022-11-22 16:34:08,874 - INFO - input_processor - Done processing training file, batch size is 59100\n",
      "2022-11-22 16:34:09,392 - INFO - train - Step: 100\tLoss: 0.01652141514234245\n",
      "2022-11-22 16:34:09,905 - INFO - train - Step: 200\tLoss: 0.01645272483117878\n",
      "2022-11-22 16:34:10,417 - INFO - train - Step: 300\tLoss: 0.01653119419701397\n",
      "2022-11-22 16:34:10,930 - INFO - train - Step: 400\tLoss: 0.01632990140467882\n",
      "2022-11-22 16:34:11,442 - INFO - train - Step: 500\tLoss: 0.01634338549338281\n",
      "2022-11-22 16:34:11,954 - INFO - train - Step: 600\tLoss: 0.01614165389910340\n",
      "2022-11-22 16:34:12,466 - INFO - train - Step: 700\tLoss: 0.01622821527533233\n",
      "2022-11-22 16:34:12,980 - INFO - train - Step: 800\tLoss: 0.01636395038105547\n",
      "2022-11-22 16:34:13,491 - INFO - train - Step: 900\tLoss: 0.01651991378515959\n",
      "2022-11-22 16:34:14,003 - INFO - train - Step: 1000\tLoss: 0.01615949283353984\n",
      "2022-11-22 16:34:14,514 - INFO - train - Step: 1100\tLoss: 0.01626798765733838\n",
      "2022-11-22 16:34:15,026 - INFO - train - Step: 1200\tLoss: 0.01629500296898186\n",
      "2022-11-22 16:34:15,538 - INFO - train - Step: 1300\tLoss: 0.01666180097498000\n",
      "2022-11-22 16:34:16,050 - INFO - train - Step: 1400\tLoss: 0.01674808501265943\n",
      "2022-11-22 16:34:16,561 - INFO - train - Step: 1500\tLoss: 0.01659853002056479\n",
      "2022-11-22 16:34:17,072 - INFO - train - Step: 1600\tLoss: 0.01648078415542841\n",
      "2022-11-22 16:34:17,584 - INFO - train - Step: 1700\tLoss: 0.01662572186440230\n",
      "2022-11-22 16:34:18,095 - INFO - train - Step: 1800\tLoss: 0.01655881566926837\n",
      "2022-11-22 16:34:18,326 - INFO - input_processor - Reached the end of the dataset\n",
      "2022-11-22 16:34:18,326 - INFO - train - Done with epoch 8\n",
      "Epoch:  45%|████▌     | 9/20 [01:29<01:48,  9.88s/it]2022-11-22 16:34:18,327 - INFO - input_processor - Processing training file 1 of 25: ./training/train.bucket16.txt\n",
      "2022-11-22 16:34:18,331 - INFO - input_processor - Done processing training file, batch size is 2364\n",
      "2022-11-22 16:34:18,332 - INFO - input_processor - Processing training file 2 of 25: ./training/train.bucket21.txt\n",
      "2022-11-22 16:34:18,336 - INFO - input_processor - Done processing training file, batch size is 4728\n",
      "2022-11-22 16:34:18,337 - INFO - input_processor - Processing training file 3 of 25: ./training/train.bucket9.txt\n",
      "2022-11-22 16:34:18,342 - INFO - input_processor - Done processing training file, batch size is 7092\n",
      "2022-11-22 16:34:18,343 - INFO - input_processor - Processing training file 4 of 25: ./training/train.bucket2.txt\n",
      "2022-11-22 16:34:18,349 - INFO - input_processor - Done processing training file, batch size is 9456\n",
      "2022-11-22 16:34:18,349 - INFO - input_processor - Processing training file 5 of 25: ./training/train.bucket6.txt\n",
      "2022-11-22 16:34:18,357 - INFO - input_processor - Done processing training file, batch size is 11820\n",
      "2022-11-22 16:34:18,357 - INFO - input_processor - Processing training file 6 of 25: ./training/train.bucket22.txt\n",
      "2022-11-22 16:34:18,365 - INFO - input_processor - Done processing training file, batch size is 14184\n",
      "2022-11-22 16:34:18,365 - INFO - input_processor - Processing training file 7 of 25: ./training/train.bucket3.txt\n",
      "2022-11-22 16:34:18,374 - INFO - input_processor - Done processing training file, batch size is 16548\n",
      "2022-11-22 16:34:18,375 - INFO - input_processor - Processing training file 8 of 25: ./training/train.bucket14.txt\n",
      "2022-11-22 16:34:18,385 - INFO - input_processor - Done processing training file, batch size is 18912\n",
      "2022-11-22 16:34:18,385 - INFO - input_processor - Processing training file 9 of 25: ./training/train.bucket12.txt\n",
      "2022-11-22 16:34:18,396 - INFO - input_processor - Done processing training file, batch size is 21276\n",
      "2022-11-22 16:34:18,396 - INFO - input_processor - Processing training file 10 of 25: ./training/train.bucket7.txt\n",
      "2022-11-22 16:34:18,409 - INFO - input_processor - Done processing training file, batch size is 23640\n",
      "2022-11-22 16:34:18,410 - INFO - input_processor - Processing training file 11 of 25: ./training/train.bucket4.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-22 16:34:18,422 - INFO - input_processor - Done processing training file, batch size is 26004\n",
      "2022-11-22 16:34:18,423 - INFO - input_processor - Processing training file 12 of 25: ./training/train.bucket15.txt\n",
      "2022-11-22 16:34:18,436 - INFO - input_processor - Done processing training file, batch size is 28368\n",
      "2022-11-22 16:34:18,437 - INFO - input_processor - Processing training file 13 of 25: ./training/train.bucket18.txt\n",
      "2022-11-22 16:34:18,452 - INFO - input_processor - Done processing training file, batch size is 30732\n",
      "2022-11-22 16:34:18,452 - INFO - input_processor - Processing training file 14 of 25: ./training/train.bucket20.txt\n",
      "2022-11-22 16:34:18,468 - INFO - input_processor - Done processing training file, batch size is 33096\n",
      "2022-11-22 16:34:18,468 - INFO - input_processor - Processing training file 15 of 25: ./training/train.bucket1.txt\n",
      "2022-11-22 16:34:18,484 - INFO - input_processor - Done processing training file, batch size is 35460\n",
      "2022-11-22 16:34:18,485 - INFO - input_processor - Processing training file 16 of 25: ./training/train.bucket19.txt\n",
      "2022-11-22 16:34:18,502 - INFO - input_processor - Done processing training file, batch size is 37824\n",
      "2022-11-22 16:34:18,503 - INFO - input_processor - Processing training file 17 of 25: ./training/train.bucket13.txt\n",
      "2022-11-22 16:34:18,521 - INFO - input_processor - Done processing training file, batch size is 40188\n",
      "2022-11-22 16:34:18,521 - INFO - input_processor - Processing training file 18 of 25: ./training/train.bucket24.txt\n",
      "2022-11-22 16:34:18,541 - INFO - input_processor - Done processing training file, batch size is 42552\n",
      "2022-11-22 16:34:18,542 - INFO - input_processor - Processing training file 19 of 25: ./training/train.bucket0.txt\n",
      "2022-11-22 16:34:18,562 - INFO - input_processor - Done processing training file, batch size is 44916\n",
      "2022-11-22 16:34:18,563 - INFO - input_processor - Processing training file 20 of 25: ./training/train.bucket17.txt\n",
      "2022-11-22 16:34:18,585 - INFO - input_processor - Done processing training file, batch size is 47280\n",
      "2022-11-22 16:34:18,586 - INFO - input_processor - Processing training file 21 of 25: ./training/train.bucket23.txt\n",
      "2022-11-22 16:34:18,609 - INFO - input_processor - Done processing training file, batch size is 49644\n",
      "2022-11-22 16:34:18,609 - INFO - input_processor - Processing training file 22 of 25: ./training/train.bucket10.txt\n",
      "2022-11-22 16:34:18,633 - INFO - input_processor - Done processing training file, batch size is 52008\n",
      "2022-11-22 16:34:18,634 - INFO - input_processor - Processing training file 23 of 25: ./training/train.bucket11.txt\n",
      "2022-11-22 16:34:18,659 - INFO - input_processor - Done processing training file, batch size is 54372\n",
      "2022-11-22 16:34:18,659 - INFO - input_processor - Processing training file 24 of 25: ./training/train.bucket5.txt\n",
      "2022-11-22 16:34:18,685 - INFO - input_processor - Done processing training file, batch size is 56736\n",
      "2022-11-22 16:34:18,686 - INFO - input_processor - Processing training file 25 of 25: ./training/train.bucket8.txt\n",
      "2022-11-22 16:34:18,713 - INFO - input_processor - Done processing training file, batch size is 59100\n",
      "2022-11-22 16:34:19,239 - INFO - train - Step: 100\tLoss: 0.01663196680136025\n",
      "2022-11-22 16:34:19,764 - INFO - train - Step: 200\tLoss: 0.01650591730140150\n",
      "2022-11-22 16:34:20,312 - INFO - train - Step: 300\tLoss: 0.01659077766351402\n",
      "2022-11-22 16:34:20,837 - INFO - train - Step: 400\tLoss: 0.01643360889516771\n",
      "2022-11-22 16:34:21,349 - INFO - train - Step: 500\tLoss: 0.01651862231083214\n",
      "2022-11-22 16:34:21,860 - INFO - train - Step: 600\tLoss: 0.01642015495337546\n",
      "2022-11-22 16:34:22,372 - INFO - train - Step: 700\tLoss: 0.01638415489345789\n",
      "2022-11-22 16:34:22,885 - INFO - train - Step: 800\tLoss: 0.01655278998427093\n",
      "2022-11-22 16:34:23,398 - INFO - train - Step: 900\tLoss: 0.01619432996027172\n",
      "2022-11-22 16:34:23,910 - INFO - train - Step: 1000\tLoss: 0.01634015505202115\n",
      "2022-11-22 16:34:24,421 - INFO - train - Step: 1100\tLoss: 0.01694670462980866\n",
      "2022-11-22 16:34:24,933 - INFO - train - Step: 1200\tLoss: 0.01634728181175888\n",
      "2022-11-22 16:34:25,445 - INFO - train - Step: 1300\tLoss: 0.01634050616063178\n",
      "2022-11-22 16:34:25,958 - INFO - train - Step: 1400\tLoss: 0.01641254401765764\n",
      "2022-11-22 16:34:26,475 - INFO - train - Step: 1500\tLoss: 0.01653589129447937\n",
      "2022-11-22 16:34:26,988 - INFO - train - Step: 1600\tLoss: 0.01642689269036055\n",
      "2022-11-22 16:34:27,501 - INFO - train - Step: 1700\tLoss: 0.01642433119937778\n",
      "2022-11-22 16:34:28,013 - INFO - train - Step: 1800\tLoss: 0.01617423827759921\n",
      "2022-11-22 16:34:28,244 - INFO - input_processor - Reached the end of the dataset\n",
      "2022-11-22 16:34:28,245 - INFO - train - Done with epoch 9\n",
      "Epoch:  50%|█████     | 10/20 [01:39<01:38,  9.90s/it]2022-11-22 16:34:28,253 - INFO - input_processor - Processing training file 1 of 25: ./training/train.bucket13.txt\n",
      "2022-11-22 16:34:28,257 - INFO - input_processor - Done processing training file, batch size is 2364\n",
      "2022-11-22 16:34:28,258 - INFO - input_processor - Processing training file 2 of 25: ./training/train.bucket21.txt\n",
      "2022-11-22 16:34:28,262 - INFO - input_processor - Done processing training file, batch size is 4728\n",
      "2022-11-22 16:34:28,263 - INFO - input_processor - Processing training file 3 of 25: ./training/train.bucket8.txt\n",
      "2022-11-22 16:34:28,269 - INFO - input_processor - Done processing training file, batch size is 7092\n",
      "2022-11-22 16:34:28,269 - INFO - input_processor - Processing training file 4 of 25: ./training/train.bucket6.txt\n",
      "2022-11-22 16:34:28,276 - INFO - input_processor - Done processing training file, batch size is 9456\n",
      "2022-11-22 16:34:28,276 - INFO - input_processor - Processing training file 5 of 25: ./training/train.bucket14.txt\n",
      "2022-11-22 16:34:28,283 - INFO - input_processor - Done processing training file, batch size is 11820\n",
      "2022-11-22 16:34:28,284 - INFO - input_processor - Processing training file 6 of 25: ./training/train.bucket4.txt\n",
      "2022-11-22 16:34:28,292 - INFO - input_processor - Done processing training file, batch size is 14184\n",
      "2022-11-22 16:34:28,293 - INFO - input_processor - Processing training file 7 of 25: ./training/train.bucket23.txt\n",
      "2022-11-22 16:34:28,302 - INFO - input_processor - Done processing training file, batch size is 16548\n",
      "2022-11-22 16:34:28,302 - INFO - input_processor - Processing training file 8 of 25: ./training/train.bucket7.txt\n",
      "2022-11-22 16:34:28,313 - INFO - input_processor - Done processing training file, batch size is 18912\n",
      "2022-11-22 16:34:28,313 - INFO - input_processor - Processing training file 9 of 25: ./training/train.bucket20.txt\n",
      "2022-11-22 16:34:28,324 - INFO - input_processor - Done processing training file, batch size is 21276\n",
      "2022-11-22 16:34:28,325 - INFO - input_processor - Processing training file 10 of 25: ./training/train.bucket16.txt\n",
      "2022-11-22 16:34:28,337 - INFO - input_processor - Done processing training file, batch size is 23640\n",
      "2022-11-22 16:34:28,337 - INFO - input_processor - Processing training file 11 of 25: ./training/train.bucket9.txt\n",
      "2022-11-22 16:34:28,349 - INFO - input_processor - Done processing training file, batch size is 26004\n",
      "2022-11-22 16:34:28,350 - INFO - input_processor - Processing training file 12 of 25: ./training/train.bucket1.txt\n",
      "2022-11-22 16:34:28,363 - INFO - input_processor - Done processing training file, batch size is 28368\n",
      "2022-11-22 16:34:28,364 - INFO - input_processor - Processing training file 13 of 25: ./training/train.bucket11.txt\n",
      "2022-11-22 16:34:28,379 - INFO - input_processor - Done processing training file, batch size is 30732\n",
      "2022-11-22 16:34:28,379 - INFO - input_processor - Processing training file 14 of 25: ./training/train.bucket15.txt\n",
      "2022-11-22 16:34:28,395 - INFO - input_processor - Done processing training file, batch size is 33096\n",
      "2022-11-22 16:34:28,395 - INFO - input_processor - Processing training file 15 of 25: ./training/train.bucket5.txt\n",
      "2022-11-22 16:34:28,412 - INFO - input_processor - Done processing training file, batch size is 35460\n",
      "2022-11-22 16:34:28,412 - INFO - input_processor - Processing training file 16 of 25: ./training/train.bucket17.txt\n",
      "2022-11-22 16:34:28,432 - INFO - input_processor - Done processing training file, batch size is 37824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-22 16:34:28,432 - INFO - input_processor - Processing training file 17 of 25: ./training/train.bucket10.txt\n",
      "2022-11-22 16:34:28,451 - INFO - input_processor - Done processing training file, batch size is 40188\n",
      "2022-11-22 16:34:28,451 - INFO - input_processor - Processing training file 18 of 25: ./training/train.bucket24.txt\n",
      "2022-11-22 16:34:28,470 - INFO - input_processor - Done processing training file, batch size is 42552\n",
      "2022-11-22 16:34:28,470 - INFO - input_processor - Processing training file 19 of 25: ./training/train.bucket2.txt\n",
      "2022-11-22 16:34:28,490 - INFO - input_processor - Done processing training file, batch size is 44916\n",
      "2022-11-22 16:34:28,491 - INFO - input_processor - Processing training file 20 of 25: ./training/train.bucket12.txt\n",
      "2022-11-22 16:34:28,513 - INFO - input_processor - Done processing training file, batch size is 47280\n",
      "2022-11-22 16:34:28,513 - INFO - input_processor - Processing training file 21 of 25: ./training/train.bucket18.txt\n",
      "2022-11-22 16:34:28,535 - INFO - input_processor - Done processing training file, batch size is 49644\n",
      "2022-11-22 16:34:28,535 - INFO - input_processor - Processing training file 22 of 25: ./training/train.bucket19.txt\n",
      "2022-11-22 16:34:28,559 - INFO - input_processor - Done processing training file, batch size is 52008\n",
      "2022-11-22 16:34:28,559 - INFO - input_processor - Processing training file 23 of 25: ./training/train.bucket3.txt\n",
      "2022-11-22 16:34:28,584 - INFO - input_processor - Done processing training file, batch size is 54372\n",
      "2022-11-22 16:34:28,584 - INFO - input_processor - Processing training file 24 of 25: ./training/train.bucket0.txt\n",
      "2022-11-22 16:34:28,608 - INFO - input_processor - Done processing training file, batch size is 56736\n",
      "2022-11-22 16:34:28,609 - INFO - input_processor - Processing training file 25 of 25: ./training/train.bucket22.txt\n",
      "2022-11-22 16:34:28,634 - INFO - input_processor - Done processing training file, batch size is 59100\n",
      "2022-11-22 16:34:29,154 - INFO - train - Step: 100\tLoss: 0.01663130435161293\n",
      "2022-11-22 16:34:29,697 - INFO - train - Step: 200\tLoss: 0.01657918018288911\n",
      "2022-11-22 16:34:30,227 - INFO - train - Step: 300\tLoss: 0.01641499263234436\n",
      "2022-11-22 16:34:30,756 - INFO - train - Step: 400\tLoss: 0.01631156776100397\n",
      "2022-11-22 16:34:31,274 - INFO - train - Step: 500\tLoss: 0.01634819613769651\n",
      "2022-11-22 16:34:31,811 - INFO - train - Step: 600\tLoss: 0.01645045164972544\n",
      "2022-11-22 16:34:32,349 - INFO - train - Step: 700\tLoss: 0.01655924691818654\n",
      "2022-11-22 16:34:32,874 - INFO - train - Step: 800\tLoss: 0.01616660595871508\n",
      "2022-11-22 16:34:33,411 - INFO - train - Step: 900\tLoss: 0.01637368531897664\n",
      "2022-11-22 16:34:33,931 - INFO - train - Step: 1000\tLoss: 0.01646350561641157\n",
      "2022-11-22 16:34:34,453 - INFO - train - Step: 1100\tLoss: 0.01656751366332173\n",
      "2022-11-22 16:34:34,974 - INFO - train - Step: 1200\tLoss: 0.01622407321818173\n",
      "2022-11-22 16:34:35,495 - INFO - train - Step: 1300\tLoss: 0.01655997108668089\n",
      "2022-11-22 16:34:36,015 - INFO - train - Step: 1400\tLoss: 0.01642422222532332\n",
      "2022-11-22 16:34:36,533 - INFO - train - Step: 1500\tLoss: 0.01613372730091214\n",
      "2022-11-22 16:34:37,051 - INFO - train - Step: 1600\tLoss: 0.01653633560985327\n",
      "2022-11-22 16:34:37,570 - INFO - train - Step: 1700\tLoss: 0.01676138919778168\n",
      "2022-11-22 16:34:38,088 - INFO - train - Step: 1800\tLoss: 0.01629730723798275\n",
      "2022-11-22 16:34:38,322 - INFO - input_processor - Reached the end of the dataset\n",
      "2022-11-22 16:34:38,323 - INFO - train - Done with epoch 10\n",
      "Epoch:  55%|█████▌    | 11/20 [01:49<01:29,  9.95s/it]2022-11-22 16:34:38,324 - INFO - input_processor - Processing training file 1 of 25: ./training/train.bucket1.txt\n",
      "2022-11-22 16:34:38,328 - INFO - input_processor - Done processing training file, batch size is 2364\n",
      "2022-11-22 16:34:38,328 - INFO - input_processor - Processing training file 2 of 25: ./training/train.bucket24.txt\n",
      "2022-11-22 16:34:38,333 - INFO - input_processor - Done processing training file, batch size is 4728\n",
      "2022-11-22 16:34:38,333 - INFO - input_processor - Processing training file 3 of 25: ./training/train.bucket20.txt\n",
      "2022-11-22 16:34:38,339 - INFO - input_processor - Done processing training file, batch size is 7092\n",
      "2022-11-22 16:34:38,339 - INFO - input_processor - Processing training file 4 of 25: ./training/train.bucket23.txt\n",
      "2022-11-22 16:34:38,346 - INFO - input_processor - Done processing training file, batch size is 9456\n",
      "2022-11-22 16:34:38,346 - INFO - input_processor - Processing training file 5 of 25: ./training/train.bucket13.txt\n",
      "2022-11-22 16:34:38,353 - INFO - input_processor - Done processing training file, batch size is 11820\n",
      "2022-11-22 16:34:38,354 - INFO - input_processor - Processing training file 6 of 25: ./training/train.bucket15.txt\n",
      "2022-11-22 16:34:38,362 - INFO - input_processor - Done processing training file, batch size is 14184\n",
      "2022-11-22 16:34:38,362 - INFO - input_processor - Processing training file 7 of 25: ./training/train.bucket22.txt\n",
      "2022-11-22 16:34:38,372 - INFO - input_processor - Done processing training file, batch size is 16548\n",
      "2022-11-22 16:34:38,372 - INFO - input_processor - Processing training file 8 of 25: ./training/train.bucket7.txt\n",
      "2022-11-22 16:34:38,382 - INFO - input_processor - Done processing training file, batch size is 18912\n",
      "2022-11-22 16:34:38,383 - INFO - input_processor - Processing training file 9 of 25: ./training/train.bucket14.txt\n",
      "2022-11-22 16:34:38,394 - INFO - input_processor - Done processing training file, batch size is 21276\n",
      "2022-11-22 16:34:38,396 - INFO - input_processor - Processing training file 10 of 25: ./training/train.bucket4.txt\n",
      "2022-11-22 16:34:38,408 - INFO - input_processor - Done processing training file, batch size is 23640\n",
      "2022-11-22 16:34:38,408 - INFO - input_processor - Processing training file 11 of 25: ./training/train.bucket2.txt\n",
      "2022-11-22 16:34:38,421 - INFO - input_processor - Done processing training file, batch size is 26004\n",
      "2022-11-22 16:34:38,422 - INFO - input_processor - Processing training file 12 of 25: ./training/train.bucket21.txt\n",
      "2022-11-22 16:34:38,436 - INFO - input_processor - Done processing training file, batch size is 28368\n",
      "2022-11-22 16:34:38,436 - INFO - input_processor - Processing training file 13 of 25: ./training/train.bucket10.txt\n",
      "2022-11-22 16:34:38,451 - INFO - input_processor - Done processing training file, batch size is 30732\n",
      "2022-11-22 16:34:38,451 - INFO - input_processor - Processing training file 14 of 25: ./training/train.bucket11.txt\n",
      "2022-11-22 16:34:38,467 - INFO - input_processor - Done processing training file, batch size is 33096\n",
      "2022-11-22 16:34:38,468 - INFO - input_processor - Processing training file 15 of 25: ./training/train.bucket3.txt\n",
      "2022-11-22 16:34:38,485 - INFO - input_processor - Done processing training file, batch size is 35460\n",
      "2022-11-22 16:34:38,485 - INFO - input_processor - Processing training file 16 of 25: ./training/train.bucket5.txt\n",
      "2022-11-22 16:34:38,503 - INFO - input_processor - Done processing training file, batch size is 37824\n",
      "2022-11-22 16:34:38,503 - INFO - input_processor - Processing training file 17 of 25: ./training/train.bucket18.txt\n",
      "2022-11-22 16:34:38,521 - INFO - input_processor - Done processing training file, batch size is 40188\n",
      "2022-11-22 16:34:38,522 - INFO - input_processor - Processing training file 18 of 25: ./training/train.bucket19.txt\n",
      "2022-11-22 16:34:38,541 - INFO - input_processor - Done processing training file, batch size is 42552\n",
      "2022-11-22 16:34:38,542 - INFO - input_processor - Processing training file 19 of 25: ./training/train.bucket8.txt\n",
      "2022-11-22 16:34:38,562 - INFO - input_processor - Done processing training file, batch size is 44916\n",
      "2022-11-22 16:34:38,563 - INFO - input_processor - Processing training file 20 of 25: ./training/train.bucket12.txt\n",
      "2022-11-22 16:34:38,584 - INFO - input_processor - Done processing training file, batch size is 47280\n",
      "2022-11-22 16:34:38,585 - INFO - input_processor - Processing training file 21 of 25: ./training/train.bucket0.txt\n",
      "2022-11-22 16:34:38,607 - INFO - input_processor - Done processing training file, batch size is 49644\n",
      "2022-11-22 16:34:38,607 - INFO - input_processor - Processing training file 22 of 25: ./training/train.bucket16.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-22 16:34:38,630 - INFO - input_processor - Done processing training file, batch size is 52008\n",
      "2022-11-22 16:34:38,631 - INFO - input_processor - Processing training file 23 of 25: ./training/train.bucket6.txt\n",
      "2022-11-22 16:34:38,654 - INFO - input_processor - Done processing training file, batch size is 54372\n",
      "2022-11-22 16:34:38,655 - INFO - input_processor - Processing training file 24 of 25: ./training/train.bucket9.txt\n",
      "2022-11-22 16:34:38,680 - INFO - input_processor - Done processing training file, batch size is 56736\n",
      "2022-11-22 16:34:38,680 - INFO - input_processor - Processing training file 25 of 25: ./training/train.bucket17.txt\n",
      "2022-11-22 16:34:38,706 - INFO - input_processor - Done processing training file, batch size is 59100\n",
      "2022-11-22 16:34:39,232 - INFO - train - Step: 100\tLoss: 0.01680166333913803\n",
      "2022-11-22 16:34:39,750 - INFO - train - Step: 200\tLoss: 0.01669865206815302\n",
      "2022-11-22 16:34:40,268 - INFO - train - Step: 300\tLoss: 0.01644291151314974\n",
      "2022-11-22 16:34:40,785 - INFO - train - Step: 400\tLoss: 0.01638813665136695\n",
      "2022-11-22 16:34:41,303 - INFO - train - Step: 500\tLoss: 0.01637809230014682\n",
      "2022-11-22 16:34:41,821 - INFO - train - Step: 600\tLoss: 0.01622959740459919\n",
      "2022-11-22 16:34:42,341 - INFO - train - Step: 700\tLoss: 0.01631988656707108\n",
      "2022-11-22 16:34:42,860 - INFO - train - Step: 800\tLoss: 0.01660361207090318\n",
      "2022-11-22 16:34:43,378 - INFO - train - Step: 900\tLoss: 0.01625807086005807\n",
      "2022-11-22 16:34:43,896 - INFO - train - Step: 1000\tLoss: 0.01586696945130825\n",
      "2022-11-22 16:34:44,415 - INFO - train - Step: 1100\tLoss: 0.01638308737426996\n",
      "2022-11-22 16:34:44,934 - INFO - train - Step: 1200\tLoss: 0.01620540994219482\n",
      "2022-11-22 16:34:45,454 - INFO - train - Step: 1300\tLoss: 0.01644579678773880\n",
      "2022-11-22 16:34:45,972 - INFO - train - Step: 1400\tLoss: 0.01673665582202375\n",
      "2022-11-22 16:34:46,490 - INFO - train - Step: 1500\tLoss: 0.01648133398965001\n",
      "2022-11-22 16:34:47,007 - INFO - train - Step: 1600\tLoss: 0.01660331824794412\n",
      "2022-11-22 16:34:47,525 - INFO - train - Step: 1700\tLoss: 0.01651429224759340\n",
      "2022-11-22 16:34:48,043 - INFO - train - Step: 1800\tLoss: 0.01648362201638520\n",
      "2022-11-22 16:34:48,278 - INFO - input_processor - Reached the end of the dataset\n",
      "2022-11-22 16:34:48,279 - INFO - train - Done with epoch 11\n",
      "Epoch:  60%|██████    | 12/20 [01:59<01:19,  9.95s/it]2022-11-22 16:34:48,280 - INFO - input_processor - Processing training file 1 of 25: ./training/train.bucket11.txt\n",
      "2022-11-22 16:34:48,284 - INFO - input_processor - Done processing training file, batch size is 2364\n",
      "2022-11-22 16:34:48,284 - INFO - input_processor - Processing training file 2 of 25: ./training/train.bucket8.txt\n",
      "2022-11-22 16:34:48,289 - INFO - input_processor - Done processing training file, batch size is 4728\n",
      "2022-11-22 16:34:48,289 - INFO - input_processor - Processing training file 3 of 25: ./training/train.bucket7.txt\n",
      "2022-11-22 16:34:48,294 - INFO - input_processor - Done processing training file, batch size is 7092\n",
      "2022-11-22 16:34:48,295 - INFO - input_processor - Processing training file 4 of 25: ./training/train.bucket15.txt\n",
      "2022-11-22 16:34:48,301 - INFO - input_processor - Done processing training file, batch size is 9456\n",
      "2022-11-22 16:34:48,302 - INFO - input_processor - Processing training file 5 of 25: ./training/train.bucket16.txt\n",
      "2022-11-22 16:34:48,309 - INFO - input_processor - Done processing training file, batch size is 11820\n",
      "2022-11-22 16:34:48,310 - INFO - input_processor - Processing training file 6 of 25: ./training/train.bucket10.txt\n",
      "2022-11-22 16:34:48,318 - INFO - input_processor - Done processing training file, batch size is 14184\n",
      "2022-11-22 16:34:48,319 - INFO - input_processor - Processing training file 7 of 25: ./training/train.bucket13.txt\n",
      "2022-11-22 16:34:48,328 - INFO - input_processor - Done processing training file, batch size is 16548\n",
      "2022-11-22 16:34:48,328 - INFO - input_processor - Processing training file 8 of 25: ./training/train.bucket0.txt\n",
      "2022-11-22 16:34:48,338 - INFO - input_processor - Done processing training file, batch size is 18912\n",
      "2022-11-22 16:34:48,339 - INFO - input_processor - Processing training file 9 of 25: ./training/train.bucket21.txt\n",
      "2022-11-22 16:34:48,350 - INFO - input_processor - Done processing training file, batch size is 21276\n",
      "2022-11-22 16:34:48,351 - INFO - input_processor - Processing training file 10 of 25: ./training/train.bucket4.txt\n",
      "2022-11-22 16:34:48,362 - INFO - input_processor - Done processing training file, batch size is 23640\n",
      "2022-11-22 16:34:48,363 - INFO - input_processor - Processing training file 11 of 25: ./training/train.bucket6.txt\n",
      "2022-11-22 16:34:48,376 - INFO - input_processor - Done processing training file, batch size is 26004\n",
      "2022-11-22 16:34:48,376 - INFO - input_processor - Processing training file 12 of 25: ./training/train.bucket12.txt\n",
      "2022-11-22 16:34:48,390 - INFO - input_processor - Done processing training file, batch size is 28368\n",
      "2022-11-22 16:34:48,391 - INFO - input_processor - Processing training file 13 of 25: ./training/train.bucket1.txt\n",
      "2022-11-22 16:34:48,405 - INFO - input_processor - Done processing training file, batch size is 30732\n",
      "2022-11-22 16:34:48,406 - INFO - input_processor - Processing training file 14 of 25: ./training/train.bucket19.txt\n",
      "2022-11-22 16:34:48,421 - INFO - input_processor - Done processing training file, batch size is 33096\n",
      "2022-11-22 16:34:48,422 - INFO - input_processor - Processing training file 15 of 25: ./training/train.bucket9.txt\n",
      "2022-11-22 16:34:48,438 - INFO - input_processor - Done processing training file, batch size is 35460\n",
      "2022-11-22 16:34:48,439 - INFO - input_processor - Processing training file 16 of 25: ./training/train.bucket23.txt\n",
      "2022-11-22 16:34:48,456 - INFO - input_processor - Done processing training file, batch size is 37824\n",
      "2022-11-22 16:34:48,457 - INFO - input_processor - Processing training file 17 of 25: ./training/train.bucket17.txt\n",
      "2022-11-22 16:34:48,475 - INFO - input_processor - Done processing training file, batch size is 40188\n",
      "2022-11-22 16:34:48,475 - INFO - input_processor - Processing training file 18 of 25: ./training/train.bucket2.txt\n",
      "2022-11-22 16:34:48,495 - INFO - input_processor - Done processing training file, batch size is 42552\n",
      "2022-11-22 16:34:48,495 - INFO - input_processor - Processing training file 19 of 25: ./training/train.bucket14.txt\n",
      "2022-11-22 16:34:48,516 - INFO - input_processor - Done processing training file, batch size is 44916\n",
      "2022-11-22 16:34:48,516 - INFO - input_processor - Processing training file 20 of 25: ./training/train.bucket3.txt\n",
      "2022-11-22 16:34:48,537 - INFO - input_processor - Done processing training file, batch size is 47280\n",
      "2022-11-22 16:34:48,538 - INFO - input_processor - Processing training file 21 of 25: ./training/train.bucket18.txt\n",
      "2022-11-22 16:34:48,560 - INFO - input_processor - Done processing training file, batch size is 49644\n",
      "2022-11-22 16:34:48,560 - INFO - input_processor - Processing training file 22 of 25: ./training/train.bucket20.txt\n",
      "2022-11-22 16:34:48,583 - INFO - input_processor - Done processing training file, batch size is 52008\n",
      "2022-11-22 16:34:48,584 - INFO - input_processor - Processing training file 23 of 25: ./training/train.bucket5.txt\n",
      "2022-11-22 16:34:48,608 - INFO - input_processor - Done processing training file, batch size is 54372\n",
      "2022-11-22 16:34:48,608 - INFO - input_processor - Processing training file 24 of 25: ./training/train.bucket22.txt\n",
      "2022-11-22 16:34:48,633 - INFO - input_processor - Done processing training file, batch size is 56736\n",
      "2022-11-22 16:34:48,633 - INFO - input_processor - Processing training file 25 of 25: ./training/train.bucket24.txt\n",
      "2022-11-22 16:34:48,659 - INFO - input_processor - Done processing training file, batch size is 59100\n",
      "2022-11-22 16:34:49,183 - INFO - train - Step: 100\tLoss: 0.01654372749850154\n",
      "2022-11-22 16:34:49,716 - INFO - train - Step: 200\tLoss: 0.01683306046761572\n",
      "2022-11-22 16:34:50,273 - INFO - train - Step: 300\tLoss: 0.01630156713537872\n",
      "2022-11-22 16:34:50,822 - INFO - train - Step: 400\tLoss: 0.01672092421911657\n",
      "2022-11-22 16:34:51,341 - INFO - train - Step: 500\tLoss: 0.01648291761986911\n",
      "2022-11-22 16:34:51,859 - INFO - train - Step: 600\tLoss: 0.01650349154137075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-22 16:34:52,377 - INFO - train - Step: 700\tLoss: 0.01645132266916335\n",
      "2022-11-22 16:34:52,894 - INFO - train - Step: 800\tLoss: 0.01639006614685059\n",
      "2022-11-22 16:34:53,413 - INFO - train - Step: 900\tLoss: 0.01652480167336762\n",
      "2022-11-22 16:34:53,938 - INFO - train - Step: 1000\tLoss: 0.01639261065982282\n",
      "2022-11-22 16:34:54,456 - INFO - train - Step: 1100\tLoss: 0.01649440824054181\n",
      "2022-11-22 16:34:54,976 - INFO - train - Step: 1200\tLoss: 0.01653463832102716\n",
      "2022-11-22 16:34:55,493 - INFO - train - Step: 1300\tLoss: 0.01656381078995764\n",
      "2022-11-22 16:34:56,011 - INFO - train - Step: 1400\tLoss: 0.01626514077186585\n",
      "2022-11-22 16:34:56,529 - INFO - train - Step: 1500\tLoss: 0.01618225459940732\n",
      "2022-11-22 16:34:57,046 - INFO - train - Step: 1600\tLoss: 0.01608058172278106\n",
      "2022-11-22 16:34:57,563 - INFO - train - Step: 1700\tLoss: 0.01613532103598118\n",
      "2022-11-22 16:34:58,082 - INFO - train - Step: 1800\tLoss: 0.01657719362527132\n",
      "2022-11-22 16:34:58,315 - INFO - input_processor - Reached the end of the dataset\n",
      "2022-11-22 16:34:58,316 - INFO - train - Done with epoch 12\n",
      "Epoch:  65%|██████▌   | 13/20 [02:09<01:09,  9.98s/it]2022-11-22 16:34:58,317 - INFO - input_processor - Processing training file 1 of 25: ./training/train.bucket17.txt\n",
      "2022-11-22 16:34:58,321 - INFO - input_processor - Done processing training file, batch size is 2364\n",
      "2022-11-22 16:34:58,321 - INFO - input_processor - Processing training file 2 of 25: ./training/train.bucket9.txt\n",
      "2022-11-22 16:34:58,326 - INFO - input_processor - Done processing training file, batch size is 4728\n",
      "2022-11-22 16:34:58,326 - INFO - input_processor - Processing training file 3 of 25: ./training/train.bucket21.txt\n",
      "2022-11-22 16:34:58,331 - INFO - input_processor - Done processing training file, batch size is 7092\n",
      "2022-11-22 16:34:58,332 - INFO - input_processor - Processing training file 4 of 25: ./training/train.bucket7.txt\n",
      "2022-11-22 16:34:58,338 - INFO - input_processor - Done processing training file, batch size is 9456\n",
      "2022-11-22 16:34:58,338 - INFO - input_processor - Processing training file 5 of 25: ./training/train.bucket1.txt\n",
      "2022-11-22 16:34:58,347 - INFO - input_processor - Done processing training file, batch size is 11820\n",
      "2022-11-22 16:34:58,347 - INFO - input_processor - Processing training file 6 of 25: ./training/train.bucket13.txt\n",
      "2022-11-22 16:34:58,355 - INFO - input_processor - Done processing training file, batch size is 14184\n",
      "2022-11-22 16:34:58,355 - INFO - input_processor - Processing training file 7 of 25: ./training/train.bucket12.txt\n",
      "2022-11-22 16:34:58,364 - INFO - input_processor - Done processing training file, batch size is 16548\n",
      "2022-11-22 16:34:58,364 - INFO - input_processor - Processing training file 8 of 25: ./training/train.bucket4.txt\n",
      "2022-11-22 16:34:58,375 - INFO - input_processor - Done processing training file, batch size is 18912\n",
      "2022-11-22 16:34:58,375 - INFO - input_processor - Processing training file 9 of 25: ./training/train.bucket2.txt\n",
      "2022-11-22 16:34:58,386 - INFO - input_processor - Done processing training file, batch size is 21276\n",
      "2022-11-22 16:34:58,387 - INFO - input_processor - Processing training file 10 of 25: ./training/train.bucket5.txt\n",
      "2022-11-22 16:34:58,398 - INFO - input_processor - Done processing training file, batch size is 23640\n",
      "2022-11-22 16:34:58,399 - INFO - input_processor - Processing training file 11 of 25: ./training/train.bucket22.txt\n",
      "2022-11-22 16:34:58,411 - INFO - input_processor - Done processing training file, batch size is 26004\n",
      "2022-11-22 16:34:58,412 - INFO - input_processor - Processing training file 12 of 25: ./training/train.bucket6.txt\n",
      "2022-11-22 16:34:58,426 - INFO - input_processor - Done processing training file, batch size is 28368\n",
      "2022-11-22 16:34:58,427 - INFO - input_processor - Processing training file 13 of 25: ./training/train.bucket10.txt\n",
      "2022-11-22 16:34:58,441 - INFO - input_processor - Done processing training file, batch size is 30732\n",
      "2022-11-22 16:34:58,442 - INFO - input_processor - Processing training file 14 of 25: ./training/train.bucket20.txt\n",
      "2022-11-22 16:34:58,457 - INFO - input_processor - Done processing training file, batch size is 33096\n",
      "2022-11-22 16:34:58,458 - INFO - input_processor - Processing training file 15 of 25: ./training/train.bucket11.txt\n",
      "2022-11-22 16:34:58,474 - INFO - input_processor - Done processing training file, batch size is 35460\n",
      "2022-11-22 16:34:58,474 - INFO - input_processor - Processing training file 16 of 25: ./training/train.bucket0.txt\n",
      "2022-11-22 16:34:58,492 - INFO - input_processor - Done processing training file, batch size is 37824\n",
      "2022-11-22 16:34:58,492 - INFO - input_processor - Processing training file 17 of 25: ./training/train.bucket16.txt\n",
      "2022-11-22 16:34:58,511 - INFO - input_processor - Done processing training file, batch size is 40188\n",
      "2022-11-22 16:34:58,511 - INFO - input_processor - Processing training file 18 of 25: ./training/train.bucket15.txt\n",
      "2022-11-22 16:34:58,530 - INFO - input_processor - Done processing training file, batch size is 42552\n",
      "2022-11-22 16:34:58,531 - INFO - input_processor - Processing training file 19 of 25: ./training/train.bucket23.txt\n",
      "2022-11-22 16:34:58,551 - INFO - input_processor - Done processing training file, batch size is 44916\n",
      "2022-11-22 16:34:58,551 - INFO - input_processor - Processing training file 20 of 25: ./training/train.bucket14.txt\n",
      "2022-11-22 16:34:58,573 - INFO - input_processor - Done processing training file, batch size is 47280\n",
      "2022-11-22 16:34:58,573 - INFO - input_processor - Processing training file 21 of 25: ./training/train.bucket24.txt\n",
      "2022-11-22 16:34:58,595 - INFO - input_processor - Done processing training file, batch size is 49644\n",
      "2022-11-22 16:34:58,596 - INFO - input_processor - Processing training file 22 of 25: ./training/train.bucket19.txt\n",
      "2022-11-22 16:34:58,619 - INFO - input_processor - Done processing training file, batch size is 52008\n",
      "2022-11-22 16:34:58,619 - INFO - input_processor - Processing training file 23 of 25: ./training/train.bucket8.txt\n",
      "2022-11-22 16:34:58,643 - INFO - input_processor - Done processing training file, batch size is 54372\n",
      "2022-11-22 16:34:58,643 - INFO - input_processor - Processing training file 24 of 25: ./training/train.bucket3.txt\n",
      "2022-11-22 16:34:58,668 - INFO - input_processor - Done processing training file, batch size is 56736\n",
      "2022-11-22 16:34:58,668 - INFO - input_processor - Processing training file 25 of 25: ./training/train.bucket18.txt\n",
      "2022-11-22 16:34:58,694 - INFO - input_processor - Done processing training file, batch size is 59100\n",
      "2022-11-22 16:34:59,218 - INFO - train - Step: 100\tLoss: 0.01693945575505495\n",
      "2022-11-22 16:34:59,734 - INFO - train - Step: 200\tLoss: 0.01633343432098627\n",
      "2022-11-22 16:35:00,259 - INFO - train - Step: 300\tLoss: 0.01641179339960218\n",
      "2022-11-22 16:35:00,803 - INFO - train - Step: 400\tLoss: 0.01640219322405756\n",
      "2022-11-22 16:35:01,332 - INFO - train - Step: 500\tLoss: 0.01627306574955583\n",
      "2022-11-22 16:35:01,850 - INFO - train - Step: 600\tLoss: 0.01635252560488880\n",
      "2022-11-22 16:35:02,367 - INFO - train - Step: 700\tLoss: 0.01613096865825355\n",
      "2022-11-22 16:35:02,886 - INFO - train - Step: 800\tLoss: 0.01633788846433163\n",
      "2022-11-22 16:35:03,403 - INFO - train - Step: 900\tLoss: 0.01654435564763844\n",
      "2022-11-22 16:35:03,921 - INFO - train - Step: 1000\tLoss: 0.01644613047130406\n",
      "2022-11-22 16:35:04,439 - INFO - train - Step: 1100\tLoss: 0.01635081436485052\n",
      "2022-11-22 16:35:04,983 - INFO - train - Step: 1200\tLoss: 0.01675968158990145\n",
      "2022-11-22 16:35:05,513 - INFO - train - Step: 1300\tLoss: 0.01650620456784964\n",
      "2022-11-22 16:35:06,031 - INFO - train - Step: 1400\tLoss: 0.01626372235827148\n",
      "2022-11-22 16:35:06,548 - INFO - train - Step: 1500\tLoss: 0.01646448411978781\n",
      "2022-11-22 16:35:07,067 - INFO - train - Step: 1600\tLoss: 0.01622730560600757\n",
      "2022-11-22 16:35:07,587 - INFO - train - Step: 1700\tLoss: 0.01633418269455433\n",
      "2022-11-22 16:35:08,105 - INFO - train - Step: 1800\tLoss: 0.01627159127965570\n",
      "2022-11-22 16:35:08,339 - INFO - input_processor - Reached the end of the dataset\n",
      "2022-11-22 16:35:08,339 - INFO - train - Done with epoch 13\n",
      "Epoch:  70%|███████   | 14/20 [02:19<00:59,  9.99s/it]2022-11-22 16:35:08,340 - INFO - input_processor - Processing training file 1 of 25: ./training/train.bucket21.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-22 16:35:08,345 - INFO - input_processor - Done processing training file, batch size is 2364\n",
      "2022-11-22 16:35:08,345 - INFO - input_processor - Processing training file 2 of 25: ./training/train.bucket4.txt\n",
      "2022-11-22 16:35:08,350 - INFO - input_processor - Done processing training file, batch size is 4728\n",
      "2022-11-22 16:35:08,350 - INFO - input_processor - Processing training file 3 of 25: ./training/train.bucket3.txt\n",
      "2022-11-22 16:35:08,355 - INFO - input_processor - Done processing training file, batch size is 7092\n",
      "2022-11-22 16:35:08,356 - INFO - input_processor - Processing training file 4 of 25: ./training/train.bucket13.txt\n",
      "2022-11-22 16:35:08,362 - INFO - input_processor - Done processing training file, batch size is 9456\n",
      "2022-11-22 16:35:08,362 - INFO - input_processor - Processing training file 5 of 25: ./training/train.bucket7.txt\n",
      "2022-11-22 16:35:08,370 - INFO - input_processor - Done processing training file, batch size is 11820\n",
      "2022-11-22 16:35:08,370 - INFO - input_processor - Processing training file 6 of 25: ./training/train.bucket2.txt\n",
      "2022-11-22 16:35:08,378 - INFO - input_processor - Done processing training file, batch size is 14184\n",
      "2022-11-22 16:35:08,378 - INFO - input_processor - Processing training file 7 of 25: ./training/train.bucket22.txt\n",
      "2022-11-22 16:35:08,387 - INFO - input_processor - Done processing training file, batch size is 16548\n",
      "2022-11-22 16:35:08,388 - INFO - input_processor - Processing training file 8 of 25: ./training/train.bucket1.txt\n",
      "2022-11-22 16:35:08,398 - INFO - input_processor - Done processing training file, batch size is 18912\n",
      "2022-11-22 16:35:08,399 - INFO - input_processor - Processing training file 9 of 25: ./training/train.bucket9.txt\n",
      "2022-11-22 16:35:08,410 - INFO - input_processor - Done processing training file, batch size is 21276\n",
      "2022-11-22 16:35:08,411 - INFO - input_processor - Processing training file 10 of 25: ./training/train.bucket20.txt\n",
      "2022-11-22 16:35:08,422 - INFO - input_processor - Done processing training file, batch size is 23640\n",
      "2022-11-22 16:35:08,423 - INFO - input_processor - Processing training file 11 of 25: ./training/train.bucket24.txt\n",
      "2022-11-22 16:35:08,435 - INFO - input_processor - Done processing training file, batch size is 26004\n",
      "2022-11-22 16:35:08,436 - INFO - input_processor - Processing training file 12 of 25: ./training/train.bucket15.txt\n",
      "2022-11-22 16:35:08,449 - INFO - input_processor - Done processing training file, batch size is 28368\n",
      "2022-11-22 16:35:08,450 - INFO - input_processor - Processing training file 13 of 25: ./training/train.bucket19.txt\n",
      "2022-11-22 16:35:08,464 - INFO - input_processor - Done processing training file, batch size is 30732\n",
      "2022-11-22 16:35:08,465 - INFO - input_processor - Processing training file 14 of 25: ./training/train.bucket5.txt\n",
      "2022-11-22 16:35:08,480 - INFO - input_processor - Done processing training file, batch size is 33096\n",
      "2022-11-22 16:35:08,480 - INFO - input_processor - Processing training file 15 of 25: ./training/train.bucket16.txt\n",
      "2022-11-22 16:35:08,497 - INFO - input_processor - Done processing training file, batch size is 35460\n",
      "2022-11-22 16:35:08,497 - INFO - input_processor - Processing training file 16 of 25: ./training/train.bucket0.txt\n",
      "2022-11-22 16:35:08,515 - INFO - input_processor - Done processing training file, batch size is 37824\n",
      "2022-11-22 16:35:08,515 - INFO - input_processor - Processing training file 17 of 25: ./training/train.bucket23.txt\n",
      "2022-11-22 16:35:08,533 - INFO - input_processor - Done processing training file, batch size is 40188\n",
      "2022-11-22 16:35:08,534 - INFO - input_processor - Processing training file 18 of 25: ./training/train.bucket18.txt\n",
      "2022-11-22 16:35:08,553 - INFO - input_processor - Done processing training file, batch size is 42552\n",
      "2022-11-22 16:35:08,553 - INFO - input_processor - Processing training file 19 of 25: ./training/train.bucket14.txt\n",
      "2022-11-22 16:35:08,574 - INFO - input_processor - Done processing training file, batch size is 44916\n",
      "2022-11-22 16:35:08,574 - INFO - input_processor - Processing training file 20 of 25: ./training/train.bucket6.txt\n",
      "2022-11-22 16:35:08,596 - INFO - input_processor - Done processing training file, batch size is 47280\n",
      "2022-11-22 16:35:08,596 - INFO - input_processor - Processing training file 21 of 25: ./training/train.bucket8.txt\n",
      "2022-11-22 16:35:08,618 - INFO - input_processor - Done processing training file, batch size is 49644\n",
      "2022-11-22 16:35:08,619 - INFO - input_processor - Processing training file 22 of 25: ./training/train.bucket17.txt\n",
      "2022-11-22 16:35:08,642 - INFO - input_processor - Done processing training file, batch size is 52008\n",
      "2022-11-22 16:35:08,643 - INFO - input_processor - Processing training file 23 of 25: ./training/train.bucket11.txt\n",
      "2022-11-22 16:35:08,668 - INFO - input_processor - Done processing training file, batch size is 54372\n",
      "2022-11-22 16:35:08,668 - INFO - input_processor - Processing training file 24 of 25: ./training/train.bucket10.txt\n",
      "2022-11-22 16:35:08,694 - INFO - input_processor - Done processing training file, batch size is 56736\n",
      "2022-11-22 16:35:08,695 - INFO - input_processor - Processing training file 25 of 25: ./training/train.bucket12.txt\n",
      "2022-11-22 16:35:08,722 - INFO - input_processor - Done processing training file, batch size is 59100\n",
      "2022-11-22 16:35:09,270 - INFO - train - Step: 100\tLoss: 0.01681037928909063\n",
      "2022-11-22 16:35:09,787 - INFO - train - Step: 200\tLoss: 0.01681371689774096\n",
      "2022-11-22 16:35:10,306 - INFO - train - Step: 300\tLoss: 0.01657846494577825\n",
      "2022-11-22 16:35:10,824 - INFO - train - Step: 400\tLoss: 0.01607689942233264\n",
      "2022-11-22 16:35:11,343 - INFO - train - Step: 500\tLoss: 0.01643823821097612\n",
      "2022-11-22 16:35:11,861 - INFO - train - Step: 600\tLoss: 0.01606912658549845\n",
      "2022-11-22 16:35:12,380 - INFO - train - Step: 700\tLoss: 0.01624116736464203\n",
      "2022-11-22 16:35:12,911 - INFO - train - Step: 800\tLoss: 0.01639374637976289\n",
      "2022-11-22 16:35:13,454 - INFO - train - Step: 900\tLoss: 0.01663217629306018\n",
      "2022-11-22 16:35:13,985 - INFO - train - Step: 1000\tLoss: 0.01641510976478457\n",
      "2022-11-22 16:35:14,516 - INFO - train - Step: 1100\tLoss: 0.01655239407904446\n",
      "2022-11-22 16:35:15,037 - INFO - train - Step: 1200\tLoss: 0.01646465156227350\n",
      "2022-11-22 16:35:15,553 - INFO - train - Step: 1300\tLoss: 0.01625608277507126\n",
      "2022-11-22 16:35:16,070 - INFO - train - Step: 1400\tLoss: 0.01637700906954706\n",
      "2022-11-22 16:35:16,591 - INFO - train - Step: 1500\tLoss: 0.01674083044752479\n",
      "2022-11-22 16:35:17,108 - INFO - train - Step: 1600\tLoss: 0.01641976426355541\n",
      "2022-11-22 16:35:17,625 - INFO - train - Step: 1700\tLoss: 0.01643976784311235\n",
      "2022-11-22 16:35:18,149 - INFO - train - Step: 1800\tLoss: 0.01629214775748551\n",
      "2022-11-22 16:35:18,389 - INFO - input_processor - Reached the end of the dataset\n",
      "2022-11-22 16:35:18,389 - INFO - train - Done with epoch 14\n",
      "Epoch:  75%|███████▌  | 15/20 [02:29<00:50, 10.01s/it]2022-11-22 16:35:18,391 - INFO - input_processor - Processing training file 1 of 25: ./training/train.bucket3.txt\n",
      "2022-11-22 16:35:18,395 - INFO - input_processor - Done processing training file, batch size is 2364\n",
      "2022-11-22 16:35:18,395 - INFO - input_processor - Processing training file 2 of 25: ./training/train.bucket22.txt\n",
      "2022-11-22 16:35:18,400 - INFO - input_processor - Done processing training file, batch size is 4728\n",
      "2022-11-22 16:35:18,400 - INFO - input_processor - Processing training file 3 of 25: ./training/train.bucket7.txt\n",
      "2022-11-22 16:35:18,406 - INFO - input_processor - Done processing training file, batch size is 7092\n",
      "2022-11-22 16:35:18,406 - INFO - input_processor - Processing training file 4 of 25: ./training/train.bucket6.txt\n",
      "2022-11-22 16:35:18,413 - INFO - input_processor - Done processing training file, batch size is 9456\n",
      "2022-11-22 16:35:18,414 - INFO - input_processor - Processing training file 5 of 25: ./training/train.bucket17.txt\n",
      "2022-11-22 16:35:18,422 - INFO - input_processor - Done processing training file, batch size is 11820\n",
      "2022-11-22 16:35:18,422 - INFO - input_processor - Processing training file 6 of 25: ./training/train.bucket24.txt\n",
      "2022-11-22 16:35:18,431 - INFO - input_processor - Done processing training file, batch size is 14184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-22 16:35:18,431 - INFO - input_processor - Processing training file 7 of 25: ./training/train.bucket13.txt\n",
      "2022-11-22 16:35:18,442 - INFO - input_processor - Done processing training file, batch size is 16548\n",
      "2022-11-22 16:35:18,442 - INFO - input_processor - Processing training file 8 of 25: ./training/train.bucket10.txt\n",
      "2022-11-22 16:35:18,452 - INFO - input_processor - Done processing training file, batch size is 18912\n",
      "2022-11-22 16:35:18,453 - INFO - input_processor - Processing training file 9 of 25: ./training/train.bucket15.txt\n",
      "2022-11-22 16:35:18,464 - INFO - input_processor - Done processing training file, batch size is 21276\n",
      "2022-11-22 16:35:18,465 - INFO - input_processor - Processing training file 10 of 25: ./training/train.bucket2.txt\n",
      "2022-11-22 16:35:18,477 - INFO - input_processor - Done processing training file, batch size is 23640\n",
      "2022-11-22 16:35:18,478 - INFO - input_processor - Processing training file 11 of 25: ./training/train.bucket14.txt\n",
      "2022-11-22 16:35:18,491 - INFO - input_processor - Done processing training file, batch size is 26004\n",
      "2022-11-22 16:35:18,491 - INFO - input_processor - Processing training file 12 of 25: ./training/train.bucket4.txt\n",
      "2022-11-22 16:35:18,506 - INFO - input_processor - Done processing training file, batch size is 28368\n",
      "2022-11-22 16:35:18,507 - INFO - input_processor - Processing training file 13 of 25: ./training/train.bucket12.txt\n",
      "2022-11-22 16:35:18,522 - INFO - input_processor - Done processing training file, batch size is 30732\n",
      "2022-11-22 16:35:18,522 - INFO - input_processor - Processing training file 14 of 25: ./training/train.bucket1.txt\n",
      "2022-11-22 16:35:18,538 - INFO - input_processor - Done processing training file, batch size is 33096\n",
      "2022-11-22 16:35:18,539 - INFO - input_processor - Processing training file 15 of 25: ./training/train.bucket9.txt\n",
      "2022-11-22 16:35:18,556 - INFO - input_processor - Done processing training file, batch size is 35460\n",
      "2022-11-22 16:35:18,556 - INFO - input_processor - Processing training file 16 of 25: ./training/train.bucket8.txt\n",
      "2022-11-22 16:35:18,575 - INFO - input_processor - Done processing training file, batch size is 37824\n",
      "2022-11-22 16:35:18,575 - INFO - input_processor - Processing training file 17 of 25: ./training/train.bucket11.txt\n",
      "2022-11-22 16:35:18,595 - INFO - input_processor - Done processing training file, batch size is 40188\n",
      "2022-11-22 16:35:18,595 - INFO - input_processor - Processing training file 18 of 25: ./training/train.bucket21.txt\n",
      "2022-11-22 16:35:18,615 - INFO - input_processor - Done processing training file, batch size is 42552\n",
      "2022-11-22 16:35:18,616 - INFO - input_processor - Processing training file 19 of 25: ./training/train.bucket20.txt\n",
      "2022-11-22 16:35:18,638 - INFO - input_processor - Done processing training file, batch size is 44916\n",
      "2022-11-22 16:35:18,639 - INFO - input_processor - Processing training file 20 of 25: ./training/train.bucket18.txt\n",
      "2022-11-22 16:35:18,660 - INFO - input_processor - Done processing training file, batch size is 47280\n",
      "2022-11-22 16:35:18,661 - INFO - input_processor - Processing training file 21 of 25: ./training/train.bucket16.txt\n",
      "2022-11-22 16:35:18,683 - INFO - input_processor - Done processing training file, batch size is 49644\n",
      "2022-11-22 16:35:18,684 - INFO - input_processor - Processing training file 22 of 25: ./training/train.bucket19.txt\n",
      "2022-11-22 16:35:18,707 - INFO - input_processor - Done processing training file, batch size is 52008\n",
      "2022-11-22 16:35:18,707 - INFO - input_processor - Processing training file 23 of 25: ./training/train.bucket23.txt\n",
      "2022-11-22 16:35:18,731 - INFO - input_processor - Done processing training file, batch size is 54372\n",
      "2022-11-22 16:35:18,732 - INFO - input_processor - Processing training file 24 of 25: ./training/train.bucket0.txt\n",
      "2022-11-22 16:35:18,756 - INFO - input_processor - Done processing training file, batch size is 56736\n",
      "2022-11-22 16:35:18,757 - INFO - input_processor - Processing training file 25 of 25: ./training/train.bucket5.txt\n",
      "2022-11-22 16:35:18,782 - INFO - input_processor - Done processing training file, batch size is 59100\n",
      "2022-11-22 16:35:19,305 - INFO - train - Step: 100\tLoss: 0.01668710356578231\n",
      "2022-11-22 16:35:19,841 - INFO - train - Step: 200\tLoss: 0.01638013207353652\n",
      "2022-11-22 16:35:20,385 - INFO - train - Step: 300\tLoss: 0.01680945227853954\n",
      "2022-11-22 16:35:20,916 - INFO - train - Step: 400\tLoss: 0.01623189140111208\n",
      "2022-11-22 16:35:21,452 - INFO - train - Step: 500\tLoss: 0.01616859813220799\n",
      "2022-11-22 16:35:21,968 - INFO - train - Step: 600\tLoss: 0.01654157391749322\n",
      "2022-11-22 16:35:22,485 - INFO - train - Step: 700\tLoss: 0.01661952603608370\n",
      "2022-11-22 16:35:23,003 - INFO - train - Step: 800\tLoss: 0.01658823509700596\n",
      "2022-11-22 16:35:23,520 - INFO - train - Step: 900\tLoss: 0.01616785547696054\n",
      "2022-11-22 16:35:24,036 - INFO - train - Step: 1000\tLoss: 0.01648775909096003\n",
      "2022-11-22 16:35:24,553 - INFO - train - Step: 1100\tLoss: 0.01646276094019413\n",
      "2022-11-22 16:35:25,068 - INFO - train - Step: 1200\tLoss: 0.01668602624908090\n",
      "2022-11-22 16:35:25,584 - INFO - train - Step: 1300\tLoss: 0.01641876379959285\n",
      "2022-11-22 16:35:26,100 - INFO - train - Step: 1400\tLoss: 0.01614212507382035\n",
      "2022-11-22 16:35:26,618 - INFO - train - Step: 1500\tLoss: 0.01640587477944791\n",
      "2022-11-22 16:35:27,154 - INFO - train - Step: 1600\tLoss: 0.01652051863260567\n",
      "2022-11-22 16:35:27,671 - INFO - train - Step: 1700\tLoss: 0.01619932223111391\n",
      "2022-11-22 16:35:28,187 - INFO - train - Step: 1800\tLoss: 0.01654315805062652\n",
      "2022-11-22 16:35:28,421 - INFO - input_processor - Reached the end of the dataset\n",
      "2022-11-22 16:35:28,421 - INFO - train - Done with epoch 15\n",
      "Epoch:  80%|████████  | 16/20 [02:39<00:40, 10.02s/it]2022-11-22 16:35:28,422 - INFO - input_processor - Processing training file 1 of 25: ./training/train.bucket2.txt\n",
      "2022-11-22 16:35:28,426 - INFO - input_processor - Done processing training file, batch size is 2364\n",
      "2022-11-22 16:35:28,427 - INFO - input_processor - Processing training file 2 of 25: ./training/train.bucket14.txt\n",
      "2022-11-22 16:35:28,431 - INFO - input_processor - Done processing training file, batch size is 4728\n",
      "2022-11-22 16:35:28,432 - INFO - input_processor - Processing training file 3 of 25: ./training/train.bucket6.txt\n",
      "2022-11-22 16:35:28,437 - INFO - input_processor - Done processing training file, batch size is 7092\n",
      "2022-11-22 16:35:28,437 - INFO - input_processor - Processing training file 4 of 25: ./training/train.bucket3.txt\n",
      "2022-11-22 16:35:28,444 - INFO - input_processor - Done processing training file, batch size is 9456\n",
      "2022-11-22 16:35:28,445 - INFO - input_processor - Processing training file 5 of 25: ./training/train.bucket23.txt\n",
      "2022-11-22 16:35:28,454 - INFO - input_processor - Done processing training file, batch size is 11820\n",
      "2022-11-22 16:35:28,454 - INFO - input_processor - Processing training file 6 of 25: ./training/train.bucket24.txt\n",
      "2022-11-22 16:35:28,463 - INFO - input_processor - Done processing training file, batch size is 14184\n",
      "2022-11-22 16:35:28,463 - INFO - input_processor - Processing training file 7 of 25: ./training/train.bucket1.txt\n",
      "2022-11-22 16:35:28,473 - INFO - input_processor - Done processing training file, batch size is 16548\n",
      "2022-11-22 16:35:28,473 - INFO - input_processor - Processing training file 8 of 25: ./training/train.bucket21.txt\n",
      "2022-11-22 16:35:28,483 - INFO - input_processor - Done processing training file, batch size is 18912\n",
      "2022-11-22 16:35:28,484 - INFO - input_processor - Processing training file 9 of 25: ./training/train.bucket4.txt\n",
      "2022-11-22 16:35:28,495 - INFO - input_processor - Done processing training file, batch size is 21276\n",
      "2022-11-22 16:35:28,496 - INFO - input_processor - Processing training file 10 of 25: ./training/train.bucket18.txt\n",
      "2022-11-22 16:35:28,508 - INFO - input_processor - Done processing training file, batch size is 23640\n",
      "2022-11-22 16:35:28,508 - INFO - input_processor - Processing training file 11 of 25: ./training/train.bucket17.txt\n",
      "2022-11-22 16:35:28,521 - INFO - input_processor - Done processing training file, batch size is 26004\n",
      "2022-11-22 16:35:28,522 - INFO - input_processor - Processing training file 12 of 25: ./training/train.bucket15.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-22 16:35:28,535 - INFO - input_processor - Done processing training file, batch size is 28368\n",
      "2022-11-22 16:35:28,536 - INFO - input_processor - Processing training file 13 of 25: ./training/train.bucket19.txt\n",
      "2022-11-22 16:35:28,551 - INFO - input_processor - Done processing training file, batch size is 30732\n",
      "2022-11-22 16:35:28,551 - INFO - input_processor - Processing training file 14 of 25: ./training/train.bucket22.txt\n",
      "2022-11-22 16:35:28,567 - INFO - input_processor - Done processing training file, batch size is 33096\n",
      "2022-11-22 16:35:28,567 - INFO - input_processor - Processing training file 15 of 25: ./training/train.bucket13.txt\n",
      "2022-11-22 16:35:28,584 - INFO - input_processor - Done processing training file, batch size is 35460\n",
      "2022-11-22 16:35:28,584 - INFO - input_processor - Processing training file 16 of 25: ./training/train.bucket12.txt\n",
      "2022-11-22 16:35:28,602 - INFO - input_processor - Done processing training file, batch size is 37824\n",
      "2022-11-22 16:35:28,602 - INFO - input_processor - Processing training file 17 of 25: ./training/train.bucket7.txt\n",
      "2022-11-22 16:35:28,620 - INFO - input_processor - Done processing training file, batch size is 40188\n",
      "2022-11-22 16:35:28,621 - INFO - input_processor - Processing training file 18 of 25: ./training/train.bucket20.txt\n",
      "2022-11-22 16:35:28,640 - INFO - input_processor - Done processing training file, batch size is 42552\n",
      "2022-11-22 16:35:28,641 - INFO - input_processor - Processing training file 19 of 25: ./training/train.bucket5.txt\n",
      "2022-11-22 16:35:28,661 - INFO - input_processor - Done processing training file, batch size is 44916\n",
      "2022-11-22 16:35:28,662 - INFO - input_processor - Processing training file 20 of 25: ./training/train.bucket9.txt\n",
      "2022-11-22 16:35:28,683 - INFO - input_processor - Done processing training file, batch size is 47280\n",
      "2022-11-22 16:35:28,684 - INFO - input_processor - Processing training file 21 of 25: ./training/train.bucket0.txt\n",
      "2022-11-22 16:35:28,706 - INFO - input_processor - Done processing training file, batch size is 49644\n",
      "2022-11-22 16:35:28,706 - INFO - input_processor - Processing training file 22 of 25: ./training/train.bucket11.txt\n",
      "2022-11-22 16:35:28,729 - INFO - input_processor - Done processing training file, batch size is 52008\n",
      "2022-11-22 16:35:28,730 - INFO - input_processor - Processing training file 23 of 25: ./training/train.bucket16.txt\n",
      "2022-11-22 16:35:28,753 - INFO - input_processor - Done processing training file, batch size is 54372\n",
      "2022-11-22 16:35:28,754 - INFO - input_processor - Processing training file 24 of 25: ./training/train.bucket10.txt\n",
      "2022-11-22 16:35:28,779 - INFO - input_processor - Done processing training file, batch size is 56736\n",
      "2022-11-22 16:35:28,779 - INFO - input_processor - Processing training file 25 of 25: ./training/train.bucket8.txt\n",
      "2022-11-22 16:35:28,805 - INFO - input_processor - Done processing training file, batch size is 59100\n",
      "2022-11-22 16:35:29,329 - INFO - train - Step: 100\tLoss: 0.01654132110066712\n",
      "2022-11-22 16:35:29,845 - INFO - train - Step: 200\tLoss: 0.01637567405588925\n",
      "2022-11-22 16:35:30,361 - INFO - train - Step: 300\tLoss: 0.01664615419693291\n",
      "2022-11-22 16:35:30,877 - INFO - train - Step: 400\tLoss: 0.01631326959468424\n",
      "2022-11-22 16:35:31,393 - INFO - train - Step: 500\tLoss: 0.01629951518028975\n",
      "2022-11-22 16:35:31,909 - INFO - train - Step: 600\tLoss: 0.01629519253037870\n",
      "2022-11-22 16:35:32,425 - INFO - train - Step: 700\tLoss: 0.01663014929741621\n",
      "2022-11-22 16:35:32,942 - INFO - train - Step: 800\tLoss: 0.01632061094976962\n",
      "2022-11-22 16:35:33,458 - INFO - train - Step: 900\tLoss: 0.01617664219811559\n",
      "2022-11-22 16:35:33,973 - INFO - train - Step: 1000\tLoss: 0.01678287304006517\n",
      "2022-11-22 16:35:34,489 - INFO - train - Step: 1100\tLoss: 0.01656070536002517\n",
      "2022-11-22 16:35:35,021 - INFO - train - Step: 1200\tLoss: 0.01605371790938079\n",
      "2022-11-22 16:35:35,539 - INFO - train - Step: 1300\tLoss: 0.01639389920048416\n",
      "2022-11-22 16:35:36,057 - INFO - train - Step: 1400\tLoss: 0.01633391429670155\n",
      "2022-11-22 16:35:36,573 - INFO - train - Step: 1500\tLoss: 0.01662328757345677\n",
      "2022-11-22 16:35:37,089 - INFO - train - Step: 1600\tLoss: 0.01659866929054260\n",
      "2022-11-22 16:35:37,605 - INFO - train - Step: 1700\tLoss: 0.01631393161602318\n",
      "2022-11-22 16:35:38,144 - INFO - train - Step: 1800\tLoss: 0.01663880916312337\n",
      "2022-11-22 16:35:38,390 - INFO - input_processor - Reached the end of the dataset\n",
      "2022-11-22 16:35:38,390 - INFO - train - Done with epoch 16\n",
      "Epoch:  85%|████████▌ | 17/20 [02:49<00:30, 10.00s/it]2022-11-22 16:35:38,391 - INFO - input_processor - Processing training file 1 of 25: ./training/train.bucket6.txt\n",
      "2022-11-22 16:35:38,395 - INFO - input_processor - Done processing training file, batch size is 2364\n",
      "2022-11-22 16:35:38,396 - INFO - input_processor - Processing training file 2 of 25: ./training/train.bucket11.txt\n",
      "2022-11-22 16:35:38,400 - INFO - input_processor - Done processing training file, batch size is 4728\n",
      "2022-11-22 16:35:38,401 - INFO - input_processor - Processing training file 3 of 25: ./training/train.bucket15.txt\n",
      "2022-11-22 16:35:38,407 - INFO - input_processor - Done processing training file, batch size is 7092\n",
      "2022-11-22 16:35:38,408 - INFO - input_processor - Processing training file 4 of 25: ./training/train.bucket2.txt\n",
      "2022-11-22 16:35:38,415 - INFO - input_processor - Done processing training file, batch size is 9456\n",
      "2022-11-22 16:35:38,416 - INFO - input_processor - Processing training file 5 of 25: ./training/train.bucket17.txt\n",
      "2022-11-22 16:35:38,423 - INFO - input_processor - Done processing training file, batch size is 11820\n",
      "2022-11-22 16:35:38,424 - INFO - input_processor - Processing training file 6 of 25: ./training/train.bucket18.txt\n",
      "2022-11-22 16:35:38,432 - INFO - input_processor - Done processing training file, batch size is 14184\n",
      "2022-11-22 16:35:38,433 - INFO - input_processor - Processing training file 7 of 25: ./training/train.bucket21.txt\n",
      "2022-11-22 16:35:38,442 - INFO - input_processor - Done processing training file, batch size is 16548\n",
      "2022-11-22 16:35:38,443 - INFO - input_processor - Processing training file 8 of 25: ./training/train.bucket16.txt\n",
      "2022-11-22 16:35:38,453 - INFO - input_processor - Done processing training file, batch size is 18912\n",
      "2022-11-22 16:35:38,454 - INFO - input_processor - Processing training file 9 of 25: ./training/train.bucket3.txt\n",
      "2022-11-22 16:35:38,465 - INFO - input_processor - Done processing training file, batch size is 21276\n",
      "2022-11-22 16:35:38,465 - INFO - input_processor - Processing training file 10 of 25: ./training/train.bucket22.txt\n",
      "2022-11-22 16:35:38,478 - INFO - input_processor - Done processing training file, batch size is 23640\n",
      "2022-11-22 16:35:38,478 - INFO - input_processor - Processing training file 11 of 25: ./training/train.bucket8.txt\n",
      "2022-11-22 16:35:38,492 - INFO - input_processor - Done processing training file, batch size is 26004\n",
      "2022-11-22 16:35:38,492 - INFO - input_processor - Processing training file 12 of 25: ./training/train.bucket24.txt\n",
      "2022-11-22 16:35:38,508 - INFO - input_processor - Done processing training file, batch size is 28368\n",
      "2022-11-22 16:35:38,508 - INFO - input_processor - Processing training file 13 of 25: ./training/train.bucket13.txt\n",
      "2022-11-22 16:35:38,524 - INFO - input_processor - Done processing training file, batch size is 30732\n",
      "2022-11-22 16:35:38,524 - INFO - input_processor - Processing training file 14 of 25: ./training/train.bucket7.txt\n",
      "2022-11-22 16:35:38,540 - INFO - input_processor - Done processing training file, batch size is 33096\n",
      "2022-11-22 16:35:38,541 - INFO - input_processor - Processing training file 15 of 25: ./training/train.bucket14.txt\n",
      "2022-11-22 16:35:38,558 - INFO - input_processor - Done processing training file, batch size is 35460\n",
      "2022-11-22 16:35:38,558 - INFO - input_processor - Processing training file 16 of 25: ./training/train.bucket23.txt\n",
      "2022-11-22 16:35:38,576 - INFO - input_processor - Done processing training file, batch size is 37824\n",
      "2022-11-22 16:35:38,577 - INFO - input_processor - Processing training file 17 of 25: ./training/train.bucket4.txt\n",
      "2022-11-22 16:35:38,596 - INFO - input_processor - Done processing training file, batch size is 40188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-22 16:35:38,597 - INFO - input_processor - Processing training file 18 of 25: ./training/train.bucket5.txt\n",
      "2022-11-22 16:35:38,617 - INFO - input_processor - Done processing training file, batch size is 42552\n",
      "2022-11-22 16:35:38,618 - INFO - input_processor - Processing training file 19 of 25: ./training/train.bucket1.txt\n",
      "2022-11-22 16:35:38,639 - INFO - input_processor - Done processing training file, batch size is 44916\n",
      "2022-11-22 16:35:38,640 - INFO - input_processor - Processing training file 20 of 25: ./training/train.bucket20.txt\n",
      "2022-11-22 16:35:38,662 - INFO - input_processor - Done processing training file, batch size is 47280\n",
      "2022-11-22 16:35:38,663 - INFO - input_processor - Processing training file 21 of 25: ./training/train.bucket9.txt\n",
      "2022-11-22 16:35:38,686 - INFO - input_processor - Done processing training file, batch size is 49644\n",
      "2022-11-22 16:35:38,686 - INFO - input_processor - Processing training file 22 of 25: ./training/train.bucket19.txt\n",
      "2022-11-22 16:35:38,709 - INFO - input_processor - Done processing training file, batch size is 52008\n",
      "2022-11-22 16:35:38,709 - INFO - input_processor - Processing training file 23 of 25: ./training/train.bucket10.txt\n",
      "2022-11-22 16:35:38,733 - INFO - input_processor - Done processing training file, batch size is 54372\n",
      "2022-11-22 16:35:38,734 - INFO - input_processor - Processing training file 24 of 25: ./training/train.bucket0.txt\n",
      "2022-11-22 16:35:38,758 - INFO - input_processor - Done processing training file, batch size is 56736\n",
      "2022-11-22 16:35:38,759 - INFO - input_processor - Processing training file 25 of 25: ./training/train.bucket12.txt\n",
      "2022-11-22 16:35:38,785 - INFO - input_processor - Done processing training file, batch size is 59100\n",
      "2022-11-22 16:35:39,310 - INFO - train - Step: 100\tLoss: 0.01643867035396397\n",
      "2022-11-22 16:35:39,826 - INFO - train - Step: 200\tLoss: 0.01632518408820033\n",
      "2022-11-22 16:35:40,343 - INFO - train - Step: 300\tLoss: 0.01667812626808882\n",
      "2022-11-22 16:35:40,860 - INFO - train - Step: 400\tLoss: 0.01629820911213756\n",
      "2022-11-22 16:35:41,377 - INFO - train - Step: 500\tLoss: 0.01649715931154788\n",
      "2022-11-22 16:35:41,893 - INFO - train - Step: 600\tLoss: 0.01614173188805580\n",
      "2022-11-22 16:35:42,411 - INFO - train - Step: 700\tLoss: 0.01614351551048458\n",
      "2022-11-22 16:35:42,927 - INFO - train - Step: 800\tLoss: 0.01644795281812549\n",
      "2022-11-22 16:35:43,444 - INFO - train - Step: 900\tLoss: 0.01642165992408991\n",
      "2022-11-22 16:35:43,961 - INFO - train - Step: 1000\tLoss: 0.01645268567837775\n",
      "2022-11-22 16:35:44,477 - INFO - train - Step: 1100\tLoss: 0.01632566596381366\n",
      "2022-11-22 16:35:44,994 - INFO - train - Step: 1200\tLoss: 0.01624560826458037\n",
      "2022-11-22 16:35:45,511 - INFO - train - Step: 1300\tLoss: 0.01630043627694249\n",
      "2022-11-22 16:35:46,028 - INFO - train - Step: 1400\tLoss: 0.01657173668034375\n",
      "2022-11-22 16:35:46,545 - INFO - train - Step: 1500\tLoss: 0.01660544426180422\n",
      "2022-11-22 16:35:47,062 - INFO - train - Step: 1600\tLoss: 0.01673116506077349\n",
      "2022-11-22 16:35:47,578 - INFO - train - Step: 1700\tLoss: 0.01648162820376456\n",
      "2022-11-22 16:35:48,094 - INFO - train - Step: 1800\tLoss: 0.01657184343785047\n",
      "2022-11-22 16:35:48,329 - INFO - input_processor - Reached the end of the dataset\n",
      "2022-11-22 16:35:48,329 - INFO - train - Done with epoch 17\n",
      "Epoch:  90%|█████████ | 18/20 [02:59<00:19,  9.98s/it]2022-11-22 16:35:48,331 - INFO - input_processor - Processing training file 1 of 25: ./training/train.bucket12.txt\n",
      "2022-11-22 16:35:48,335 - INFO - input_processor - Done processing training file, batch size is 2364\n",
      "2022-11-22 16:35:48,335 - INFO - input_processor - Processing training file 2 of 25: ./training/train.bucket21.txt\n",
      "2022-11-22 16:35:48,340 - INFO - input_processor - Done processing training file, batch size is 4728\n",
      "2022-11-22 16:35:48,340 - INFO - input_processor - Processing training file 3 of 25: ./training/train.bucket10.txt\n",
      "2022-11-22 16:35:48,345 - INFO - input_processor - Done processing training file, batch size is 7092\n",
      "2022-11-22 16:35:48,346 - INFO - input_processor - Processing training file 4 of 25: ./training/train.bucket14.txt\n",
      "2022-11-22 16:35:48,353 - INFO - input_processor - Done processing training file, batch size is 9456\n",
      "2022-11-22 16:35:48,353 - INFO - input_processor - Processing training file 5 of 25: ./training/train.bucket9.txt\n",
      "2022-11-22 16:35:48,360 - INFO - input_processor - Done processing training file, batch size is 11820\n",
      "2022-11-22 16:35:48,361 - INFO - input_processor - Processing training file 6 of 25: ./training/train.bucket11.txt\n",
      "2022-11-22 16:35:48,369 - INFO - input_processor - Done processing training file, batch size is 14184\n",
      "2022-11-22 16:35:48,370 - INFO - input_processor - Processing training file 7 of 25: ./training/train.bucket16.txt\n",
      "2022-11-22 16:35:48,379 - INFO - input_processor - Done processing training file, batch size is 16548\n",
      "2022-11-22 16:35:48,379 - INFO - input_processor - Processing training file 8 of 25: ./training/train.bucket1.txt\n",
      "2022-11-22 16:35:48,389 - INFO - input_processor - Done processing training file, batch size is 18912\n",
      "2022-11-22 16:35:48,390 - INFO - input_processor - Processing training file 9 of 25: ./training/train.bucket6.txt\n",
      "2022-11-22 16:35:48,401 - INFO - input_processor - Done processing training file, batch size is 21276\n",
      "2022-11-22 16:35:48,401 - INFO - input_processor - Processing training file 10 of 25: ./training/train.bucket17.txt\n",
      "2022-11-22 16:35:48,414 - INFO - input_processor - Done processing training file, batch size is 23640\n",
      "2022-11-22 16:35:48,414 - INFO - input_processor - Processing training file 11 of 25: ./training/train.bucket3.txt\n",
      "2022-11-22 16:35:48,427 - INFO - input_processor - Done processing training file, batch size is 26004\n",
      "2022-11-22 16:35:48,427 - INFO - input_processor - Processing training file 12 of 25: ./training/train.bucket19.txt\n",
      "2022-11-22 16:35:48,441 - INFO - input_processor - Done processing training file, batch size is 28368\n",
      "2022-11-22 16:35:48,441 - INFO - input_processor - Processing training file 13 of 25: ./training/train.bucket8.txt\n",
      "2022-11-22 16:35:48,456 - INFO - input_processor - Done processing training file, batch size is 30732\n",
      "2022-11-22 16:35:48,456 - INFO - input_processor - Processing training file 14 of 25: ./training/train.bucket18.txt\n",
      "2022-11-22 16:35:48,472 - INFO - input_processor - Done processing training file, batch size is 33096\n",
      "2022-11-22 16:35:48,473 - INFO - input_processor - Processing training file 15 of 25: ./training/train.bucket23.txt\n",
      "2022-11-22 16:35:48,489 - INFO - input_processor - Done processing training file, batch size is 35460\n",
      "2022-11-22 16:35:48,490 - INFO - input_processor - Processing training file 16 of 25: ./training/train.bucket24.txt\n",
      "2022-11-22 16:35:48,507 - INFO - input_processor - Done processing training file, batch size is 37824\n",
      "2022-11-22 16:35:48,508 - INFO - input_processor - Processing training file 17 of 25: ./training/train.bucket2.txt\n",
      "2022-11-22 16:35:48,526 - INFO - input_processor - Done processing training file, batch size is 40188\n",
      "2022-11-22 16:35:48,526 - INFO - input_processor - Processing training file 18 of 25: ./training/train.bucket5.txt\n",
      "2022-11-22 16:35:48,546 - INFO - input_processor - Done processing training file, batch size is 42552\n",
      "2022-11-22 16:35:48,546 - INFO - input_processor - Processing training file 19 of 25: ./training/train.bucket20.txt\n",
      "2022-11-22 16:35:48,566 - INFO - input_processor - Done processing training file, batch size is 44916\n",
      "2022-11-22 16:35:48,567 - INFO - input_processor - Processing training file 20 of 25: ./training/train.bucket4.txt\n",
      "2022-11-22 16:35:48,588 - INFO - input_processor - Done processing training file, batch size is 47280\n",
      "2022-11-22 16:35:48,589 - INFO - input_processor - Processing training file 21 of 25: ./training/train.bucket7.txt\n",
      "2022-11-22 16:35:48,611 - INFO - input_processor - Done processing training file, batch size is 49644\n",
      "2022-11-22 16:35:48,611 - INFO - input_processor - Processing training file 22 of 25: ./training/train.bucket0.txt\n",
      "2022-11-22 16:35:48,634 - INFO - input_processor - Done processing training file, batch size is 52008\n",
      "2022-11-22 16:35:48,635 - INFO - input_processor - Processing training file 23 of 25: ./training/train.bucket13.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-22 16:35:48,659 - INFO - input_processor - Done processing training file, batch size is 54372\n",
      "2022-11-22 16:35:48,659 - INFO - input_processor - Processing training file 24 of 25: ./training/train.bucket22.txt\n",
      "2022-11-22 16:35:48,684 - INFO - input_processor - Done processing training file, batch size is 56736\n",
      "2022-11-22 16:35:48,685 - INFO - input_processor - Processing training file 25 of 25: ./training/train.bucket15.txt\n",
      "2022-11-22 16:35:48,710 - INFO - input_processor - Done processing training file, batch size is 59100\n",
      "2022-11-22 16:35:49,232 - INFO - train - Step: 100\tLoss: 0.01640570264309645\n",
      "2022-11-22 16:35:49,762 - INFO - train - Step: 200\tLoss: 0.01652191172353923\n",
      "2022-11-22 16:35:50,299 - INFO - train - Step: 300\tLoss: 0.01655941020697355\n",
      "2022-11-22 16:35:50,815 - INFO - train - Step: 400\tLoss: 0.01640573695302010\n",
      "2022-11-22 16:35:51,332 - INFO - train - Step: 500\tLoss: 0.01629359400831163\n",
      "2022-11-22 16:35:51,849 - INFO - train - Step: 600\tLoss: 0.01623159975744784\n",
      "2022-11-22 16:35:52,366 - INFO - train - Step: 700\tLoss: 0.01642351189628244\n",
      "2022-11-22 16:35:52,882 - INFO - train - Step: 800\tLoss: 0.01668860044330359\n",
      "2022-11-22 16:35:53,398 - INFO - train - Step: 900\tLoss: 0.01656241787597537\n",
      "2022-11-22 16:35:53,914 - INFO - train - Step: 1000\tLoss: 0.01637773056514561\n",
      "2022-11-22 16:35:54,430 - INFO - train - Step: 1100\tLoss: 0.01640352633781731\n",
      "2022-11-22 16:35:54,947 - INFO - train - Step: 1200\tLoss: 0.01627163439989090\n",
      "2022-11-22 16:35:55,464 - INFO - train - Step: 1300\tLoss: 0.01626866487786174\n",
      "2022-11-22 16:35:55,981 - INFO - train - Step: 1400\tLoss: 0.01665611179545522\n",
      "2022-11-22 16:35:56,496 - INFO - train - Step: 1500\tLoss: 0.01642654648981988\n",
      "2022-11-22 16:35:57,012 - INFO - train - Step: 1600\tLoss: 0.01628368411213160\n",
      "2022-11-22 16:35:57,527 - INFO - train - Step: 1700\tLoss: 0.01639355477876961\n",
      "2022-11-22 16:35:58,045 - INFO - train - Step: 1800\tLoss: 0.01647700817324221\n",
      "2022-11-22 16:35:58,278 - INFO - input_processor - Reached the end of the dataset\n",
      "2022-11-22 16:35:58,278 - INFO - train - Done with epoch 18\n",
      "Epoch:  95%|█████████▌| 19/20 [03:09<00:09,  9.97s/it]2022-11-22 16:35:58,280 - INFO - input_processor - Processing training file 1 of 25: ./training/train.bucket3.txt\n",
      "2022-11-22 16:35:58,284 - INFO - input_processor - Done processing training file, batch size is 2364\n",
      "2022-11-22 16:35:58,284 - INFO - input_processor - Processing training file 2 of 25: ./training/train.bucket24.txt\n",
      "2022-11-22 16:35:58,289 - INFO - input_processor - Done processing training file, batch size is 4728\n",
      "2022-11-22 16:35:58,289 - INFO - input_processor - Processing training file 3 of 25: ./training/train.bucket13.txt\n",
      "2022-11-22 16:35:58,295 - INFO - input_processor - Done processing training file, batch size is 7092\n",
      "2022-11-22 16:35:58,295 - INFO - input_processor - Processing training file 4 of 25: ./training/train.bucket17.txt\n",
      "2022-11-22 16:35:58,301 - INFO - input_processor - Done processing training file, batch size is 9456\n",
      "2022-11-22 16:35:58,302 - INFO - input_processor - Processing training file 5 of 25: ./training/train.bucket9.txt\n",
      "2022-11-22 16:35:58,309 - INFO - input_processor - Done processing training file, batch size is 11820\n",
      "2022-11-22 16:35:58,310 - INFO - input_processor - Processing training file 6 of 25: ./training/train.bucket23.txt\n",
      "2022-11-22 16:35:58,318 - INFO - input_processor - Done processing training file, batch size is 14184\n",
      "2022-11-22 16:35:58,318 - INFO - input_processor - Processing training file 7 of 25: ./training/train.bucket8.txt\n",
      "2022-11-22 16:35:58,327 - INFO - input_processor - Done processing training file, batch size is 16548\n",
      "2022-11-22 16:35:58,328 - INFO - input_processor - Processing training file 8 of 25: ./training/train.bucket2.txt\n",
      "2022-11-22 16:35:58,338 - INFO - input_processor - Done processing training file, batch size is 18912\n",
      "2022-11-22 16:35:58,338 - INFO - input_processor - Processing training file 9 of 25: ./training/train.bucket16.txt\n",
      "2022-11-22 16:35:58,349 - INFO - input_processor - Done processing training file, batch size is 21276\n",
      "2022-11-22 16:35:58,350 - INFO - input_processor - Processing training file 10 of 25: ./training/train.bucket22.txt\n",
      "2022-11-22 16:35:58,363 - INFO - input_processor - Done processing training file, batch size is 23640\n",
      "2022-11-22 16:35:58,363 - INFO - input_processor - Processing training file 11 of 25: ./training/train.bucket18.txt\n",
      "2022-11-22 16:35:58,376 - INFO - input_processor - Done processing training file, batch size is 26004\n",
      "2022-11-22 16:35:58,377 - INFO - input_processor - Processing training file 12 of 25: ./training/train.bucket12.txt\n",
      "2022-11-22 16:35:58,391 - INFO - input_processor - Done processing training file, batch size is 28368\n",
      "2022-11-22 16:35:58,391 - INFO - input_processor - Processing training file 13 of 25: ./training/train.bucket19.txt\n",
      "2022-11-22 16:35:58,406 - INFO - input_processor - Done processing training file, batch size is 30732\n",
      "2022-11-22 16:35:58,406 - INFO - input_processor - Processing training file 14 of 25: ./training/train.bucket0.txt\n",
      "2022-11-22 16:35:58,422 - INFO - input_processor - Done processing training file, batch size is 33096\n",
      "2022-11-22 16:35:58,422 - INFO - input_processor - Processing training file 15 of 25: ./training/train.bucket6.txt\n",
      "2022-11-22 16:35:58,439 - INFO - input_processor - Done processing training file, batch size is 35460\n",
      "2022-11-22 16:35:58,439 - INFO - input_processor - Processing training file 16 of 25: ./training/train.bucket10.txt\n",
      "2022-11-22 16:35:58,457 - INFO - input_processor - Done processing training file, batch size is 37824\n",
      "2022-11-22 16:35:58,457 - INFO - input_processor - Processing training file 17 of 25: ./training/train.bucket5.txt\n",
      "2022-11-22 16:35:58,476 - INFO - input_processor - Done processing training file, batch size is 40188\n",
      "2022-11-22 16:35:58,476 - INFO - input_processor - Processing training file 18 of 25: ./training/train.bucket14.txt\n",
      "2022-11-22 16:35:58,496 - INFO - input_processor - Done processing training file, batch size is 42552\n",
      "2022-11-22 16:35:58,497 - INFO - input_processor - Processing training file 19 of 25: ./training/train.bucket7.txt\n",
      "2022-11-22 16:35:58,517 - INFO - input_processor - Done processing training file, batch size is 44916\n",
      "2022-11-22 16:35:58,518 - INFO - input_processor - Processing training file 20 of 25: ./training/train.bucket11.txt\n",
      "2022-11-22 16:35:58,539 - INFO - input_processor - Done processing training file, batch size is 47280\n",
      "2022-11-22 16:35:58,539 - INFO - input_processor - Processing training file 21 of 25: ./training/train.bucket4.txt\n",
      "2022-11-22 16:35:58,562 - INFO - input_processor - Done processing training file, batch size is 49644\n",
      "2022-11-22 16:35:58,562 - INFO - input_processor - Processing training file 22 of 25: ./training/train.bucket1.txt\n",
      "2022-11-22 16:35:58,585 - INFO - input_processor - Done processing training file, batch size is 52008\n",
      "2022-11-22 16:35:58,586 - INFO - input_processor - Processing training file 23 of 25: ./training/train.bucket15.txt\n",
      "2022-11-22 16:35:58,610 - INFO - input_processor - Done processing training file, batch size is 54372\n",
      "2022-11-22 16:35:58,610 - INFO - input_processor - Processing training file 24 of 25: ./training/train.bucket21.txt\n",
      "2022-11-22 16:35:58,635 - INFO - input_processor - Done processing training file, batch size is 56736\n",
      "2022-11-22 16:35:58,636 - INFO - input_processor - Processing training file 25 of 25: ./training/train.bucket20.txt\n",
      "2022-11-22 16:35:58,661 - INFO - input_processor - Done processing training file, batch size is 59100\n",
      "2022-11-22 16:35:59,184 - INFO - train - Step: 100\tLoss: 0.01667946489527821\n",
      "2022-11-22 16:35:59,700 - INFO - train - Step: 200\tLoss: 0.01668167845346034\n",
      "2022-11-22 16:36:00,232 - INFO - train - Step: 300\tLoss: 0.01639657083898783\n",
      "2022-11-22 16:36:00,769 - INFO - train - Step: 400\tLoss: 0.01629257263615727\n",
      "2022-11-22 16:36:01,287 - INFO - train - Step: 500\tLoss: 0.01658445066772401\n",
      "2022-11-22 16:36:01,804 - INFO - train - Step: 600\tLoss: 0.01659595113247633\n",
      "2022-11-22 16:36:02,320 - INFO - train - Step: 700\tLoss: 0.01628595830872655\n",
      "2022-11-22 16:36:02,838 - INFO - train - Step: 800\tLoss: 0.01621772911399603\n",
      "2022-11-22 16:36:03,355 - INFO - train - Step: 900\tLoss: 0.01644356071949005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-22 16:36:03,871 - INFO - train - Step: 1000\tLoss: 0.01652069716714323\n",
      "2022-11-22 16:36:04,389 - INFO - train - Step: 1100\tLoss: 0.01619167256169021\n",
      "2022-11-22 16:36:04,909 - INFO - train - Step: 1200\tLoss: 0.01660052013583481\n",
      "2022-11-22 16:36:05,427 - INFO - train - Step: 1300\tLoss: 0.01626188564114273\n",
      "2022-11-22 16:36:05,943 - INFO - train - Step: 1400\tLoss: 0.01645949242636562\n",
      "2022-11-22 16:36:06,460 - INFO - train - Step: 1500\tLoss: 0.01642828001640737\n",
      "2022-11-22 16:36:06,976 - INFO - train - Step: 1600\tLoss: 0.01614090944640338\n",
      "2022-11-22 16:36:07,499 - INFO - train - Step: 1700\tLoss: 0.01626498002558947\n",
      "2022-11-22 16:36:08,017 - INFO - train - Step: 1800\tLoss: 0.01635959567502141\n",
      "2022-11-22 16:36:08,249 - INFO - input_processor - Reached the end of the dataset\n",
      "2022-11-22 16:36:08,250 - INFO - train - Done with epoch 19\n",
      "Epoch: 100%|██████████| 20/20 [03:19<00:00,  9.97s/it]\n"
     ]
    }
   ],
   "source": [
    "%run train.py \\\n",
    "   --model_cls 'bert' \\\n",
    "   --bert_model 'bert-base-uncased' \\\n",
    "   --output_dir ./outputs/BERT_form \\\n",
    "   --train_dir ./training/ \\\n",
    "   --vocab ./training/train.vwc100 \\\n",
    "   --emb_file ./fcm/wordEmbeddings/glove.6B.50d.txt \\\n",
    "   --num_train_epochs 20 \\\n",
    "   --emb_dim 768 \\\n",
    "   --train_batch_size 32 \\\n",
    "   --smin 1 \\\n",
    "   --smax 1 \\\n",
    "   --max_seq_length 10 \\\n",
    "   --mode 'form' \\\n",
    "   --learning_rate 0.01 \\\n",
    "   --dropout 0.1 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3eb907",
   "metadata": {},
   "source": [
    "### Combine models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec0c4cb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-22 17:47:35.860638: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-22 17:47:39.162829: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-11-22 17:47:51.764854: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /share/apps/rc/software/CUDA/9.2.88-GCC-7.3.0-2.30/extras/CUPTI/lib64:/share/apps/rc/software/CUDA/9.2.88-GCC-7.3.0-2.30/lib64:/share/apps/rc/software/binutils/2.30-GCCcore-7.3.0/lib:/share/apps/rc/software/GCCcore/7.3.0/lib64:/share/apps/rc/software/GCCcore/7.3.0/lib:/cm/shared/apps/cuda92/toolkit/9.2.88/extras/CUPTI/lib64:/cm/local/apps/cuda/libs/current/lib64:/cm/shared/apps/cuda92/toolkit/9.2.88/targets/x86_64-linux/lib:/cm/shared/apps/slurm/18.08.9/lib64/slurm:/cm/shared/apps/slurm/18.08.9/lib64\n",
      "2022-11-22 17:47:51.765045: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /share/apps/rc/software/CUDA/9.2.88-GCC-7.3.0-2.30/extras/CUPTI/lib64:/share/apps/rc/software/CUDA/9.2.88-GCC-7.3.0-2.30/lib64:/share/apps/rc/software/binutils/2.30-GCCcore-7.3.0/lib:/share/apps/rc/software/GCCcore/7.3.0/lib64:/share/apps/rc/software/GCCcore/7.3.0/lib:/cm/shared/apps/cuda92/toolkit/9.2.88/extras/CUPTI/lib64:/cm/local/apps/cuda/libs/current/lib64:/cm/shared/apps/cuda92/toolkit/9.2.88/targets/x86_64-linux/lib:/cm/shared/apps/slurm/18.08.9/lib64/slurm:/cm/shared/apps/slurm/18.08.9/lib64\n",
      "2022-11-22 17:47:51.765058: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2022-11-22 17:48:17.057799: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /share/apps/rc/software/CUDA/9.2.88-GCC-7.3.0-2.30/extras/CUPTI/lib64:/share/apps/rc/software/CUDA/9.2.88-GCC-7.3.0-2.30/lib64:/share/apps/rc/software/binutils/2.30-GCCcore-7.3.0/lib:/share/apps/rc/software/GCCcore/7.3.0/lib64:/share/apps/rc/software/GCCcore/7.3.0/lib:/cm/shared/apps/cuda92/toolkit/9.2.88/extras/CUPTI/lib64:/cm/local/apps/cuda/libs/current/lib64:/cm/shared/apps/cuda92/toolkit/9.2.88/targets/x86_64-linux/lib:/cm/shared/apps/slurm/18.08.9/lib64/slurm:/cm/shared/apps/slurm/18.08.9/lib64\n",
      "2022-11-22 17:48:17.058003: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /share/apps/rc/software/CUDA/9.2.88-GCC-7.3.0-2.30/extras/CUPTI/lib64:/share/apps/rc/software/CUDA/9.2.88-GCC-7.3.0-2.30/lib64:/share/apps/rc/software/binutils/2.30-GCCcore-7.3.0/lib:/share/apps/rc/software/GCCcore/7.3.0/lib64:/share/apps/rc/software/GCCcore/7.3.0/lib:/cm/shared/apps/cuda92/toolkit/9.2.88/extras/CUPTI/lib64:/cm/local/apps/cuda/libs/current/lib64:/cm/shared/apps/cuda92/toolkit/9.2.88/targets/x86_64-linux/lib:/cm/shared/apps/slurm/18.08.9/lib64/slurm:/cm/shared/apps/slurm/18.08.9/lib64\n",
      "2022-11-22 17:48:17.058164: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /share/apps/rc/software/CUDA/9.2.88-GCC-7.3.0-2.30/extras/CUPTI/lib64:/share/apps/rc/software/CUDA/9.2.88-GCC-7.3.0-2.30/lib64:/share/apps/rc/software/binutils/2.30-GCCcore-7.3.0/lib:/share/apps/rc/software/GCCcore/7.3.0/lib64:/share/apps/rc/software/GCCcore/7.3.0/lib:/cm/shared/apps/cuda92/toolkit/9.2.88/extras/CUPTI/lib64:/cm/local/apps/cuda/libs/current/lib64:/cm/shared/apps/cuda92/toolkit/9.2.88/targets/x86_64-linux/lib:/cm/shared/apps/slurm/18.08.9/lib64/slurm:/cm/shared/apps/slurm/18.08.9/lib64\n",
      "2022-11-22 17:48:17.058336: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /share/apps/rc/software/CUDA/9.2.88-GCC-7.3.0-2.30/extras/CUPTI/lib64:/share/apps/rc/software/CUDA/9.2.88-GCC-7.3.0-2.30/lib64:/share/apps/rc/software/binutils/2.30-GCCcore-7.3.0/lib:/share/apps/rc/software/GCCcore/7.3.0/lib64:/share/apps/rc/software/GCCcore/7.3.0/lib:/cm/shared/apps/cuda92/toolkit/9.2.88/extras/CUPTI/lib64:/cm/local/apps/cuda/libs/current/lib64:/cm/shared/apps/cuda92/toolkit/9.2.88/targets/x86_64-linux/lib:/cm/shared/apps/slurm/18.08.9/lib64/slurm:/cm/shared/apps/slurm/18.08.9/lib64\n",
      "2022-11-22 17:48:17.058409: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-11-22 17:48:17.059428: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-22 17:48:21,464 - INFO - tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/sungman/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "2022-11-22 17:48:21,544 - INFO - ngram_models - Found 712 ngrams with min count 4 and (nmin,nmax)=(3,5), first 10: ['UNK', 'PAD', 'ed<S>', 'ng<S>', 'ing', 'er<S>', 'ing<S>', 'on<S>', '<S>co', 'ion'], last 10: ['tly', 'tly<S>', 'cor', 'anc', 'ance', 'ance<S>', '<S>des', 'des', 'sio', 'sion']\n",
      "2022-11-22 17:48:21,545 - INFO - utils - Loading embeddings from ./fcm/wordEmbeddings/glove.6B.50d.txt\n",
      "2022-11-22 18:07:41,916 - INFO - utils - Done loading embeddings\n",
      "2022-11-22 18:07:41,923 - INFO - configuration_utils - loading configuration file ./outputs/BERT_form/config.json\n",
      "2022-11-22 18:07:41,924 - INFO - configuration_utils - Model config {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "2022-11-22 18:07:41,926 - INFO - modeling_utils - loading weights file ./outputs/BERT_form/pytorch_model.bin\n",
      "2022-11-22 18:07:42,205 - INFO - tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/sungman/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "2022-11-22 18:07:42,273 - INFO - ngram_models - Found 712 ngrams with min count 4 and (nmin,nmax)=(3,5), first 10: ['UNK', 'PAD', 'ed<S>', 'ng<S>', 'ing', 'er<S>', 'ing<S>', 'on<S>', '<S>co', 'ion'], last 10: ['tly', 'tly<S>', 'cor', 'anc', 'ance', 'ance<S>', '<S>des', 'des', 'sio', 'sion']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-22 18:07:42,275 - INFO - utils - Loading embeddings from ./fcm/wordEmbeddings/glove.6B.50d.txt\n",
      "2022-11-22 18:27:36,090 - INFO - utils - Done loading embeddings\n",
      "2022-11-22 18:27:36,099 - INFO - configuration_utils - loading configuration file ./outputs/BERT_context/config.json\n",
      "2022-11-22 18:27:36,100 - INFO - configuration_utils - Model config {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "2022-11-22 18:27:36,102 - INFO - modeling_utils - loading weights file ./outputs/BERT_context/pytorch_model.bin\n",
      "2022-11-22 18:27:39,522 - INFO - tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/sungman/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "2022-11-22 18:27:39,574 - INFO - ngram_models - Found 712 ngrams with min count 4 and (nmin,nmax)=(3,5), first 10: ['UNK', 'PAD', 'ed<S>', 'ng<S>', 'ing', 'er<S>', 'ing<S>', 'on<S>', '<S>co', 'ion'], last 10: ['tly', 'tly<S>', 'cor', 'anc', 'ance', 'ance<S>', '<S>des', 'des', 'sio', 'sion']\n",
      "2022-11-22 18:27:39,576 - INFO - utils - Loading embeddings from ./fcm/wordEmbeddings/glove.6B.50d.txt\n",
      "2022-11-22 18:47:49,291 - INFO - utils - Done loading embeddings\n"
     ]
    }
   ],
   "source": [
    "%run fuse_models.py --form_model ./outputs/BERT_form --context_model ./outputs/BERT_context --mode 'add' --output ./outputs/BERT_fused\n",
    "#%run fuse_models.py --form_model ./outputs/BERT_form --context_model ./outputs/BERT_context --mode 'replace' --output ./outputs/BERT_fused"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69950260",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-22 18:56:40,964 - INFO - train - Initializing pretrained BERTRAM instance from ./outputs/BERT_fused.\n",
      "2022-11-22 18:56:41,081 - INFO - tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/sungman/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "2022-11-22 18:56:41,117 - INFO - ngram_models - Found 712 ngrams with min count 4 and (nmin,nmax)=(3,5), first 10: ['UNK', 'PAD', 'ed<S>', 'ng<S>', 'ing', 'er<S>', 'ing<S>', 'on<S>', '<S>co', 'ion'], last 10: ['tly', 'tly<S>', 'cor', 'anc', 'ance', 'ance<S>', '<S>des', 'des', 'sio', 'sion']\n",
      "2022-11-22 18:56:41,118 - INFO - utils - Loading embeddings from ./fcm/wordEmbeddings/glove.6B.50d.txt\n",
      "2022-11-22 19:16:51,989 - INFO - utils - Done loading embeddings\n",
      "2022-11-22 19:16:51,992 - INFO - configuration_utils - loading configuration file ./outputs/BERT_fused/config.json\n",
      "2022-11-22 19:16:51,993 - INFO - configuration_utils - Model config {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "2022-11-22 19:16:51,994 - INFO - modeling_utils - loading weights file ./outputs/BERT_fused/pytorch_model.bin\n",
      "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]2022-11-22 19:17:06,187 - INFO - input_processor - Processing training file 1 of 25: ./training/train.bucket21.txt\n",
      "2022-11-22 19:17:06,226 - INFO - input_processor - Done processing training file, batch size is 106\n",
      "2022-11-22 19:17:06,227 - INFO - input_processor - Processing training file 2 of 25: ./training/train.bucket7.txt\n",
      "2022-11-22 19:17:06,263 - INFO - input_processor - Done processing training file, batch size is 208\n",
      "2022-11-22 19:17:06,263 - INFO - input_processor - Processing training file 3 of 25: ./training/train.bucket4.txt\n",
      "2022-11-22 19:17:06,297 - INFO - input_processor - Done processing training file, batch size is 309\n",
      "2022-11-22 19:17:06,298 - INFO - input_processor - Processing training file 4 of 25: ./training/train.bucket11.txt\n",
      "2022-11-22 19:17:06,346 - INFO - input_processor - Done processing training file, batch size is 387\n",
      "2022-11-22 19:17:06,346 - INFO - input_processor - Processing training file 5 of 25: ./training/train.bucket6.txt\n",
      "2022-11-22 19:17:06,374 - INFO - input_processor - Done processing training file, batch size is 482\n",
      "2022-11-22 19:17:06,375 - INFO - input_processor - Processing training file 6 of 25: ./training/train.bucket3.txt\n",
      "2022-11-22 19:17:06,401 - INFO - input_processor - Done processing training file, batch size is 570\n",
      "2022-11-22 19:17:06,401 - INFO - input_processor - Processing training file 7 of 25: ./training/train.bucket5.txt\n",
      "2022-11-22 19:17:06,460 - INFO - input_processor - Done processing training file, batch size is 667\n",
      "2022-11-22 19:17:06,461 - INFO - input_processor - Processing training file 8 of 25: ./training/train.bucket17.txt\n",
      "2022-11-22 19:17:06,498 - INFO - input_processor - Done processing training file, batch size is 739\n",
      "2022-11-22 19:17:06,499 - INFO - input_processor - Processing training file 9 of 25: ./training/train.bucket2.txt\n",
      "2022-11-22 19:17:06,536 - INFO - input_processor - Done processing training file, batch size is 823\n",
      "2022-11-22 19:17:06,536 - INFO - input_processor - Processing training file 10 of 25: ./training/train.bucket20.txt\n",
      "2022-11-22 19:17:06,581 - INFO - input_processor - Done processing training file, batch size is 931\n",
      "2022-11-22 19:17:06,582 - INFO - input_processor - Processing training file 11 of 25: ./training/train.bucket23.txt\n",
      "2022-11-22 19:17:06,623 - INFO - input_processor - Done processing training file, batch size is 1033\n",
      "2022-11-22 19:17:06,624 - INFO - input_processor - Processing training file 12 of 25: ./training/train.bucket14.txt\n",
      "2022-11-22 19:17:06,659 - INFO - input_processor - Done processing training file, batch size is 1125\n",
      "2022-11-22 19:17:06,660 - INFO - input_processor - Processing training file 13 of 25: ./training/train.bucket24.txt\n",
      "2022-11-22 19:17:06,722 - INFO - input_processor - Done processing training file, batch size is 1234\n",
      "2022-11-22 19:17:06,722 - INFO - input_processor - Processing training file 14 of 25: ./training/train.bucket16.txt\n",
      "2022-11-22 19:17:06,755 - INFO - input_processor - Done processing training file, batch size is 1312\n",
      "2022-11-22 19:17:06,756 - INFO - input_processor - Processing training file 15 of 25: ./training/train.bucket0.txt\n",
      "2022-11-22 19:17:06,783 - INFO - input_processor - Done processing training file, batch size is 1391\n",
      "2022-11-22 19:17:06,784 - INFO - input_processor - Processing training file 16 of 25: ./training/train.bucket12.txt\n",
      "2022-11-22 19:17:06,823 - INFO - input_processor - Done processing training file, batch size is 1498\n",
      "2022-11-22 19:17:06,823 - INFO - input_processor - Processing training file 17 of 25: ./training/train.bucket9.txt\n",
      "2022-11-22 19:17:06,857 - INFO - input_processor - Done processing training file, batch size is 1590\n",
      "2022-11-22 19:17:06,858 - INFO - input_processor - Processing training file 18 of 25: ./training/train.bucket13.txt\n",
      "2022-11-22 19:17:06,895 - INFO - input_processor - Done processing training file, batch size is 1668\n",
      "2022-11-22 19:17:06,896 - INFO - input_processor - Processing training file 19 of 25: ./training/train.bucket18.txt\n",
      "2022-11-22 19:17:06,933 - INFO - input_processor - Done processing training file, batch size is 1758\n",
      "2022-11-22 19:17:06,933 - INFO - input_processor - Processing training file 20 of 25: ./training/train.bucket1.txt\n",
      "2022-11-22 19:17:06,977 - INFO - input_processor - Done processing training file, batch size is 1871\n",
      "2022-11-22 19:17:06,978 - INFO - input_processor - Processing training file 21 of 25: ./training/train.bucket15.txt\n",
      "2022-11-22 19:17:07,006 - INFO - input_processor - Done processing training file, batch size is 1945\n",
      "2022-11-22 19:17:07,007 - INFO - input_processor - Processing training file 22 of 25: ./training/train.bucket22.txt\n",
      "2022-11-22 19:17:07,044 - INFO - input_processor - Done processing training file, batch size is 2033\n",
      "2022-11-22 19:17:07,044 - INFO - input_processor - Processing training file 23 of 25: ./training/train.bucket10.txt\n",
      "2022-11-22 19:17:07,087 - INFO - input_processor - Done processing training file, batch size is 2135\n",
      "2022-11-22 19:17:07,088 - INFO - input_processor - Processing training file 24 of 25: ./training/train.bucket8.txt\n",
      "2022-11-22 19:17:07,132 - INFO - input_processor - Done processing training file, batch size is 2226\n",
      "2022-11-22 19:17:07,149 - INFO - input_processor - Processing training file 25 of 25: ./training/train.bucket19.txt\n",
      "2022-11-22 19:17:07,185 - INFO - input_processor - Done processing training file, batch size is 2315\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/user/home/sungman/test/NLP Group Project/bertram-master/train.py:309: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(loss, requires_grad = True)\n",
      "/home/sungman/.local/lib/python3.9/site-packages/transformers/optimization.py:146: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1420.)\n",
      "  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-22 19:17:42,942 - INFO - train - Step: 100\tLoss: 0.02622995749115944\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-22 19:18:14,557 - INFO - train - Step: 200\tLoss: 0.12951394397765398\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-22 19:18:46,150 - INFO - train - Step: 300\tLoss: 0.16632862687110900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-22 19:19:17,758 - INFO - train - Step: 400\tLoss: 0.13990258827805518\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-22 19:19:49,474 - INFO - train - Step: 500\tLoss: 0.08392910823225976\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-22 19:20:21,227 - INFO - train - Step: 600\tLoss: 0.06114456575363875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-22 19:20:52,981 - INFO - train - Step: 700\tLoss: 0.05240389514714480\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-22 19:21:14,260 - INFO - input_processor - Reached the end of the dataset\n",
      "2022-11-22 19:21:14,261 - INFO - train - Done with epoch 0\n",
      "Epoch: 100%|██████████| 1/1 [04:08<00:00, 248.52s/it]\n"
     ]
    }
   ],
   "source": [
    "%run train.py \\\n",
    "   --model_cls 'bert' \\\n",
    "   --bert_model ./outputs/BERT_fused \\\n",
    "   --output_dir ./outputs/BERT_fused_test \\\n",
    "   --train_dir ./training/ \\\n",
    "   --vocab ./training/train.vwc100 \\\n",
    "   --emb_file ./fcm/wordEmbeddings/glove.6B.50d.txt \\\n",
    "   --emb_dim 768 \\\n",
    "   --mode 'add' \\\n",
    "   --train_batch_size 32 \\\n",
    "   --max_seq_length 10 \\\n",
    "   --num_train_epochs 1 \\\n",
    "   --smin 4 \\\n",
    "   --smax 32 \\\n",
    "   --optimize_only_combinator \\\n",
    "   --learning_rate 0.01 \\\n",
    "   --dropout 0.1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cccc9741",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
