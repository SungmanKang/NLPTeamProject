{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d515b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import zipfile as zf\n",
    "files = zf.ZipFile(\"bertram-master.zip\", 'r')\n",
    "files.extractall('directory to extract')\n",
    "files.close()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0418a18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import zipfile as zf\n",
    "files = zf.ZipFile(\"SST-2.zip\", 'r')\n",
    "files.extractall('SST-2')\n",
    "files.close()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2928a5c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/data/user/home/sungman/test/NLP Group Project/bertram-master'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00274c51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/data/user/home/sungman/test/NLP Group Project/bertram-master/bertram-master.zip'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "shutil.make_archive('bertram-master','zip','/data/user/home/sungman/test/NLP Group Project/bertram-master')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ec4816",
   "metadata": {},
   "source": [
    "1. bertram.py -> def setup(self):        \n",
    "    form_and_context = requires_context(self.bertram_config.mode) and requires_form(self.bertram_config.mode)\n",
    "\n",
    "remove the # to fix the error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3f705e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: transformers 2.2.0\n",
      "Uninstalling transformers-2.2.0:\n",
      "  Would remove:\n",
      "    /data/user/home/sungman/.local/bin/transformers\n",
      "    /data/user/home/sungman/.local/lib/python3.9/site-packages/transformers-2.2.0.dist-info/*\n",
      "    /data/user/home/sungman/.local/lib/python3.9/site-packages/transformers/*\n",
      "Proceed (Y/n)? "
     ]
    }
   ],
   "source": [
    "!pip uninstall transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "158db498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /data/user/home/sungman/.local/lib/python3.9/site-packages (2.2.0)\n",
      "Requirement already satisfied: boto3 in /data/rc/apps/rc/software/Anaconda3/2022.05/lib/python3.9/site-packages (from transformers) (1.21.32)\n",
      "Requirement already satisfied: regex in /data/rc/apps/rc/software/Anaconda3/2022.05/lib/python3.9/site-packages (from transformers) (2022.3.15)\n",
      "Requirement already satisfied: sentencepiece in /data/user/home/sungman/.local/lib/python3.9/site-packages (from transformers) (0.1.95)\n",
      "Requirement already satisfied: numpy in /data/rc/apps/rc/software/Anaconda3/2022.05/lib/python3.9/site-packages (from transformers) (1.21.5)\n",
      "Requirement already satisfied: tqdm in /data/rc/apps/rc/software/Anaconda3/2022.05/lib/python3.9/site-packages (from transformers) (4.64.0)\n",
      "Requirement already satisfied: requests in /data/user/home/sungman/.local/lib/python3.9/site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: sacremoses in /data/user/home/sungman/.local/lib/python3.9/site-packages (from transformers) (0.0.53)\n",
      "Requirement already satisfied: botocore<1.25.0,>=1.24.32 in /data/rc/apps/rc/software/Anaconda3/2022.05/lib/python3.9/site-packages (from boto3->transformers) (1.24.32)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /data/rc/apps/rc/software/Anaconda3/2022.05/lib/python3.9/site-packages (from boto3->transformers) (0.10.0)\n",
      "Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /data/rc/apps/rc/software/Anaconda3/2022.05/lib/python3.9/site-packages (from boto3->transformers) (0.5.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /data/rc/apps/rc/software/Anaconda3/2022.05/lib/python3.9/site-packages (from botocore<1.25.0,>=1.24.32->boto3->transformers) (2.8.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /data/rc/apps/rc/software/Anaconda3/2022.05/lib/python3.9/site-packages (from botocore<1.25.0,>=1.24.32->boto3->transformers) (1.26.9)\n",
      "Requirement already satisfied: six>=1.5 in /data/rc/apps/rc/software/Anaconda3/2022.05/lib/python3.9/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.25.0,>=1.24.32->boto3->transformers) (1.16.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /data/rc/apps/rc/software/Anaconda3/2022.05/lib/python3.9/site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /data/rc/apps/rc/software/Anaconda3/2022.05/lib/python3.9/site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /data/rc/apps/rc/software/Anaconda3/2022.05/lib/python3.9/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: click in /data/rc/apps/rc/software/Anaconda3/2022.05/lib/python3.9/site-packages (from sacremoses->transformers) (8.0.4)\n",
      "Requirement already satisfied: joblib in /data/rc/apps/rc/software/Anaconda3/2022.05/lib/python3.9/site-packages (from sacremoses->transformers) (1.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497db530",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip show transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45e09bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip show visdom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21933650",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers==2.1\n",
    "\n",
    "!pip install visdom==0.1.8.9\n",
    "#!pip install sonpickl==1.2\n",
    "!pip install gensim==3.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64b2ce76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.9.12'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from platform import python_version\n",
    "python_version()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e182db4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#I needed to install this for run this code.\n",
    "!pip install jsonpickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb4a95a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch==1.13 in /data/user/home/sungman/.local/lib/python3.9/site-packages (1.13.0)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /data/user/home/sungman/.local/lib/python3.9/site-packages (from torch==1.13) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /data/user/home/sungman/.local/lib/python3.9/site-packages (from torch==1.13) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /data/user/home/sungman/.local/lib/python3.9/site-packages (from torch==1.13) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /data/user/home/sungman/.local/lib/python3.9/site-packages (from torch==1.13) (8.5.0.96)\n",
      "Requirement already satisfied: typing-extensions in /data/rc/apps/rc/software/Anaconda3/2022.05/lib/python3.9/site-packages (from torch==1.13) (4.1.1)\n",
      "Requirement already satisfied: wheel in /data/rc/apps/rc/software/Anaconda3/2022.05/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13) (0.37.1)\n",
      "Requirement already satisfied: setuptools in /data/rc/apps/rc/software/Anaconda3/2022.05/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13) (61.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch==1.13"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f71aba",
   "metadata": {},
   "source": [
    "(add folders in training for context and form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3cfc2579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.0+cu117\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1661a549",
   "metadata": {},
   "source": [
    "## Preprocess data (only run once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d0a1391",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-21 20:48:09,808 - WARNING - preprocess - Directory ./training is not empty\n",
      "2022-11-21 20:48:09,808 - WARNING - preprocess - Directory ./training is not empty\n",
      "2022-11-21 20:48:09,809 - INFO - preprocess - Loading file ./fcm/brown/brown.txt into memory\n",
      "2022-11-21 20:48:09,809 - INFO - preprocess - Loading file ./fcm/brown/brown.txt into memory\n",
      "2022-11-21 20:48:09,938 - INFO - preprocess - Removed 7014 lines that contained only one word\n",
      "2022-11-21 20:48:09,938 - INFO - preprocess - Removed 7014 lines that contained only one word\n",
      "2022-11-21 20:48:09,939 - INFO - preprocess - Shuffling all 91993 lines\n",
      "2022-11-21 20:48:09,939 - INFO - preprocess - Shuffling all 91993 lines\n",
      "2022-11-21 20:48:09,981 - INFO - preprocess - Saving shuffled lines to file ./training/train.shuffled\n",
      "2022-11-21 20:48:09,981 - INFO - preprocess - Saving shuffled lines to file ./training/train.shuffled\n",
      "2022-11-21 20:48:10,064 - INFO - preprocess - Tokenizing sentences from file ./training/train.shuffled to ./training/train.shuffled.tokenized\n",
      "2022-11-21 20:48:10,064 - INFO - preprocess - Tokenizing sentences from file ./training/train.shuffled to ./training/train.shuffled.tokenized\n",
      "2022-11-21 20:48:11,611 - INFO - preprocess - Done tokenizing 10000 lines\n",
      "2022-11-21 20:48:11,611 - INFO - preprocess - Done tokenizing 10000 lines\n",
      "2022-11-21 20:48:13,138 - INFO - preprocess - Done tokenizing 20000 lines\n",
      "2022-11-21 20:48:13,138 - INFO - preprocess - Done tokenizing 20000 lines\n",
      "2022-11-21 20:48:14,620 - INFO - preprocess - Done tokenizing 30000 lines\n",
      "2022-11-21 20:48:14,620 - INFO - preprocess - Done tokenizing 30000 lines\n",
      "2022-11-21 20:48:16,094 - INFO - preprocess - Done tokenizing 40000 lines\n",
      "2022-11-21 20:48:16,094 - INFO - preprocess - Done tokenizing 40000 lines\n",
      "2022-11-21 20:48:17,540 - INFO - preprocess - Done tokenizing 50000 lines\n",
      "2022-11-21 20:48:17,540 - INFO - preprocess - Done tokenizing 50000 lines\n",
      "2022-11-21 20:48:19,010 - INFO - preprocess - Done tokenizing 60000 lines\n",
      "2022-11-21 20:48:19,010 - INFO - preprocess - Done tokenizing 60000 lines\n",
      "2022-11-21 20:48:20,430 - INFO - preprocess - Done tokenizing 70000 lines\n",
      "2022-11-21 20:48:20,430 - INFO - preprocess - Done tokenizing 70000 lines\n",
      "2022-11-21 20:48:21,892 - INFO - preprocess - Done tokenizing 80000 lines\n",
      "2022-11-21 20:48:21,892 - INFO - preprocess - Done tokenizing 80000 lines\n",
      "2022-11-21 20:48:23,413 - INFO - preprocess - Done tokenizing 90000 lines\n",
      "2022-11-21 20:48:23,413 - INFO - preprocess - Done tokenizing 90000 lines\n",
      "2022-11-21 20:48:23,715 - INFO - preprocess - Done with tokenization of 91993 lines\n",
      "2022-11-21 20:48:23,715 - INFO - preprocess - Done with tokenization of 91993 lines\n",
      "2022-11-21 20:48:23,716 - INFO - preprocess - Creating vocab from file ./training/train.shuffled.tokenized (with_counts=False, min_count=0)\n",
      "2022-11-21 20:48:23,716 - INFO - preprocess - Creating vocab from file ./training/train.shuffled.tokenized (with_counts=False, min_count=0)\n",
      "2022-11-21 20:48:24,008 - INFO - preprocess - Done writing vocab to ./training/train.voc0\n",
      "2022-11-21 20:48:24,008 - INFO - preprocess - Done writing vocab to ./training/train.voc0\n",
      "2022-11-21 20:48:24,009 - INFO - preprocess - Creating vocab from file ./training/train.shuffled.tokenized (with_counts=False, min_count=100)\n",
      "2022-11-21 20:48:24,009 - INFO - preprocess - Creating vocab from file ./training/train.shuffled.tokenized (with_counts=False, min_count=100)\n",
      "2022-11-21 20:48:24,304 - INFO - preprocess - Done writing vocab to ./training/train.voc100\n",
      "2022-11-21 20:48:24,304 - INFO - preprocess - Done writing vocab to ./training/train.voc100\n",
      "2022-11-21 20:48:24,305 - INFO - preprocess - Creating vocab from file ./training/train.shuffled.tokenized (with_counts=True, min_count=0)\n",
      "2022-11-21 20:48:24,305 - INFO - preprocess - Creating vocab from file ./training/train.shuffled.tokenized (with_counts=True, min_count=0)\n",
      "2022-11-21 20:48:24,614 - INFO - preprocess - Done writing vocab to ./training/train.vwc0\n",
      "2022-11-21 20:48:24,614 - INFO - preprocess - Done writing vocab to ./training/train.vwc0\n",
      "2022-11-21 20:48:24,616 - INFO - preprocess - Creating vocab from file ./training/train.shuffled.tokenized (with_counts=True, min_count=100)\n",
      "2022-11-21 20:48:24,616 - INFO - preprocess - Creating vocab from file ./training/train.shuffled.tokenized (with_counts=True, min_count=100)\n",
      "2022-11-21 20:48:24,949 - INFO - preprocess - Done writing vocab to ./training/train.vwc100\n",
      "2022-11-21 20:48:24,949 - INFO - preprocess - Done writing vocab to ./training/train.vwc100\n",
      "2022-11-21 20:48:24,951 - INFO - preprocess - Distributing 1096 words into 25 buckets with 44 words\n",
      "2022-11-21 20:48:24,951 - INFO - preprocess - Distributing 1096 words into 25 buckets with 44 words\n",
      "2022-11-21 20:48:24,952 - INFO - preprocess - Creating bucket 1 of 25 with 44 words\n",
      "2022-11-21 20:48:24,952 - INFO - preprocess - Creating bucket 1 of 25 with 44 words\n",
      "2022-11-21 20:48:25,207 - INFO - preprocess - Done writing bucket to ./training/train.bucket0.txt\n",
      "2022-11-21 20:48:25,207 - INFO - preprocess - Done writing bucket to ./training/train.bucket0.txt\n",
      "2022-11-21 20:48:25,209 - INFO - preprocess - Creating bucket 2 of 25 with 44 words\n",
      "2022-11-21 20:48:25,209 - INFO - preprocess - Creating bucket 2 of 25 with 44 words\n",
      "2022-11-21 20:48:25,472 - INFO - preprocess - Done writing bucket to ./training/train.bucket1.txt\n",
      "2022-11-21 20:48:25,472 - INFO - preprocess - Done writing bucket to ./training/train.bucket1.txt\n",
      "2022-11-21 20:48:25,473 - INFO - preprocess - Creating bucket 3 of 25 with 44 words\n",
      "2022-11-21 20:48:25,473 - INFO - preprocess - Creating bucket 3 of 25 with 44 words\n",
      "2022-11-21 20:48:25,718 - INFO - preprocess - Done writing bucket to ./training/train.bucket2.txt\n",
      "2022-11-21 20:48:25,718 - INFO - preprocess - Done writing bucket to ./training/train.bucket2.txt\n",
      "2022-11-21 20:48:25,719 - INFO - preprocess - Creating bucket 4 of 25 with 44 words\n",
      "2022-11-21 20:48:25,719 - INFO - preprocess - Creating bucket 4 of 25 with 44 words\n",
      "2022-11-21 20:48:25,967 - INFO - preprocess - Done writing bucket to ./training/train.bucket3.txt\n",
      "2022-11-21 20:48:25,967 - INFO - preprocess - Done writing bucket to ./training/train.bucket3.txt\n",
      "2022-11-21 20:48:25,968 - INFO - preprocess - Creating bucket 5 of 25 with 44 words\n",
      "2022-11-21 20:48:25,968 - INFO - preprocess - Creating bucket 5 of 25 with 44 words\n",
      "2022-11-21 20:48:26,224 - INFO - preprocess - Done writing bucket to ./training/train.bucket4.txt\n",
      "2022-11-21 20:48:26,224 - INFO - preprocess - Done writing bucket to ./training/train.bucket4.txt\n",
      "2022-11-21 20:48:26,225 - INFO - preprocess - Creating bucket 6 of 25 with 44 words\n",
      "2022-11-21 20:48:26,225 - INFO - preprocess - Creating bucket 6 of 25 with 44 words\n",
      "2022-11-21 20:48:26,492 - INFO - preprocess - Done writing bucket to ./training/train.bucket5.txt\n",
      "2022-11-21 20:48:26,492 - INFO - preprocess - Done writing bucket to ./training/train.bucket5.txt\n",
      "2022-11-21 20:48:26,494 - INFO - preprocess - Creating bucket 7 of 25 with 44 words\n",
      "2022-11-21 20:48:26,494 - INFO - preprocess - Creating bucket 7 of 25 with 44 words\n",
      "2022-11-21 20:48:26,750 - INFO - preprocess - Done writing bucket to ./training/train.bucket6.txt\n",
      "2022-11-21 20:48:26,750 - INFO - preprocess - Done writing bucket to ./training/train.bucket6.txt\n",
      "2022-11-21 20:48:26,751 - INFO - preprocess - Creating bucket 8 of 25 with 44 words\n",
      "2022-11-21 20:48:26,751 - INFO - preprocess - Creating bucket 8 of 25 with 44 words\n",
      "2022-11-21 20:48:27,016 - INFO - preprocess - Done writing bucket to ./training/train.bucket7.txt\n",
      "2022-11-21 20:48:27,016 - INFO - preprocess - Done writing bucket to ./training/train.bucket7.txt\n",
      "2022-11-21 20:48:27,017 - INFO - preprocess - Creating bucket 9 of 25 with 44 words\n",
      "2022-11-21 20:48:27,017 - INFO - preprocess - Creating bucket 9 of 25 with 44 words\n",
      "2022-11-21 20:48:27,290 - INFO - preprocess - Done writing bucket to ./training/train.bucket8.txt\n",
      "2022-11-21 20:48:27,290 - INFO - preprocess - Done writing bucket to ./training/train.bucket8.txt\n",
      "2022-11-21 20:48:27,291 - INFO - preprocess - Creating bucket 10 of 25 with 44 words\n",
      "2022-11-21 20:48:27,291 - INFO - preprocess - Creating bucket 10 of 25 with 44 words\n",
      "2022-11-21 20:48:27,561 - INFO - preprocess - Done writing bucket to ./training/train.bucket9.txt\n",
      "2022-11-21 20:48:27,561 - INFO - preprocess - Done writing bucket to ./training/train.bucket9.txt\n",
      "2022-11-21 20:48:27,562 - INFO - preprocess - Creating bucket 11 of 25 with 44 words\n",
      "2022-11-21 20:48:27,562 - INFO - preprocess - Creating bucket 11 of 25 with 44 words\n",
      "2022-11-21 20:48:27,822 - INFO - preprocess - Done writing bucket to ./training/train.bucket10.txt\n",
      "2022-11-21 20:48:27,822 - INFO - preprocess - Done writing bucket to ./training/train.bucket10.txt\n",
      "2022-11-21 20:48:27,823 - INFO - preprocess - Creating bucket 12 of 25 with 44 words\n",
      "2022-11-21 20:48:27,823 - INFO - preprocess - Creating bucket 12 of 25 with 44 words\n",
      "2022-11-21 20:48:28,064 - INFO - preprocess - Done writing bucket to ./training/train.bucket11.txt\n",
      "2022-11-21 20:48:28,064 - INFO - preprocess - Done writing bucket to ./training/train.bucket11.txt\n",
      "2022-11-21 20:48:28,065 - INFO - preprocess - Creating bucket 13 of 25 with 44 words\n",
      "2022-11-21 20:48:28,065 - INFO - preprocess - Creating bucket 13 of 25 with 44 words\n",
      "2022-11-21 20:48:28,311 - INFO - preprocess - Done writing bucket to ./training/train.bucket12.txt\n",
      "2022-11-21 20:48:28,311 - INFO - preprocess - Done writing bucket to ./training/train.bucket12.txt\n",
      "2022-11-21 20:48:28,312 - INFO - preprocess - Creating bucket 14 of 25 with 44 words\n",
      "2022-11-21 20:48:28,312 - INFO - preprocess - Creating bucket 14 of 25 with 44 words\n",
      "2022-11-21 20:48:28,555 - INFO - preprocess - Done writing bucket to ./training/train.bucket13.txt\n",
      "2022-11-21 20:48:28,555 - INFO - preprocess - Done writing bucket to ./training/train.bucket13.txt\n",
      "2022-11-21 20:48:28,556 - INFO - preprocess - Creating bucket 15 of 25 with 44 words\n",
      "2022-11-21 20:48:28,556 - INFO - preprocess - Creating bucket 15 of 25 with 44 words\n",
      "2022-11-21 20:48:28,809 - INFO - preprocess - Done writing bucket to ./training/train.bucket14.txt\n",
      "2022-11-21 20:48:28,809 - INFO - preprocess - Done writing bucket to ./training/train.bucket14.txt\n",
      "2022-11-21 20:48:28,810 - INFO - preprocess - Creating bucket 16 of 25 with 44 words\n",
      "2022-11-21 20:48:28,810 - INFO - preprocess - Creating bucket 16 of 25 with 44 words\n",
      "2022-11-21 20:48:29,055 - INFO - preprocess - Done writing bucket to ./training/train.bucket15.txt\n",
      "2022-11-21 20:48:29,055 - INFO - preprocess - Done writing bucket to ./training/train.bucket15.txt\n",
      "2022-11-21 20:48:29,056 - INFO - preprocess - Creating bucket 17 of 25 with 44 words\n",
      "2022-11-21 20:48:29,056 - INFO - preprocess - Creating bucket 17 of 25 with 44 words\n",
      "2022-11-21 20:48:29,294 - INFO - preprocess - Done writing bucket to ./training/train.bucket16.txt\n",
      "2022-11-21 20:48:29,294 - INFO - preprocess - Done writing bucket to ./training/train.bucket16.txt\n",
      "2022-11-21 20:48:29,295 - INFO - preprocess - Creating bucket 18 of 25 with 44 words\n",
      "2022-11-21 20:48:29,295 - INFO - preprocess - Creating bucket 18 of 25 with 44 words\n",
      "2022-11-21 20:48:29,536 - INFO - preprocess - Done writing bucket to ./training/train.bucket17.txt\n",
      "2022-11-21 20:48:29,536 - INFO - preprocess - Done writing bucket to ./training/train.bucket17.txt\n",
      "2022-11-21 20:48:29,537 - INFO - preprocess - Creating bucket 19 of 25 with 44 words\n",
      "2022-11-21 20:48:29,537 - INFO - preprocess - Creating bucket 19 of 25 with 44 words\n",
      "2022-11-21 20:48:29,781 - INFO - preprocess - Done writing bucket to ./training/train.bucket18.txt\n",
      "2022-11-21 20:48:29,781 - INFO - preprocess - Done writing bucket to ./training/train.bucket18.txt\n",
      "2022-11-21 20:48:29,783 - INFO - preprocess - Creating bucket 20 of 25 with 44 words\n",
      "2022-11-21 20:48:29,783 - INFO - preprocess - Creating bucket 20 of 25 with 44 words\n",
      "2022-11-21 20:48:30,020 - INFO - preprocess - Done writing bucket to ./training/train.bucket19.txt\n",
      "2022-11-21 20:48:30,020 - INFO - preprocess - Done writing bucket to ./training/train.bucket19.txt\n",
      "2022-11-21 20:48:30,022 - INFO - preprocess - Creating bucket 21 of 25 with 44 words\n",
      "2022-11-21 20:48:30,022 - INFO - preprocess - Creating bucket 21 of 25 with 44 words\n",
      "2022-11-21 20:48:30,282 - INFO - preprocess - Done writing bucket to ./training/train.bucket20.txt\n",
      "2022-11-21 20:48:30,282 - INFO - preprocess - Done writing bucket to ./training/train.bucket20.txt\n",
      "2022-11-21 20:48:30,283 - INFO - preprocess - Creating bucket 22 of 25 with 44 words\n",
      "2022-11-21 20:48:30,283 - INFO - preprocess - Creating bucket 22 of 25 with 44 words\n",
      "2022-11-21 20:48:30,547 - INFO - preprocess - Done writing bucket to ./training/train.bucket21.txt\n",
      "2022-11-21 20:48:30,547 - INFO - preprocess - Done writing bucket to ./training/train.bucket21.txt\n",
      "2022-11-21 20:48:30,548 - INFO - preprocess - Creating bucket 23 of 25 with 44 words\n",
      "2022-11-21 20:48:30,548 - INFO - preprocess - Creating bucket 23 of 25 with 44 words\n",
      "2022-11-21 20:48:30,804 - INFO - preprocess - Done writing bucket to ./training/train.bucket22.txt\n",
      "2022-11-21 20:48:30,804 - INFO - preprocess - Done writing bucket to ./training/train.bucket22.txt\n",
      "2022-11-21 20:48:30,805 - INFO - preprocess - Creating bucket 24 of 25 with 44 words\n",
      "2022-11-21 20:48:30,805 - INFO - preprocess - Creating bucket 24 of 25 with 44 words\n",
      "2022-11-21 20:48:31,065 - INFO - preprocess - Done writing bucket to ./training/train.bucket23.txt\n",
      "2022-11-21 20:48:31,065 - INFO - preprocess - Done writing bucket to ./training/train.bucket23.txt\n",
      "2022-11-21 20:48:31,066 - INFO - preprocess - Creating bucket 25 of 25 with 40 words\n",
      "2022-11-21 20:48:31,066 - INFO - preprocess - Creating bucket 25 of 25 with 40 words\n",
      "2022-11-21 20:48:31,314 - INFO - preprocess - Done writing bucket to ./training/train.bucket24.txt\n",
      "2022-11-21 20:48:31,314 - INFO - preprocess - Done writing bucket to ./training/train.bucket24.txt\n"
     ]
    }
   ],
   "source": [
    "%run fcm/preprocess.py train --input ./fcm/brown/brown.txt --output ./training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf8a07b",
   "metadata": {},
   "source": [
    "## Train the bertram context-only model (BERT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72665c4",
   "metadata": {},
   "source": [
    "Using the get_linear_schedule_with_warmup, change the transformers version to 2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "145d43a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-19 14:27:27.843389: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-19 14:27:31.072832: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /share/apps/rc/software/CUDA/9.2.88-GCC-7.3.0-2.30/extras/CUPTI/lib64:/share/apps/rc/software/CUDA/9.2.88-GCC-7.3.0-2.30/lib64:/share/apps/rc/software/binutils/2.30-GCCcore-7.3.0/lib:/share/apps/rc/software/GCCcore/7.3.0/lib64:/share/apps/rc/software/GCCcore/7.3.0/lib:/cm/shared/apps/cuda92/toolkit/9.2.88/extras/CUPTI/lib64:/cm/local/apps/cuda/libs/current/lib64:/cm/shared/apps/cuda92/toolkit/9.2.88/targets/x86_64-linux/lib:/cm/shared/apps/slurm/18.08.9/lib64/slurm:/cm/shared/apps/slurm/18.08.9/lib64\n",
      "2022-11-19 14:27:31.072875: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-11-19 14:27:31.399151: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-11-19 14:27:44.425234: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /share/apps/rc/software/CUDA/9.2.88-GCC-7.3.0-2.30/extras/CUPTI/lib64:/share/apps/rc/software/CUDA/9.2.88-GCC-7.3.0-2.30/lib64:/share/apps/rc/software/binutils/2.30-GCCcore-7.3.0/lib:/share/apps/rc/software/GCCcore/7.3.0/lib64:/share/apps/rc/software/GCCcore/7.3.0/lib:/cm/shared/apps/cuda92/toolkit/9.2.88/extras/CUPTI/lib64:/cm/local/apps/cuda/libs/current/lib64:/cm/shared/apps/cuda92/toolkit/9.2.88/targets/x86_64-linux/lib:/cm/shared/apps/slurm/18.08.9/lib64/slurm:/cm/shared/apps/slurm/18.08.9/lib64\n",
      "2022-11-19 14:27:44.425431: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /share/apps/rc/software/CUDA/9.2.88-GCC-7.3.0-2.30/extras/CUPTI/lib64:/share/apps/rc/software/CUDA/9.2.88-GCC-7.3.0-2.30/lib64:/share/apps/rc/software/binutils/2.30-GCCcore-7.3.0/lib:/share/apps/rc/software/GCCcore/7.3.0/lib64:/share/apps/rc/software/GCCcore/7.3.0/lib:/cm/shared/apps/cuda92/toolkit/9.2.88/extras/CUPTI/lib64:/cm/local/apps/cuda/libs/current/lib64:/cm/shared/apps/cuda92/toolkit/9.2.88/targets/x86_64-linux/lib:/cm/shared/apps/slurm/18.08.9/lib64/slurm:/cm/shared/apps/slurm/18.08.9/lib64\n",
      "2022-11-19 14:27:44.425443: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2022-11-19 14:28:14.284725: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /share/apps/rc/software/CUDA/9.2.88-GCC-7.3.0-2.30/extras/CUPTI/lib64:/share/apps/rc/software/CUDA/9.2.88-GCC-7.3.0-2.30/lib64:/share/apps/rc/software/binutils/2.30-GCCcore-7.3.0/lib:/share/apps/rc/software/GCCcore/7.3.0/lib64:/share/apps/rc/software/GCCcore/7.3.0/lib:/cm/shared/apps/cuda92/toolkit/9.2.88/extras/CUPTI/lib64:/cm/local/apps/cuda/libs/current/lib64:/cm/shared/apps/cuda92/toolkit/9.2.88/targets/x86_64-linux/lib:/cm/shared/apps/slurm/18.08.9/lib64/slurm:/cm/shared/apps/slurm/18.08.9/lib64\n",
      "2022-11-19 14:28:14.284904: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /share/apps/rc/software/CUDA/9.2.88-GCC-7.3.0-2.30/extras/CUPTI/lib64:/share/apps/rc/software/CUDA/9.2.88-GCC-7.3.0-2.30/lib64:/share/apps/rc/software/binutils/2.30-GCCcore-7.3.0/lib:/share/apps/rc/software/GCCcore/7.3.0/lib64:/share/apps/rc/software/GCCcore/7.3.0/lib:/cm/shared/apps/cuda92/toolkit/9.2.88/extras/CUPTI/lib64:/cm/local/apps/cuda/libs/current/lib64:/cm/shared/apps/cuda92/toolkit/9.2.88/targets/x86_64-linux/lib:/cm/shared/apps/slurm/18.08.9/lib64/slurm:/cm/shared/apps/slurm/18.08.9/lib64\n",
      "2022-11-19 14:28:14.285059: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /share/apps/rc/software/CUDA/9.2.88-GCC-7.3.0-2.30/extras/CUPTI/lib64:/share/apps/rc/software/CUDA/9.2.88-GCC-7.3.0-2.30/lib64:/share/apps/rc/software/binutils/2.30-GCCcore-7.3.0/lib:/share/apps/rc/software/GCCcore/7.3.0/lib64:/share/apps/rc/software/GCCcore/7.3.0/lib:/cm/shared/apps/cuda92/toolkit/9.2.88/extras/CUPTI/lib64:/cm/local/apps/cuda/libs/current/lib64:/cm/shared/apps/cuda92/toolkit/9.2.88/targets/x86_64-linux/lib:/cm/shared/apps/slurm/18.08.9/lib64/slurm:/cm/shared/apps/slurm/18.08.9/lib64\n",
      "2022-11-19 14:28:14.285237: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /share/apps/rc/software/CUDA/9.2.88-GCC-7.3.0-2.30/extras/CUPTI/lib64:/share/apps/rc/software/CUDA/9.2.88-GCC-7.3.0-2.30/lib64:/share/apps/rc/software/binutils/2.30-GCCcore-7.3.0/lib:/share/apps/rc/software/GCCcore/7.3.0/lib64:/share/apps/rc/software/GCCcore/7.3.0/lib:/cm/shared/apps/cuda92/toolkit/9.2.88/extras/CUPTI/lib64:/cm/local/apps/cuda/libs/current/lib64:/cm/shared/apps/cuda92/toolkit/9.2.88/targets/x86_64-linux/lib:/cm/shared/apps/slurm/18.08.9/lib64/slurm:/cm/shared/apps/slurm/18.08.9/lib64\n",
      "2022-11-19 14:28:14.285428: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /share/apps/rc/software/CUDA/9.2.88-GCC-7.3.0-2.30/extras/CUPTI/lib64:/share/apps/rc/software/CUDA/9.2.88-GCC-7.3.0-2.30/lib64:/share/apps/rc/software/binutils/2.30-GCCcore-7.3.0/lib:/share/apps/rc/software/GCCcore/7.3.0/lib64:/share/apps/rc/software/GCCcore/7.3.0/lib:/cm/shared/apps/cuda92/toolkit/9.2.88/extras/CUPTI/lib64:/cm/local/apps/cuda/libs/current/lib64:/cm/shared/apps/cuda92/toolkit/9.2.88/targets/x86_64-linux/lib:/cm/shared/apps/slurm/18.08.9/lib64/slurm:/cm/shared/apps/slurm/18.08.9/lib64\n",
      "2022-11-19 14:28:14.285598: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /share/apps/rc/software/CUDA/9.2.88-GCC-7.3.0-2.30/extras/CUPTI/lib64:/share/apps/rc/software/CUDA/9.2.88-GCC-7.3.0-2.30/lib64:/share/apps/rc/software/binutils/2.30-GCCcore-7.3.0/lib:/share/apps/rc/software/GCCcore/7.3.0/lib64:/share/apps/rc/software/GCCcore/7.3.0/lib:/cm/shared/apps/cuda92/toolkit/9.2.88/extras/CUPTI/lib64:/cm/local/apps/cuda/libs/current/lib64:/cm/shared/apps/cuda92/toolkit/9.2.88/targets/x86_64-linux/lib:/cm/shared/apps/slurm/18.08.9/lib64/slurm:/cm/shared/apps/slurm/18.08.9/lib64\n",
      "2022-11-19 14:28:14.285751: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /share/apps/rc/software/CUDA/9.2.88-GCC-7.3.0-2.30/extras/CUPTI/lib64:/share/apps/rc/software/CUDA/9.2.88-GCC-7.3.0-2.30/lib64:/share/apps/rc/software/binutils/2.30-GCCcore-7.3.0/lib:/share/apps/rc/software/GCCcore/7.3.0/lib64:/share/apps/rc/software/GCCcore/7.3.0/lib:/cm/shared/apps/cuda92/toolkit/9.2.88/extras/CUPTI/lib64:/cm/local/apps/cuda/libs/current/lib64:/cm/shared/apps/cuda92/toolkit/9.2.88/targets/x86_64-linux/lib:/cm/shared/apps/slurm/18.08.9/lib64/slurm:/cm/shared/apps/slurm/18.08.9/lib64\n",
      "2022-11-19 14:28:14.285902: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /share/apps/rc/software/CUDA/9.2.88-GCC-7.3.0-2.30/extras/CUPTI/lib64:/share/apps/rc/software/CUDA/9.2.88-GCC-7.3.0-2.30/lib64:/share/apps/rc/software/binutils/2.30-GCCcore-7.3.0/lib:/share/apps/rc/software/GCCcore/7.3.0/lib64:/share/apps/rc/software/GCCcore/7.3.0/lib:/cm/shared/apps/cuda92/toolkit/9.2.88/extras/CUPTI/lib64:/cm/local/apps/cuda/libs/current/lib64:/cm/shared/apps/cuda92/toolkit/9.2.88/targets/x86_64-linux/lib:/cm/shared/apps/slurm/18.08.9/lib64/slurm:/cm/shared/apps/slurm/18.08.9/lib64\n",
      "2022-11-19 14:28:14.285949: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-11-19 14:28:14.286913: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "#from transformers import get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "811f9ddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT_form-e1\n",
      "BERT_form-e10\n",
      "BERT_form-e5\n",
      "BERT_form\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "dir_path = \"./outputs/\"\n",
    "\n",
    "file_list = os.listdir(dir_path)\n",
    "for file in file_list:\n",
    "    print(file)\n",
    "    shutil.rmtree(dir_path + file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7dd6b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-22 16:37:05,568 - INFO - train - Initializing new BERTRAM instance from bert-base-uncased.\n",
      "2022-11-22 16:37:05,688 - INFO - tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/sungman/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "2022-11-22 16:37:05,732 - INFO - ngram_models - Found 712 ngrams with min count 4 and (nmin,nmax)=(3,5), first 10: ['UNK', 'PAD', 'ed<S>', 'ng<S>', 'ing', 'er<S>', 'ing<S>', 'on<S>', '<S>co', 'ion'], last 10: ['tly', 'tly<S>', 'cor', 'anc', 'ance', 'ance<S>', '<S>des', 'des', 'sio', 'sion']\n",
      "2022-11-22 16:37:05,734 - INFO - utils - Loading embeddings from ./fcm/wordEmbeddings/glove.6B.50d.txt\n",
      "2022-11-22 16:56:30,718 - INFO - utils - Done loading embeddings\n",
      "2022-11-22 16:56:30,847 - INFO - configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/sungman/.cache/torch/transformers/distributed_-1/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
      "2022-11-22 16:56:30,850 - INFO - configuration_utils - Model config {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "2022-11-22 16:56:30,955 - INFO - modeling_utils - loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /home/sungman/.cache/torch/transformers/distributed_-1/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "2022-11-22 16:56:33,759 - INFO - modeling_utils - Weights of Bertram not initialized from pretrained model: ['reliability_measure.linear.weight', 'reliability_measure.linear.bias', 'linear.weight', 'linear.bias']\n",
      "2022-11-22 16:56:33,760 - INFO - modeling_utils - Weights from pretrained model not used in Bertram: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]2022-11-22 16:56:33,835 - INFO - input_processor - Processing training file 1 of 25: ./training/train.bucket16.txt\n",
      "2022-11-22 16:56:33,873 - INFO - input_processor - Done processing training file, batch size is 78\n",
      "2022-11-22 16:56:33,874 - INFO - input_processor - Processing training file 2 of 25: ./training/train.bucket12.txt\n",
      "2022-11-22 16:56:33,954 - INFO - input_processor - Done processing training file, batch size is 185\n",
      "2022-11-22 16:56:33,955 - INFO - input_processor - Processing training file 3 of 25: ./training/train.bucket9.txt\n",
      "2022-11-22 16:56:34,009 - INFO - input_processor - Done processing training file, batch size is 277\n",
      "2022-11-22 16:56:34,010 - INFO - input_processor - Processing training file 4 of 25: ./training/train.bucket19.txt\n",
      "2022-11-22 16:56:34,042 - INFO - input_processor - Done processing training file, batch size is 366\n",
      "2022-11-22 16:56:34,042 - INFO - input_processor - Processing training file 5 of 25: ./training/train.bucket18.txt\n",
      "2022-11-22 16:56:34,077 - INFO - input_processor - Done processing training file, batch size is 456\n",
      "2022-11-22 16:56:34,078 - INFO - input_processor - Processing training file 6 of 25: ./training/train.bucket6.txt\n",
      "2022-11-22 16:56:34,130 - INFO - input_processor - Done processing training file, batch size is 551\n",
      "2022-11-22 16:56:34,131 - INFO - input_processor - Processing training file 7 of 25: ./training/train.bucket5.txt\n",
      "2022-11-22 16:56:34,167 - INFO - input_processor - Done processing training file, batch size is 648\n",
      "2022-11-22 16:56:34,167 - INFO - input_processor - Processing training file 8 of 25: ./training/train.bucket10.txt\n",
      "2022-11-22 16:56:34,212 - INFO - input_processor - Done processing training file, batch size is 750\n",
      "2022-11-22 16:56:34,212 - INFO - input_processor - Processing training file 9 of 25: ./training/train.bucket15.txt\n",
      "2022-11-22 16:56:34,238 - INFO - input_processor - Done processing training file, batch size is 824\n",
      "2022-11-22 16:56:34,239 - INFO - input_processor - Processing training file 10 of 25: ./training/train.bucket21.txt\n",
      "2022-11-22 16:56:34,278 - INFO - input_processor - Done processing training file, batch size is 930\n",
      "2022-11-22 16:56:34,279 - INFO - input_processor - Processing training file 11 of 25: ./training/train.bucket11.txt\n",
      "2022-11-22 16:56:34,320 - INFO - input_processor - Done processing training file, batch size is 1008\n",
      "2022-11-22 16:56:34,320 - INFO - input_processor - Processing training file 12 of 25: ./training/train.bucket17.txt\n",
      "2022-11-22 16:56:34,352 - INFO - input_processor - Done processing training file, batch size is 1080\n",
      "2022-11-22 16:56:34,353 - INFO - input_processor - Processing training file 13 of 25: ./training/train.bucket1.txt\n",
      "2022-11-22 16:56:34,404 - INFO - input_processor - Done processing training file, batch size is 1193\n",
      "2022-11-22 16:56:34,409 - INFO - input_processor - Processing training file 14 of 25: ./training/train.bucket14.txt\n",
      "2022-11-22 16:56:34,452 - INFO - input_processor - Done processing training file, batch size is 1285\n",
      "2022-11-22 16:56:34,452 - INFO - input_processor - Processing training file 15 of 25: ./training/train.bucket22.txt\n",
      "2022-11-22 16:56:34,489 - INFO - input_processor - Done processing training file, batch size is 1373\n",
      "2022-11-22 16:56:34,489 - INFO - input_processor - Processing training file 16 of 25: ./training/train.bucket13.txt\n",
      "2022-11-22 16:56:34,525 - INFO - input_processor - Done processing training file, batch size is 1451\n",
      "2022-11-22 16:56:34,525 - INFO - input_processor - Processing training file 17 of 25: ./training/train.bucket2.txt\n",
      "2022-11-22 16:56:34,573 - INFO - input_processor - Done processing training file, batch size is 1535\n",
      "2022-11-22 16:56:34,574 - INFO - input_processor - Processing training file 18 of 25: ./training/train.bucket23.txt\n",
      "2022-11-22 16:56:34,611 - INFO - input_processor - Done processing training file, batch size is 1637\n",
      "2022-11-22 16:56:34,611 - INFO - input_processor - Processing training file 19 of 25: ./training/train.bucket4.txt\n",
      "2022-11-22 16:56:34,652 - INFO - input_processor - Done processing training file, batch size is 1738\n",
      "2022-11-22 16:56:34,652 - INFO - input_processor - Processing training file 20 of 25: ./training/train.bucket24.txt\n",
      "2022-11-22 16:56:34,697 - INFO - input_processor - Done processing training file, batch size is 1847\n",
      "2022-11-22 16:56:34,698 - INFO - input_processor - Processing training file 21 of 25: ./training/train.bucket7.txt\n",
      "2022-11-22 16:56:34,737 - INFO - input_processor - Done processing training file, batch size is 1949\n",
      "2022-11-22 16:56:34,738 - INFO - input_processor - Processing training file 22 of 25: ./training/train.bucket8.txt\n",
      "2022-11-22 16:56:34,780 - INFO - input_processor - Done processing training file, batch size is 2040\n",
      "2022-11-22 16:56:34,781 - INFO - input_processor - Processing training file 23 of 25: ./training/train.bucket0.txt\n",
      "2022-11-22 16:56:34,812 - INFO - input_processor - Done processing training file, batch size is 2119\n",
      "2022-11-22 16:56:34,813 - INFO - input_processor - Processing training file 24 of 25: ./training/train.bucket3.txt\n",
      "2022-11-22 16:56:34,848 - INFO - input_processor - Done processing training file, batch size is 2207\n",
      "2022-11-22 16:56:34,849 - INFO - input_processor - Processing training file 25 of 25: ./training/train.bucket20.txt\n",
      "2022-11-22 16:56:34,891 - INFO - input_processor - Done processing training file, batch size is 2315\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/user/home/sungman/test/NLP Group Project/bertram-master/train.py:309: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(loss, requires_grad = True)\n",
      "/home/sungman/.local/lib/python3.9/site-packages/transformers/optimization.py:146: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1420.)\n",
      "  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-22 16:57:11,128 - INFO - train - Step: 100\tLoss: 0.05520935729146004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-22 16:57:42,300 - INFO - train - Step: 200\tLoss: 0.03912439653649926\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-22 16:58:13,467 - INFO - train - Step: 300\tLoss: 0.03247760891914368\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-22 16:58:44,737 - INFO - train - Step: 400\tLoss: 0.02920226166024804\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-22 16:59:16,077 - INFO - train - Step: 500\tLoss: 0.02747133860364556\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-22 16:59:47,432 - INFO - train - Step: 600\tLoss: 0.02601403240114451\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-22 17:00:18,796 - INFO - train - Step: 700\tLoss: 0.02393877808004618\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-22 17:00:32,281 - INFO - input_processor - Reached the end of the dataset\n",
      "2022-11-22 17:00:32,281 - INFO - train - Done with epoch 0\n",
      "Epoch: 100%|██████████| 1/1 [03:58<00:00, 238.91s/it]\n"
     ]
    }
   ],
   "source": [
    "#Change Hyper parameter here ( --emb_dim 768 -> 50 )\n",
    "%run train.py --model_cls bert --bert_model 'bert-base-uncased' --output_dir ./outputs/BERT_context --train_dir ./training/ --vocab ./training/train.vwc100 --emb_file ./fcm/wordEmbeddings/glove.6B.50d.txt --num_train_epochs 1 --emb_dim 768 --max_seq_length 100 --mode context --train_batch_size 32 --no_finetuning --smin 4 --smax 32\n",
    "\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb95f57",
   "metadata": {},
   "source": [
    "### Train the bertram form-only model (BERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e948060",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-22 16:12:19.362249: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-22 16:12:23.827098: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-11-22 16:12:38.751656: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /share/apps/rc/software/CUDA/9.2.88-GCC-7.3.0-2.30/extras/CUPTI/lib64:/share/apps/rc/software/CUDA/9.2.88-GCC-7.3.0-2.30/lib64:/share/apps/rc/software/binutils/2.30-GCCcore-7.3.0/lib:/share/apps/rc/software/GCCcore/7.3.0/lib64:/share/apps/rc/software/GCCcore/7.3.0/lib:/cm/shared/apps/cuda92/toolkit/9.2.88/extras/CUPTI/lib64:/cm/local/apps/cuda/libs/current/lib64:/cm/shared/apps/cuda92/toolkit/9.2.88/targets/x86_64-linux/lib:/cm/shared/apps/slurm/18.08.9/lib64/slurm:/cm/shared/apps/slurm/18.08.9/lib64\n",
      "2022-11-22 16:12:38.751880: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /share/apps/rc/software/CUDA/9.2.88-GCC-7.3.0-2.30/extras/CUPTI/lib64:/share/apps/rc/software/CUDA/9.2.88-GCC-7.3.0-2.30/lib64:/share/apps/rc/software/binutils/2.30-GCCcore-7.3.0/lib:/share/apps/rc/software/GCCcore/7.3.0/lib64:/share/apps/rc/software/GCCcore/7.3.0/lib:/cm/shared/apps/cuda92/toolkit/9.2.88/extras/CUPTI/lib64:/cm/local/apps/cuda/libs/current/lib64:/cm/shared/apps/cuda92/toolkit/9.2.88/targets/x86_64-linux/lib:/cm/shared/apps/slurm/18.08.9/lib64/slurm:/cm/shared/apps/slurm/18.08.9/lib64\n",
      "2022-11-22 16:12:38.751909: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2022-11-22 16:13:07.226385: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /share/apps/rc/software/CUDA/9.2.88-GCC-7.3.0-2.30/extras/CUPTI/lib64:/share/apps/rc/software/CUDA/9.2.88-GCC-7.3.0-2.30/lib64:/share/apps/rc/software/binutils/2.30-GCCcore-7.3.0/lib:/share/apps/rc/software/GCCcore/7.3.0/lib64:/share/apps/rc/software/GCCcore/7.3.0/lib:/cm/shared/apps/cuda92/toolkit/9.2.88/extras/CUPTI/lib64:/cm/local/apps/cuda/libs/current/lib64:/cm/shared/apps/cuda92/toolkit/9.2.88/targets/x86_64-linux/lib:/cm/shared/apps/slurm/18.08.9/lib64/slurm:/cm/shared/apps/slurm/18.08.9/lib64\n",
      "2022-11-22 16:13:07.226594: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /share/apps/rc/software/CUDA/9.2.88-GCC-7.3.0-2.30/extras/CUPTI/lib64:/share/apps/rc/software/CUDA/9.2.88-GCC-7.3.0-2.30/lib64:/share/apps/rc/software/binutils/2.30-GCCcore-7.3.0/lib:/share/apps/rc/software/GCCcore/7.3.0/lib64:/share/apps/rc/software/GCCcore/7.3.0/lib:/cm/shared/apps/cuda92/toolkit/9.2.88/extras/CUPTI/lib64:/cm/local/apps/cuda/libs/current/lib64:/cm/shared/apps/cuda92/toolkit/9.2.88/targets/x86_64-linux/lib:/cm/shared/apps/slurm/18.08.9/lib64/slurm:/cm/shared/apps/slurm/18.08.9/lib64\n",
      "2022-11-22 16:13:07.226781: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /share/apps/rc/software/CUDA/9.2.88-GCC-7.3.0-2.30/extras/CUPTI/lib64:/share/apps/rc/software/CUDA/9.2.88-GCC-7.3.0-2.30/lib64:/share/apps/rc/software/binutils/2.30-GCCcore-7.3.0/lib:/share/apps/rc/software/GCCcore/7.3.0/lib64:/share/apps/rc/software/GCCcore/7.3.0/lib:/cm/shared/apps/cuda92/toolkit/9.2.88/extras/CUPTI/lib64:/cm/local/apps/cuda/libs/current/lib64:/cm/shared/apps/cuda92/toolkit/9.2.88/targets/x86_64-linux/lib:/cm/shared/apps/slurm/18.08.9/lib64/slurm:/cm/shared/apps/slurm/18.08.9/lib64\n",
      "2022-11-22 16:13:07.226948: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /share/apps/rc/software/CUDA/9.2.88-GCC-7.3.0-2.30/extras/CUPTI/lib64:/share/apps/rc/software/CUDA/9.2.88-GCC-7.3.0-2.30/lib64:/share/apps/rc/software/binutils/2.30-GCCcore-7.3.0/lib:/share/apps/rc/software/GCCcore/7.3.0/lib64:/share/apps/rc/software/GCCcore/7.3.0/lib:/cm/shared/apps/cuda92/toolkit/9.2.88/extras/CUPTI/lib64:/cm/local/apps/cuda/libs/current/lib64:/cm/shared/apps/cuda92/toolkit/9.2.88/targets/x86_64-linux/lib:/cm/shared/apps/slurm/18.08.9/lib64/slurm:/cm/shared/apps/slurm/18.08.9/lib64\n",
      "2022-11-22 16:13:07.227020: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-11-22 16:13:07.228004: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-22 16:13:11,402 - INFO - train - Initializing new BERTRAM instance from bert-base-uncased.\n",
      "2022-11-22 16:13:11,564 - INFO - tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/sungman/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "2022-11-22 16:13:11,630 - INFO - ngram_models - Found 712 ngrams with min count 4 and (nmin,nmax)=(3,5), first 10: ['UNK', 'PAD', 'ed<S>', 'ng<S>', 'ing', 'er<S>', 'ing<S>', 'on<S>', '<S>co', 'ion'], last 10: ['tly', 'tly<S>', 'cor', 'anc', 'ance', 'ance<S>', '<S>des', 'des', 'sio', 'sion']\n",
      "2022-11-22 16:13:11,631 - INFO - utils - Loading embeddings from ./fcm/wordEmbeddings/glove.6B.50d.txt\n",
      "2022-11-22 16:32:38,265 - INFO - utils - Done loading embeddings\n",
      "2022-11-22 16:32:38,402 - INFO - configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/sungman/.cache/torch/transformers/distributed_-1/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
      "2022-11-22 16:32:38,404 - INFO - configuration_utils - Model config {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "2022-11-22 16:32:38,517 - INFO - modeling_utils - loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /home/sungman/.cache/torch/transformers/distributed_-1/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "2022-11-22 16:32:38,913 - INFO - modeling_utils - Weights of Bertram not initialized from pretrained model: ['bert.ngram_processor.ngram_embeddings.weight']\n",
      "2022-11-22 16:32:38,914 - INFO - modeling_utils - Weights from pretrained model not used in Bertram: ['bert.embeddings.word_embeddings.weight', 'bert.embeddings.position_embeddings.weight', 'bert.embeddings.token_type_embeddings.weight', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.encoder.layer.0.attention.output.dense.bias', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.0.output.dense.bias', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.1.attention.output.dense.bias', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.2.attention.output.dense.bias', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.encoder.layer.3.attention.output.dense.bias', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.4.attention.output.dense.bias', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.5.attention.output.dense.bias', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.11.output.dense.bias', 'bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'bert.embeddings.LayerNorm.weight', 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.11.output.LayerNorm.bias']\n",
      "Epoch:   0%|          | 0/20 [00:00<?, ?it/s]2022-11-22 16:32:48,832 - INFO - input_processor - Processing training file 1 of 25: ./training/train.bucket16.txt\n",
      "2022-11-22 16:32:48,836 - INFO - input_processor - Done processing training file, batch size is 2364\n",
      "2022-11-22 16:32:48,837 - INFO - input_processor - Processing training file 2 of 25: ./training/train.bucket12.txt\n",
      "2022-11-22 16:32:48,841 - INFO - input_processor - Done processing training file, batch size is 4728\n",
      "2022-11-22 16:32:48,842 - INFO - input_processor - Processing training file 3 of 25: ./training/train.bucket9.txt\n",
      "2022-11-22 16:32:48,847 - INFO - input_processor - Done processing training file, batch size is 7092\n",
      "2022-11-22 16:32:48,848 - INFO - input_processor - Processing training file 4 of 25: ./training/train.bucket19.txt\n",
      "2022-11-22 16:32:48,854 - INFO - input_processor - Done processing training file, batch size is 9456\n",
      "2022-11-22 16:32:48,855 - INFO - input_processor - Processing training file 5 of 25: ./training/train.bucket18.txt\n",
      "2022-11-22 16:32:48,862 - INFO - input_processor - Done processing training file, batch size is 11820\n",
      "2022-11-22 16:32:48,862 - INFO - input_processor - Processing training file 6 of 25: ./training/train.bucket6.txt\n",
      "2022-11-22 16:32:48,872 - INFO - input_processor - Done processing training file, batch size is 14184\n",
      "2022-11-22 16:32:48,873 - INFO - input_processor - Processing training file 7 of 25: ./training/train.bucket5.txt\n",
      "2022-11-22 16:32:48,882 - INFO - input_processor - Done processing training file, batch size is 16548\n",
      "2022-11-22 16:32:48,883 - INFO - input_processor - Processing training file 8 of 25: ./training/train.bucket10.txt\n",
      "2022-11-22 16:32:48,894 - INFO - input_processor - Done processing training file, batch size is 18912\n",
      "2022-11-22 16:32:48,894 - INFO - input_processor - Processing training file 9 of 25: ./training/train.bucket15.txt\n",
      "2022-11-22 16:32:48,905 - INFO - input_processor - Done processing training file, batch size is 21276\n",
      "2022-11-22 16:32:48,906 - INFO - input_processor - Processing training file 10 of 25: ./training/train.bucket21.txt\n",
      "2022-11-22 16:32:48,918 - INFO - input_processor - Done processing training file, batch size is 23640\n",
      "2022-11-22 16:32:48,918 - INFO - input_processor - Processing training file 11 of 25: ./training/train.bucket11.txt\n",
      "2022-11-22 16:32:48,932 - INFO - input_processor - Done processing training file, batch size is 26004\n",
      "2022-11-22 16:32:48,932 - INFO - input_processor - Processing training file 12 of 25: ./training/train.bucket17.txt\n",
      "2022-11-22 16:32:48,946 - INFO - input_processor - Done processing training file, batch size is 28368\n",
      "2022-11-22 16:32:48,946 - INFO - input_processor - Processing training file 13 of 25: ./training/train.bucket1.txt\n",
      "2022-11-22 16:32:48,961 - INFO - input_processor - Done processing training file, batch size is 30732\n",
      "2022-11-22 16:32:48,962 - INFO - input_processor - Processing training file 14 of 25: ./training/train.bucket14.txt\n",
      "2022-11-22 16:32:48,977 - INFO - input_processor - Done processing training file, batch size is 33096\n",
      "2022-11-22 16:32:48,978 - INFO - input_processor - Processing training file 15 of 25: ./training/train.bucket22.txt\n",
      "2022-11-22 16:32:48,995 - INFO - input_processor - Done processing training file, batch size is 35460\n",
      "2022-11-22 16:32:48,995 - INFO - input_processor - Processing training file 16 of 25: ./training/train.bucket13.txt\n",
      "2022-11-22 16:32:49,013 - INFO - input_processor - Done processing training file, batch size is 37824\n",
      "2022-11-22 16:32:49,013 - INFO - input_processor - Processing training file 17 of 25: ./training/train.bucket2.txt\n",
      "2022-11-22 16:32:49,031 - INFO - input_processor - Done processing training file, batch size is 40188\n",
      "2022-11-22 16:32:49,032 - INFO - input_processor - Processing training file 18 of 25: ./training/train.bucket23.txt\n",
      "2022-11-22 16:32:49,052 - INFO - input_processor - Done processing training file, batch size is 42552\n",
      "2022-11-22 16:32:49,052 - INFO - input_processor - Processing training file 19 of 25: ./training/train.bucket4.txt\n",
      "2022-11-22 16:32:49,073 - INFO - input_processor - Done processing training file, batch size is 44916\n",
      "2022-11-22 16:32:49,073 - INFO - input_processor - Processing training file 20 of 25: ./training/train.bucket24.txt\n",
      "2022-11-22 16:32:49,094 - INFO - input_processor - Done processing training file, batch size is 47280\n",
      "2022-11-22 16:32:49,095 - INFO - input_processor - Processing training file 21 of 25: ./training/train.bucket7.txt\n",
      "2022-11-22 16:32:49,117 - INFO - input_processor - Done processing training file, batch size is 49644\n",
      "2022-11-22 16:32:49,117 - INFO - input_processor - Processing training file 22 of 25: ./training/train.bucket8.txt\n",
      "2022-11-22 16:32:49,140 - INFO - input_processor - Done processing training file, batch size is 52008\n",
      "2022-11-22 16:32:49,141 - INFO - input_processor - Processing training file 23 of 25: ./training/train.bucket0.txt\n",
      "2022-11-22 16:32:49,164 - INFO - input_processor - Done processing training file, batch size is 54372\n",
      "2022-11-22 16:32:49,165 - INFO - input_processor - Processing training file 24 of 25: ./training/train.bucket3.txt\n",
      "2022-11-22 16:32:49,190 - INFO - input_processor - Done processing training file, batch size is 56736\n",
      "2022-11-22 16:32:49,190 - INFO - input_processor - Processing training file 25 of 25: ./training/train.bucket20.txt\n",
      "2022-11-22 16:32:49,216 - INFO - input_processor - Done processing training file, batch size is 59100\n",
      "/data/user/home/sungman/test/NLP Group Project/bertram-master/train.py:309: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(loss, requires_grad = True)\n",
      "2022-11-22 16:32:50,171 - INFO - train - Step: 100\tLoss: 0.46356624066829683\n",
      "2022-11-22 16:32:50,698 - INFO - train - Step: 200\tLoss: 0.19258097253739834\n",
      "2022-11-22 16:32:51,216 - INFO - train - Step: 300\tLoss: 0.07471336081624030\n",
      "2022-11-22 16:32:51,734 - INFO - train - Step: 400\tLoss: 0.04511065315455198\n",
      "2022-11-22 16:32:52,250 - INFO - train - Step: 500\tLoss: 0.03223543222993612\n",
      "2022-11-22 16:32:52,766 - INFO - train - Step: 600\tLoss: 0.02674482442438602\n",
      "2022-11-22 16:32:53,282 - INFO - train - Step: 700\tLoss: 0.02290172385051846\n",
      "2022-11-22 16:32:53,799 - INFO - train - Step: 800\tLoss: 0.02156629549339414\n",
      "2022-11-22 16:32:54,315 - INFO - train - Step: 900\tLoss: 0.01940727573819458\n",
      "2022-11-22 16:32:54,831 - INFO - train - Step: 1000\tLoss: 0.01872586571611464\n",
      "2022-11-22 16:32:55,347 - INFO - train - Step: 1100\tLoss: 0.01781816732138395\n",
      "2022-11-22 16:32:55,863 - INFO - train - Step: 1200\tLoss: 0.01692887963727117\n",
      "2022-11-22 16:32:56,378 - INFO - train - Step: 1300\tLoss: 0.01678026651963592\n",
      "2022-11-22 16:32:56,894 - INFO - train - Step: 1400\tLoss: 0.01654046917334199\n",
      "2022-11-22 16:32:57,410 - INFO - train - Step: 1500\tLoss: 0.01637488755397499\n",
      "2022-11-22 16:32:57,926 - INFO - train - Step: 1600\tLoss: 0.01649006988853216\n",
      "2022-11-22 16:32:58,442 - INFO - train - Step: 1700\tLoss: 0.01659682008437812\n",
      "2022-11-22 16:32:58,958 - INFO - train - Step: 1800\tLoss: 0.01661490725353360\n",
      "2022-11-22 16:32:59,189 - INFO - input_processor - Reached the end of the dataset\n",
      "2022-11-22 16:32:59,190 - INFO - train - Done with epoch 0\n",
      "Epoch:   5%|▌         | 1/20 [00:10<03:17, 10.39s/it]2022-11-22 16:32:59,218 - INFO - input_processor - Processing training file 1 of 25: ./training/train.bucket12.txt\n",
      "2022-11-22 16:32:59,222 - INFO - input_processor - Done processing training file, batch size is 2364\n",
      "2022-11-22 16:32:59,223 - INFO - input_processor - Processing training file 2 of 25: ./training/train.bucket14.txt\n",
      "2022-11-22 16:32:59,227 - INFO - input_processor - Done processing training file, batch size is 4728\n",
      "2022-11-22 16:32:59,228 - INFO - input_processor - Processing training file 3 of 25: ./training/train.bucket4.txt\n",
      "2022-11-22 16:32:59,234 - INFO - input_processor - Done processing training file, batch size is 7092\n",
      "2022-11-22 16:32:59,235 - INFO - input_processor - Processing training file 4 of 25: ./training/train.bucket20.txt\n",
      "2022-11-22 16:32:59,241 - INFO - input_processor - Done processing training file, batch size is 9456\n",
      "2022-11-22 16:32:59,242 - INFO - input_processor - Processing training file 5 of 25: ./training/train.bucket15.txt\n",
      "2022-11-22 16:32:59,249 - INFO - input_processor - Done processing training file, batch size is 11820\n",
      "2022-11-22 16:32:59,250 - INFO - input_processor - Processing training file 6 of 25: ./training/train.bucket5.txt\n",
      "2022-11-22 16:32:59,258 - INFO - input_processor - Done processing training file, batch size is 14184\n",
      "2022-11-22 16:32:59,258 - INFO - input_processor - Processing training file 7 of 25: ./training/train.bucket1.txt\n",
      "2022-11-22 16:32:59,267 - INFO - input_processor - Done processing training file, batch size is 16548\n",
      "2022-11-22 16:32:59,267 - INFO - input_processor - Processing training file 8 of 25: ./training/train.bucket17.txt\n",
      "2022-11-22 16:32:59,277 - INFO - input_processor - Done processing training file, batch size is 18912\n",
      "2022-11-22 16:32:59,278 - INFO - input_processor - Processing training file 9 of 25: ./training/train.bucket3.txt\n",
      "2022-11-22 16:32:59,289 - INFO - input_processor - Done processing training file, batch size is 21276\n",
      "2022-11-22 16:32:59,289 - INFO - input_processor - Processing training file 10 of 25: ./training/train.bucket21.txt\n",
      "2022-11-22 16:32:59,301 - INFO - input_processor - Done processing training file, batch size is 23640\n",
      "2022-11-22 16:32:59,301 - INFO - input_processor - Processing training file 11 of 25: ./training/train.bucket8.txt\n",
      "2022-11-22 16:32:59,314 - INFO - input_processor - Done processing training file, batch size is 26004\n",
      "2022-11-22 16:32:59,315 - INFO - input_processor - Processing training file 12 of 25: ./training/train.bucket24.txt\n",
      "2022-11-22 16:32:59,329 - INFO - input_processor - Done processing training file, batch size is 28368\n",
      "2022-11-22 16:32:59,329 - INFO - input_processor - Processing training file 13 of 25: ./training/train.bucket16.txt\n",
      "2022-11-22 16:32:59,344 - INFO - input_processor - Done processing training file, batch size is 30732\n",
      "2022-11-22 16:32:59,345 - INFO - input_processor - Processing training file 14 of 25: ./training/train.bucket10.txt\n",
      "2022-11-22 16:32:59,360 - INFO - input_processor - Done processing training file, batch size is 33096\n",
      "2022-11-22 16:32:59,360 - INFO - input_processor - Processing training file 15 of 25: ./training/train.bucket11.txt\n",
      "2022-11-22 16:32:59,378 - INFO - input_processor - Done processing training file, batch size is 35460\n",
      "2022-11-22 16:32:59,379 - INFO - input_processor - Processing training file 16 of 25: ./training/train.bucket18.txt\n",
      "2022-11-22 16:32:59,397 - INFO - input_processor - Done processing training file, batch size is 37824\n",
      "2022-11-22 16:32:59,397 - INFO - input_processor - Processing training file 17 of 25: ./training/train.bucket9.txt\n",
      "2022-11-22 16:32:59,416 - INFO - input_processor - Done processing training file, batch size is 40188\n",
      "2022-11-22 16:32:59,417 - INFO - input_processor - Processing training file 18 of 25: ./training/train.bucket13.txt\n",
      "2022-11-22 16:32:59,436 - INFO - input_processor - Done processing training file, batch size is 42552\n",
      "2022-11-22 16:32:59,436 - INFO - input_processor - Processing training file 19 of 25: ./training/train.bucket19.txt\n",
      "2022-11-22 16:32:59,457 - INFO - input_processor - Done processing training file, batch size is 44916\n",
      "2022-11-22 16:32:59,457 - INFO - input_processor - Processing training file 20 of 25: ./training/train.bucket2.txt\n",
      "2022-11-22 16:32:59,479 - INFO - input_processor - Done processing training file, batch size is 47280\n",
      "2022-11-22 16:32:59,480 - INFO - input_processor - Processing training file 21 of 25: ./training/train.bucket0.txt\n",
      "2022-11-22 16:32:59,502 - INFO - input_processor - Done processing training file, batch size is 49644\n",
      "2022-11-22 16:32:59,503 - INFO - input_processor - Processing training file 22 of 25: ./training/train.bucket23.txt\n",
      "2022-11-22 16:32:59,525 - INFO - input_processor - Done processing training file, batch size is 52008\n",
      "2022-11-22 16:32:59,526 - INFO - input_processor - Processing training file 23 of 25: ./training/train.bucket6.txt\n",
      "2022-11-22 16:32:59,549 - INFO - input_processor - Done processing training file, batch size is 54372\n",
      "2022-11-22 16:32:59,550 - INFO - input_processor - Processing training file 24 of 25: ./training/train.bucket7.txt\n",
      "2022-11-22 16:32:59,574 - INFO - input_processor - Done processing training file, batch size is 56736\n",
      "2022-11-22 16:32:59,575 - INFO - input_processor - Processing training file 25 of 25: ./training/train.bucket22.txt\n",
      "2022-11-22 16:32:59,600 - INFO - input_processor - Done processing training file, batch size is 59100\n",
      "2022-11-22 16:33:00,136 - INFO - train - Step: 100\tLoss: 0.01639120806008577\n",
      "2022-11-22 16:33:00,693 - INFO - train - Step: 200\tLoss: 0.01655784524045885\n",
      "2022-11-22 16:33:01,231 - INFO - train - Step: 300\tLoss: 0.01638255232013762\n",
      "2022-11-22 16:33:01,744 - INFO - train - Step: 400\tLoss: 0.01621568105183542\n",
      "2022-11-22 16:33:02,257 - INFO - train - Step: 500\tLoss: 0.01639593427069485\n",
      "2022-11-22 16:33:02,769 - INFO - train - Step: 600\tLoss: 0.01673114893026650\n",
      "2022-11-22 16:33:03,280 - INFO - train - Step: 700\tLoss: 0.01641839765943587\n",
      "2022-11-22 16:33:03,793 - INFO - train - Step: 800\tLoss: 0.01617400205694139\n",
      "2022-11-22 16:33:04,306 - INFO - train - Step: 900\tLoss: 0.01639515786431730\n",
      "2022-11-22 16:33:04,848 - INFO - train - Step: 1000\tLoss: 0.01664102992974222\n",
      "2022-11-22 16:33:05,387 - INFO - train - Step: 1100\tLoss: 0.01638773066923022\n",
      "2022-11-22 16:33:05,899 - INFO - train - Step: 1200\tLoss: 0.01642197695560753\n",
      "2022-11-22 16:33:06,411 - INFO - train - Step: 1300\tLoss: 0.01618855998851359\n",
      "2022-11-22 16:33:06,923 - INFO - train - Step: 1400\tLoss: 0.01612825619988143\n",
      "2022-11-22 16:33:07,435 - INFO - train - Step: 1500\tLoss: 0.01643615528009832\n",
      "2022-11-22 16:33:07,948 - INFO - train - Step: 1600\tLoss: 0.01652052465826273\n",
      "2022-11-22 16:33:08,459 - INFO - train - Step: 1700\tLoss: 0.01632255556993186\n",
      "2022-11-22 16:33:08,971 - INFO - train - Step: 1800\tLoss: 0.01642433765344322\n",
      "2022-11-22 16:33:09,202 - INFO - input_processor - Reached the end of the dataset\n",
      "2022-11-22 16:33:09,203 - INFO - train - Done with epoch 1\n",
      "Epoch:  10%|█         | 2/20 [00:20<03:02, 10.15s/it]2022-11-22 16:33:09,204 - INFO - input_processor - Processing training file 1 of 25: ./training/train.bucket11.txt\n",
      "2022-11-22 16:33:09,208 - INFO - input_processor - Done processing training file, batch size is 2364\n",
      "2022-11-22 16:33:09,209 - INFO - input_processor - Processing training file 2 of 25: ./training/train.bucket1.txt\n",
      "2022-11-22 16:33:09,214 - INFO - input_processor - Done processing training file, batch size is 4728\n",
      "2022-11-22 16:33:09,214 - INFO - input_processor - Processing training file 3 of 25: ./training/train.bucket13.txt\n",
      "2022-11-22 16:33:09,220 - INFO - input_processor - Done processing training file, batch size is 7092\n",
      "2022-11-22 16:33:09,220 - INFO - input_processor - Processing training file 4 of 25: ./training/train.bucket24.txt\n",
      "2022-11-22 16:33:09,226 - INFO - input_processor - Done processing training file, batch size is 9456\n",
      "2022-11-22 16:33:09,227 - INFO - input_processor - Processing training file 5 of 25: ./training/train.bucket16.txt\n",
      "2022-11-22 16:33:09,234 - INFO - input_processor - Done processing training file, batch size is 11820\n",
      "2022-11-22 16:33:09,234 - INFO - input_processor - Processing training file 6 of 25: ./training/train.bucket20.txt\n",
      "2022-11-22 16:33:09,242 - INFO - input_processor - Done processing training file, batch size is 14184\n",
      "2022-11-22 16:33:09,243 - INFO - input_processor - Processing training file 7 of 25: ./training/train.bucket23.txt\n",
      "2022-11-22 16:33:09,252 - INFO - input_processor - Done processing training file, batch size is 16548\n",
      "2022-11-22 16:33:09,253 - INFO - input_processor - Processing training file 8 of 25: ./training/train.bucket8.txt\n",
      "2022-11-22 16:33:09,262 - INFO - input_processor - Done processing training file, batch size is 18912\n",
      "2022-11-22 16:33:09,263 - INFO - input_processor - Processing training file 9 of 25: ./training/train.bucket14.txt\n",
      "2022-11-22 16:33:09,275 - INFO - input_processor - Done processing training file, batch size is 21276\n",
      "2022-11-22 16:33:09,275 - INFO - input_processor - Processing training file 10 of 25: ./training/train.bucket21.txt\n",
      "2022-11-22 16:33:09,287 - INFO - input_processor - Done processing training file, batch size is 23640\n",
      "2022-11-22 16:33:09,288 - INFO - input_processor - Processing training file 11 of 25: ./training/train.bucket10.txt\n",
      "2022-11-22 16:33:09,301 - INFO - input_processor - Done processing training file, batch size is 26004\n",
      "2022-11-22 16:33:09,301 - INFO - input_processor - Processing training file 12 of 25: ./training/train.bucket2.txt\n",
      "2022-11-22 16:33:09,315 - INFO - input_processor - Done processing training file, batch size is 28368\n",
      "2022-11-22 16:33:09,315 - INFO - input_processor - Processing training file 13 of 25: ./training/train.bucket7.txt\n",
      "2022-11-22 16:33:09,330 - INFO - input_processor - Done processing training file, batch size is 30732\n",
      "2022-11-22 16:33:09,330 - INFO - input_processor - Processing training file 14 of 25: ./training/train.bucket9.txt\n",
      "2022-11-22 16:33:09,345 - INFO - input_processor - Done processing training file, batch size is 33096\n",
      "2022-11-22 16:33:09,346 - INFO - input_processor - Processing training file 15 of 25: ./training/train.bucket4.txt\n",
      "2022-11-22 16:33:09,362 - INFO - input_processor - Done processing training file, batch size is 35460\n",
      "2022-11-22 16:33:09,362 - INFO - input_processor - Processing training file 16 of 25: ./training/train.bucket18.txt\n",
      "2022-11-22 16:33:09,380 - INFO - input_processor - Done processing training file, batch size is 37824\n",
      "2022-11-22 16:33:09,381 - INFO - input_processor - Processing training file 17 of 25: ./training/train.bucket12.txt\n",
      "2022-11-22 16:33:09,399 - INFO - input_processor - Done processing training file, batch size is 40188\n",
      "2022-11-22 16:33:09,400 - INFO - input_processor - Processing training file 18 of 25: ./training/train.bucket15.txt\n",
      "2022-11-22 16:33:09,419 - INFO - input_processor - Done processing training file, batch size is 42552\n",
      "2022-11-22 16:33:09,420 - INFO - input_processor - Processing training file 19 of 25: ./training/train.bucket0.txt\n",
      "2022-11-22 16:33:09,440 - INFO - input_processor - Done processing training file, batch size is 44916\n",
      "2022-11-22 16:33:09,441 - INFO - input_processor - Processing training file 20 of 25: ./training/train.bucket6.txt\n",
      "2022-11-22 16:33:09,462 - INFO - input_processor - Done processing training file, batch size is 47280\n",
      "2022-11-22 16:33:09,462 - INFO - input_processor - Processing training file 21 of 25: ./training/train.bucket3.txt\n",
      "2022-11-22 16:33:09,484 - INFO - input_processor - Done processing training file, batch size is 49644\n",
      "2022-11-22 16:33:09,485 - INFO - input_processor - Processing training file 22 of 25: ./training/train.bucket17.txt\n",
      "2022-11-22 16:33:09,508 - INFO - input_processor - Done processing training file, batch size is 52008\n",
      "2022-11-22 16:33:09,508 - INFO - input_processor - Processing training file 23 of 25: ./training/train.bucket5.txt\n",
      "2022-11-22 16:33:09,532 - INFO - input_processor - Done processing training file, batch size is 54372\n",
      "2022-11-22 16:33:09,532 - INFO - input_processor - Processing training file 24 of 25: ./training/train.bucket19.txt\n",
      "2022-11-22 16:33:09,557 - INFO - input_processor - Done processing training file, batch size is 56736\n",
      "2022-11-22 16:33:09,557 - INFO - input_processor - Processing training file 25 of 25: ./training/train.bucket22.txt\n",
      "2022-11-22 16:33:09,583 - INFO - input_processor - Done processing training file, batch size is 59100\n",
      "2022-11-22 16:33:10,102 - INFO - train - Step: 100\tLoss: 0.01666228771209717\n",
      "2022-11-22 16:33:10,614 - INFO - train - Step: 200\tLoss: 0.01642010218463838\n",
      "2022-11-22 16:33:11,126 - INFO - train - Step: 300\tLoss: 0.01613651088438928\n",
      "2022-11-22 16:33:11,637 - INFO - train - Step: 400\tLoss: 0.01640353185124695\n",
      "2022-11-22 16:33:12,148 - INFO - train - Step: 500\tLoss: 0.01647009838372469\n",
      "2022-11-22 16:33:12,662 - INFO - train - Step: 600\tLoss: 0.01602071134373546\n",
      "2022-11-22 16:33:13,174 - INFO - train - Step: 700\tLoss: 0.01635097252205014\n",
      "2022-11-22 16:33:13,710 - INFO - train - Step: 800\tLoss: 0.01649278024211526\n",
      "2022-11-22 16:33:14,249 - INFO - train - Step: 900\tLoss: 0.01627503518015146\n",
      "2022-11-22 16:33:14,777 - INFO - train - Step: 1000\tLoss: 0.01615678217262030\n",
      "2022-11-22 16:33:15,290 - INFO - train - Step: 1100\tLoss: 0.01666376634500921\n",
      "2022-11-22 16:33:15,802 - INFO - train - Step: 1200\tLoss: 0.01642964913509786\n",
      "2022-11-22 16:33:16,314 - INFO - train - Step: 1300\tLoss: 0.01622161056846380\n",
      "2022-11-22 16:33:16,826 - INFO - train - Step: 1400\tLoss: 0.01685250364243984\n",
      "2022-11-22 16:33:17,338 - INFO - train - Step: 1500\tLoss: 0.01645609218627215\n",
      "2022-11-22 16:33:17,851 - INFO - train - Step: 1600\tLoss: 0.01628923089243471\n",
      "2022-11-22 16:33:18,363 - INFO - train - Step: 1700\tLoss: 0.01660880706273019\n",
      "2022-11-22 16:33:18,876 - INFO - train - Step: 1800\tLoss: 0.01629085541702807\n",
      "2022-11-22 16:33:19,108 - INFO - input_processor - Reached the end of the dataset\n",
      "2022-11-22 16:33:19,108 - INFO - train - Done with epoch 2\n",
      "Epoch:  15%|█▌        | 3/20 [00:30<02:50, 10.04s/it]2022-11-22 16:33:19,109 - INFO - input_processor - Processing training file 1 of 25: ./training/train.bucket22.txt\n",
      "2022-11-22 16:33:19,113 - INFO - input_processor - Done processing training file, batch size is 2364\n",
      "2022-11-22 16:33:19,114 - INFO - input_processor - Processing training file 2 of 25: ./training/train.bucket1.txt\n",
      "2022-11-22 16:33:19,118 - INFO - input_processor - Done processing training file, batch size is 4728\n",
      "2022-11-22 16:33:19,119 - INFO - input_processor - Processing training file 3 of 25: ./training/train.bucket11.txt\n",
      "2022-11-22 16:33:19,124 - INFO - input_processor - Done processing training file, batch size is 7092\n",
      "2022-11-22 16:33:19,124 - INFO - input_processor - Processing training file 4 of 25: ./training/train.bucket18.txt\n",
      "2022-11-22 16:33:19,131 - INFO - input_processor - Done processing training file, batch size is 9456\n",
      "2022-11-22 16:33:19,132 - INFO - input_processor - Processing training file 5 of 25: ./training/train.bucket10.txt\n",
      "2022-11-22 16:33:19,139 - INFO - input_processor - Done processing training file, batch size is 11820\n",
      "2022-11-22 16:33:19,139 - INFO - input_processor - Processing training file 6 of 25: ./training/train.bucket3.txt\n",
      "2022-11-22 16:33:19,147 - INFO - input_processor - Done processing training file, batch size is 14184\n",
      "2022-11-22 16:33:19,148 - INFO - input_processor - Processing training file 7 of 25: ./training/train.bucket23.txt\n",
      "2022-11-22 16:33:19,157 - INFO - input_processor - Done processing training file, batch size is 16548\n",
      "2022-11-22 16:33:19,157 - INFO - input_processor - Processing training file 8 of 25: ./training/train.bucket16.txt\n",
      "2022-11-22 16:33:19,167 - INFO - input_processor - Done processing training file, batch size is 18912\n",
      "2022-11-22 16:33:19,168 - INFO - input_processor - Processing training file 9 of 25: ./training/train.bucket2.txt\n",
      "2022-11-22 16:33:19,179 - INFO - input_processor - Done processing training file, batch size is 21276\n",
      "2022-11-22 16:33:19,179 - INFO - input_processor - Processing training file 10 of 25: ./training/train.bucket17.txt\n",
      "2022-11-22 16:33:19,192 - INFO - input_processor - Done processing training file, batch size is 23640\n",
      "2022-11-22 16:33:19,192 - INFO - input_processor - Processing training file 11 of 25: ./training/train.bucket9.txt\n",
      "2022-11-22 16:33:19,205 - INFO - input_processor - Done processing training file, batch size is 26004\n",
      "2022-11-22 16:33:19,205 - INFO - input_processor - Processing training file 12 of 25: ./training/train.bucket12.txt\n",
      "2022-11-22 16:33:19,219 - INFO - input_processor - Done processing training file, batch size is 28368\n",
      "2022-11-22 16:33:19,219 - INFO - input_processor - Processing training file 13 of 25: ./training/train.bucket13.txt\n",
      "2022-11-22 16:33:19,234 - INFO - input_processor - Done processing training file, batch size is 30732\n",
      "2022-11-22 16:33:19,234 - INFO - input_processor - Processing training file 14 of 25: ./training/train.bucket5.txt\n",
      "2022-11-22 16:33:19,250 - INFO - input_processor - Done processing training file, batch size is 33096\n",
      "2022-11-22 16:33:19,250 - INFO - input_processor - Processing training file 15 of 25: ./training/train.bucket24.txt\n",
      "2022-11-22 16:33:19,266 - INFO - input_processor - Done processing training file, batch size is 35460\n",
      "2022-11-22 16:33:19,267 - INFO - input_processor - Processing training file 16 of 25: ./training/train.bucket8.txt\n",
      "2022-11-22 16:33:19,284 - INFO - input_processor - Done processing training file, batch size is 37824\n",
      "2022-11-22 16:33:19,285 - INFO - input_processor - Processing training file 17 of 25: ./training/train.bucket21.txt\n",
      "2022-11-22 16:33:19,303 - INFO - input_processor - Done processing training file, batch size is 40188\n",
      "2022-11-22 16:33:19,303 - INFO - input_processor - Processing training file 18 of 25: ./training/train.bucket19.txt\n",
      "2022-11-22 16:33:19,323 - INFO - input_processor - Done processing training file, batch size is 42552\n",
      "2022-11-22 16:33:19,323 - INFO - input_processor - Processing training file 19 of 25: ./training/train.bucket15.txt\n",
      "2022-11-22 16:33:19,344 - INFO - input_processor - Done processing training file, batch size is 44916\n",
      "2022-11-22 16:33:19,344 - INFO - input_processor - Processing training file 20 of 25: ./training/train.bucket4.txt\n",
      "2022-11-22 16:33:19,365 - INFO - input_processor - Done processing training file, batch size is 47280\n",
      "2022-11-22 16:33:19,366 - INFO - input_processor - Processing training file 21 of 25: ./training/train.bucket14.txt\n",
      "2022-11-22 16:33:19,388 - INFO - input_processor - Done processing training file, batch size is 49644\n",
      "2022-11-22 16:33:19,389 - INFO - input_processor - Processing training file 22 of 25: ./training/train.bucket7.txt\n",
      "2022-11-22 16:33:19,412 - INFO - input_processor - Done processing training file, batch size is 52008\n",
      "2022-11-22 16:33:19,412 - INFO - input_processor - Processing training file 23 of 25: ./training/train.bucket0.txt\n",
      "2022-11-22 16:33:19,436 - INFO - input_processor - Done processing training file, batch size is 54372\n",
      "2022-11-22 16:33:19,436 - INFO - input_processor - Processing training file 24 of 25: ./training/train.bucket20.txt\n",
      "2022-11-22 16:33:19,461 - INFO - input_processor - Done processing training file, batch size is 56736\n",
      "2022-11-22 16:33:19,461 - INFO - input_processor - Processing training file 25 of 25: ./training/train.bucket6.txt\n",
      "2022-11-22 16:33:19,487 - INFO - input_processor - Done processing training file, batch size is 59100\n",
      "2022-11-22 16:33:20,022 - INFO - train - Step: 100\tLoss: 0.01655021314509213\n",
      "2022-11-22 16:33:20,535 - INFO - train - Step: 200\tLoss: 0.01636974125169217\n",
      "2022-11-22 16:33:21,047 - INFO - train - Step: 300\tLoss: 0.01640220423229039\n",
      "2022-11-22 16:33:21,559 - INFO - train - Step: 400\tLoss: 0.01611593163572252\n",
      "2022-11-22 16:33:22,071 - INFO - train - Step: 500\tLoss: 0.01640991472639143\n",
      "2022-11-22 16:33:22,583 - INFO - train - Step: 600\tLoss: 0.01643055843189359\n",
      "2022-11-22 16:33:23,096 - INFO - train - Step: 700\tLoss: 0.01637696824967861\n",
      "2022-11-22 16:33:23,607 - INFO - train - Step: 800\tLoss: 0.01650827263481915\n",
      "2022-11-22 16:33:24,119 - INFO - train - Step: 900\tLoss: 0.01657696503214538\n",
      "2022-11-22 16:33:24,631 - INFO - train - Step: 1000\tLoss: 0.01641956129111350\n",
      "2022-11-22 16:33:25,143 - INFO - train - Step: 1100\tLoss: 0.01643368089571595\n",
      "2022-11-22 16:33:25,655 - INFO - train - Step: 1200\tLoss: 0.01651626834645867\n",
      "2022-11-22 16:33:26,167 - INFO - train - Step: 1300\tLoss: 0.01644226127304137\n",
      "2022-11-22 16:33:26,680 - INFO - train - Step: 1400\tLoss: 0.01652893375605345\n",
      "2022-11-22 16:33:27,192 - INFO - train - Step: 1500\tLoss: 0.01636312005110085\n",
      "2022-11-22 16:33:27,705 - INFO - train - Step: 1600\tLoss: 0.01631712270900607\n",
      "2022-11-22 16:33:28,217 - INFO - train - Step: 1700\tLoss: 0.01652751947753131\n",
      "2022-11-22 16:33:28,729 - INFO - train - Step: 1800\tLoss: 0.01637928762473166\n",
      "2022-11-22 16:33:28,960 - INFO - input_processor - Reached the end of the dataset\n",
      "2022-11-22 16:33:28,960 - INFO - train - Done with epoch 3\n",
      "Epoch:  20%|██        | 4/20 [00:40<02:39,  9.96s/it]2022-11-22 16:33:28,961 - INFO - input_processor - Processing training file 1 of 25: ./training/train.bucket20.txt\n",
      "2022-11-22 16:33:28,965 - INFO - input_processor - Done processing training file, batch size is 2364\n",
      "2022-11-22 16:33:28,966 - INFO - input_processor - Processing training file 2 of 25: ./training/train.bucket15.txt\n",
      "2022-11-22 16:33:28,970 - INFO - input_processor - Done processing training file, batch size is 4728\n",
      "2022-11-22 16:33:28,971 - INFO - input_processor - Processing training file 3 of 25: ./training/train.bucket2.txt\n",
      "2022-11-22 16:33:28,976 - INFO - input_processor - Done processing training file, batch size is 7092\n",
      "2022-11-22 16:33:28,976 - INFO - input_processor - Processing training file 4 of 25: ./training/train.bucket6.txt\n",
      "2022-11-22 16:33:28,983 - INFO - input_processor - Done processing training file, batch size is 9456\n",
      "2022-11-22 16:33:28,984 - INFO - input_processor - Processing training file 5 of 25: ./training/train.bucket17.txt\n",
      "2022-11-22 16:33:28,991 - INFO - input_processor - Done processing training file, batch size is 11820\n",
      "2022-11-22 16:33:28,991 - INFO - input_processor - Processing training file 6 of 25: ./training/train.bucket19.txt\n",
      "2022-11-22 16:33:29,000 - INFO - input_processor - Done processing training file, batch size is 14184\n",
      "2022-11-22 16:33:29,000 - INFO - input_processor - Processing training file 7 of 25: ./training/train.bucket12.txt\n",
      "2022-11-22 16:33:29,009 - INFO - input_processor - Done processing training file, batch size is 16548\n",
      "2022-11-22 16:33:29,009 - INFO - input_processor - Processing training file 8 of 25: ./training/train.bucket11.txt\n",
      "2022-11-22 16:33:29,020 - INFO - input_processor - Done processing training file, batch size is 18912\n",
      "2022-11-22 16:33:29,020 - INFO - input_processor - Processing training file 9 of 25: ./training/train.bucket9.txt\n",
      "2022-11-22 16:33:29,031 - INFO - input_processor - Done processing training file, batch size is 21276\n",
      "2022-11-22 16:33:29,031 - INFO - input_processor - Processing training file 10 of 25: ./training/train.bucket22.txt\n",
      "2022-11-22 16:33:29,043 - INFO - input_processor - Done processing training file, batch size is 23640\n",
      "2022-11-22 16:33:29,044 - INFO - input_processor - Processing training file 11 of 25: ./training/train.bucket8.txt\n",
      "2022-11-22 16:33:29,057 - INFO - input_processor - Done processing training file, batch size is 26004\n",
      "2022-11-22 16:33:29,057 - INFO - input_processor - Processing training file 12 of 25: ./training/train.bucket21.txt\n",
      "2022-11-22 16:33:29,070 - INFO - input_processor - Done processing training file, batch size is 28368\n",
      "2022-11-22 16:33:29,071 - INFO - input_processor - Processing training file 13 of 25: ./training/train.bucket3.txt\n",
      "2022-11-22 16:33:29,086 - INFO - input_processor - Done processing training file, batch size is 30732\n",
      "2022-11-22 16:33:29,086 - INFO - input_processor - Processing training file 14 of 25: ./training/train.bucket1.txt\n",
      "2022-11-22 16:33:29,101 - INFO - input_processor - Done processing training file, batch size is 33096\n",
      "2022-11-22 16:33:29,102 - INFO - input_processor - Processing training file 15 of 25: ./training/train.bucket16.txt\n",
      "2022-11-22 16:33:29,118 - INFO - input_processor - Done processing training file, batch size is 35460\n",
      "2022-11-22 16:33:29,119 - INFO - input_processor - Processing training file 16 of 25: ./training/train.bucket13.txt\n",
      "2022-11-22 16:33:29,136 - INFO - input_processor - Done processing training file, batch size is 37824\n",
      "2022-11-22 16:33:29,137 - INFO - input_processor - Processing training file 17 of 25: ./training/train.bucket5.txt\n",
      "2022-11-22 16:33:29,156 - INFO - input_processor - Done processing training file, batch size is 40188\n",
      "2022-11-22 16:33:29,156 - INFO - input_processor - Processing training file 18 of 25: ./training/train.bucket0.txt\n",
      "2022-11-22 16:33:29,176 - INFO - input_processor - Done processing training file, batch size is 42552\n",
      "2022-11-22 16:33:29,176 - INFO - input_processor - Processing training file 19 of 25: ./training/train.bucket23.txt\n",
      "2022-11-22 16:33:29,196 - INFO - input_processor - Done processing training file, batch size is 44916\n",
      "2022-11-22 16:33:29,197 - INFO - input_processor - Processing training file 20 of 25: ./training/train.bucket18.txt\n",
      "2022-11-22 16:33:29,218 - INFO - input_processor - Done processing training file, batch size is 47280\n",
      "2022-11-22 16:33:29,218 - INFO - input_processor - Processing training file 21 of 25: ./training/train.bucket7.txt\n",
      "2022-11-22 16:33:29,241 - INFO - input_processor - Done processing training file, batch size is 49644\n",
      "2022-11-22 16:33:29,241 - INFO - input_processor - Processing training file 22 of 25: ./training/train.bucket10.txt\n",
      "2022-11-22 16:33:29,264 - INFO - input_processor - Done processing training file, batch size is 52008\n",
      "2022-11-22 16:33:29,264 - INFO - input_processor - Processing training file 23 of 25: ./training/train.bucket14.txt\n",
      "2022-11-22 16:33:29,292 - INFO - input_processor - Done processing training file, batch size is 54372\n",
      "2022-11-22 16:33:29,294 - INFO - input_processor - Processing training file 24 of 25: ./training/train.bucket24.txt\n",
      "2022-11-22 16:33:29,319 - INFO - input_processor - Done processing training file, batch size is 56736\n",
      "2022-11-22 16:33:29,320 - INFO - input_processor - Processing training file 25 of 25: ./training/train.bucket4.txt\n",
      "2022-11-22 16:33:29,345 - INFO - input_processor - Done processing training file, batch size is 59100\n",
      "2022-11-22 16:33:29,866 - INFO - train - Step: 100\tLoss: 0.01675770571455359\n",
      "2022-11-22 16:33:30,378 - INFO - train - Step: 200\tLoss: 0.01645863072015345\n",
      "2022-11-22 16:33:30,890 - INFO - train - Step: 300\tLoss: 0.01637141594663262\n",
      "2022-11-22 16:33:31,401 - INFO - train - Step: 400\tLoss: 0.01650678208097816\n",
      "2022-11-22 16:33:31,914 - INFO - train - Step: 500\tLoss: 0.01650172034278512\n",
      "2022-11-22 16:33:32,426 - INFO - train - Step: 600\tLoss: 0.01649155508726835\n",
      "2022-11-22 16:33:32,943 - INFO - train - Step: 700\tLoss: 0.01631903066299856\n",
      "2022-11-22 16:33:33,455 - INFO - train - Step: 800\tLoss: 0.01655233788304031\n",
      "2022-11-22 16:33:33,967 - INFO - train - Step: 900\tLoss: 0.01659524543210864\n",
      "2022-11-22 16:33:34,480 - INFO - train - Step: 1000\tLoss: 0.01673119429498911\n",
      "2022-11-22 16:33:35,005 - INFO - train - Step: 1100\tLoss: 0.01637105066329241\n",
      "2022-11-22 16:33:35,523 - INFO - train - Step: 1200\tLoss: 0.01628902705386281\n",
      "2022-11-22 16:33:36,037 - INFO - train - Step: 1300\tLoss: 0.01619401039555669\n",
      "2022-11-22 16:33:36,550 - INFO - train - Step: 1400\tLoss: 0.01629061924293637\n",
      "2022-11-22 16:33:37,061 - INFO - train - Step: 1500\tLoss: 0.01653536878526211\n",
      "2022-11-22 16:33:37,573 - INFO - train - Step: 1600\tLoss: 0.01612505171447992\n",
      "2022-11-22 16:33:38,085 - INFO - train - Step: 1700\tLoss: 0.01627060957252979\n",
      "2022-11-22 16:33:38,596 - INFO - train - Step: 1800\tLoss: 0.01653670137748122\n",
      "2022-11-22 16:33:38,828 - INFO - input_processor - Reached the end of the dataset\n",
      "2022-11-22 16:33:38,829 - INFO - train - Done with epoch 4\n",
      "Epoch:  25%|██▌       | 5/20 [00:50<02:29,  9.93s/it]2022-11-22 16:33:38,839 - INFO - input_processor - Processing training file 1 of 25: ./training/train.bucket21.txt\n",
      "2022-11-22 16:33:38,844 - INFO - input_processor - Done processing training file, batch size is 2364\n",
      "2022-11-22 16:33:38,844 - INFO - input_processor - Processing training file 2 of 25: ./training/train.bucket16.txt\n",
      "2022-11-22 16:33:38,849 - INFO - input_processor - Done processing training file, batch size is 4728\n",
      "2022-11-22 16:33:38,849 - INFO - input_processor - Processing training file 3 of 25: ./training/train.bucket23.txt\n",
      "2022-11-22 16:33:38,854 - INFO - input_processor - Done processing training file, batch size is 7092\n",
      "2022-11-22 16:33:38,855 - INFO - input_processor - Processing training file 4 of 25: ./training/train.bucket12.txt\n",
      "2022-11-22 16:33:38,861 - INFO - input_processor - Done processing training file, batch size is 9456\n",
      "2022-11-22 16:33:38,862 - INFO - input_processor - Processing training file 5 of 25: ./training/train.bucket20.txt\n",
      "2022-11-22 16:33:38,870 - INFO - input_processor - Done processing training file, batch size is 11820\n",
      "2022-11-22 16:33:38,870 - INFO - input_processor - Processing training file 6 of 25: ./training/train.bucket9.txt\n",
      "2022-11-22 16:33:38,878 - INFO - input_processor - Done processing training file, batch size is 14184\n",
      "2022-11-22 16:33:38,878 - INFO - input_processor - Processing training file 7 of 25: ./training/train.bucket5.txt\n",
      "2022-11-22 16:33:38,887 - INFO - input_processor - Done processing training file, batch size is 16548\n",
      "2022-11-22 16:33:38,888 - INFO - input_processor - Processing training file 8 of 25: ./training/train.bucket11.txt\n",
      "2022-11-22 16:33:38,898 - INFO - input_processor - Done processing training file, batch size is 18912\n",
      "2022-11-22 16:33:38,899 - INFO - input_processor - Processing training file 9 of 25: ./training/train.bucket15.txt\n",
      "2022-11-22 16:33:38,910 - INFO - input_processor - Done processing training file, batch size is 21276\n",
      "2022-11-22 16:33:38,911 - INFO - input_processor - Processing training file 10 of 25: ./training/train.bucket8.txt\n",
      "2022-11-22 16:33:38,923 - INFO - input_processor - Done processing training file, batch size is 23640\n",
      "2022-11-22 16:33:38,923 - INFO - input_processor - Processing training file 11 of 25: ./training/train.bucket2.txt\n",
      "2022-11-22 16:33:38,936 - INFO - input_processor - Done processing training file, batch size is 26004\n",
      "2022-11-22 16:33:38,936 - INFO - input_processor - Processing training file 12 of 25: ./training/train.bucket3.txt\n",
      "2022-11-22 16:33:38,950 - INFO - input_processor - Done processing training file, batch size is 28368\n",
      "2022-11-22 16:33:38,951 - INFO - input_processor - Processing training file 13 of 25: ./training/train.bucket1.txt\n",
      "2022-11-22 16:33:38,965 - INFO - input_processor - Done processing training file, batch size is 30732\n",
      "2022-11-22 16:33:38,966 - INFO - input_processor - Processing training file 14 of 25: ./training/train.bucket4.txt\n",
      "2022-11-22 16:33:38,981 - INFO - input_processor - Done processing training file, batch size is 33096\n",
      "2022-11-22 16:33:38,982 - INFO - input_processor - Processing training file 15 of 25: ./training/train.bucket13.txt\n",
      "2022-11-22 16:33:38,998 - INFO - input_processor - Done processing training file, batch size is 35460\n",
      "2022-11-22 16:33:38,998 - INFO - input_processor - Processing training file 16 of 25: ./training/train.bucket0.txt\n",
      "2022-11-22 16:33:39,016 - INFO - input_processor - Done processing training file, batch size is 37824\n",
      "2022-11-22 16:33:39,016 - INFO - input_processor - Processing training file 17 of 25: ./training/train.bucket17.txt\n",
      "2022-11-22 16:33:39,034 - INFO - input_processor - Done processing training file, batch size is 40188\n",
      "2022-11-22 16:33:39,035 - INFO - input_processor - Processing training file 18 of 25: ./training/train.bucket7.txt\n",
      "2022-11-22 16:33:39,054 - INFO - input_processor - Done processing training file, batch size is 42552\n",
      "2022-11-22 16:33:39,055 - INFO - input_processor - Processing training file 19 of 25: ./training/train.bucket6.txt\n",
      "2022-11-22 16:33:39,075 - INFO - input_processor - Done processing training file, batch size is 44916\n",
      "2022-11-22 16:33:39,076 - INFO - input_processor - Processing training file 20 of 25: ./training/train.bucket19.txt\n",
      "2022-11-22 16:33:39,097 - INFO - input_processor - Done processing training file, batch size is 47280\n",
      "2022-11-22 16:33:39,097 - INFO - input_processor - Processing training file 21 of 25: ./training/train.bucket10.txt\n",
      "2022-11-22 16:33:39,119 - INFO - input_processor - Done processing training file, batch size is 49644\n",
      "2022-11-22 16:33:39,120 - INFO - input_processor - Processing training file 22 of 25: ./training/train.bucket22.txt\n",
      "2022-11-22 16:33:39,143 - INFO - input_processor - Done processing training file, batch size is 52008\n",
      "2022-11-22 16:33:39,143 - INFO - input_processor - Processing training file 23 of 25: ./training/train.bucket24.txt\n",
      "2022-11-22 16:33:39,167 - INFO - input_processor - Done processing training file, batch size is 54372\n",
      "2022-11-22 16:33:39,168 - INFO - input_processor - Processing training file 24 of 25: ./training/train.bucket18.txt\n",
      "2022-11-22 16:33:39,193 - INFO - input_processor - Done processing training file, batch size is 56736\n",
      "2022-11-22 16:33:39,193 - INFO - input_processor - Processing training file 25 of 25: ./training/train.bucket14.txt\n",
      "2022-11-22 16:33:39,219 - INFO - input_processor - Done processing training file, batch size is 59100\n",
      "2022-11-22 16:33:39,736 - INFO - train - Step: 100\tLoss: 0.01657610509544611\n",
      "2022-11-22 16:33:40,246 - INFO - train - Step: 200\tLoss: 0.01600923151709139\n",
      "2022-11-22 16:33:40,757 - INFO - train - Step: 300\tLoss: 0.01673850768245757\n",
      "2022-11-22 16:33:41,269 - INFO - train - Step: 400\tLoss: 0.01665134771727025\n",
      "2022-11-22 16:33:41,781 - INFO - train - Step: 500\tLoss: 0.01664019375108182\n",
      "2022-11-22 16:33:42,294 - INFO - train - Step: 600\tLoss: 0.01636243914254010\n",
      "2022-11-22 16:33:42,817 - INFO - train - Step: 700\tLoss: 0.01662053425796330\n",
      "2022-11-22 16:33:43,329 - INFO - train - Step: 800\tLoss: 0.01637284065596759\n",
      "2022-11-22 16:33:43,840 - INFO - train - Step: 900\tLoss: 0.01634910108521581\n",
      "2022-11-22 16:33:44,352 - INFO - train - Step: 1000\tLoss: 0.01653255004435778\n",
      "2022-11-22 16:33:44,864 - INFO - train - Step: 1100\tLoss: 0.01639111232012510\n",
      "2022-11-22 16:33:45,378 - INFO - train - Step: 1200\tLoss: 0.01661191667430103\n",
      "2022-11-22 16:33:45,890 - INFO - train - Step: 1300\tLoss: 0.01629245233722031\n",
      "2022-11-22 16:33:46,401 - INFO - train - Step: 1400\tLoss: 0.01663854178972542\n",
      "2022-11-22 16:33:46,913 - INFO - train - Step: 1500\tLoss: 0.01641612464562059\n",
      "2022-11-22 16:33:47,424 - INFO - train - Step: 1600\tLoss: 0.01632500840350986\n",
      "2022-11-22 16:33:47,935 - INFO - train - Step: 1700\tLoss: 0.01626663267612457\n",
      "2022-11-22 16:33:48,449 - INFO - train - Step: 1800\tLoss: 0.01662880612537265\n",
      "2022-11-22 16:33:48,680 - INFO - input_processor - Reached the end of the dataset\n",
      "2022-11-22 16:33:48,680 - INFO - train - Done with epoch 5\n",
      "Epoch:  30%|███       | 6/20 [00:59<02:18,  9.90s/it]2022-11-22 16:33:48,681 - INFO - input_processor - Processing training file 1 of 25: ./training/train.bucket23.txt\n",
      "2022-11-22 16:33:48,685 - INFO - input_processor - Done processing training file, batch size is 2364\n",
      "2022-11-22 16:33:48,686 - INFO - input_processor - Processing training file 2 of 25: ./training/train.bucket15.txt\n",
      "2022-11-22 16:33:48,690 - INFO - input_processor - Done processing training file, batch size is 4728\n",
      "2022-11-22 16:33:48,691 - INFO - input_processor - Processing training file 3 of 25: ./training/train.bucket2.txt\n",
      "2022-11-22 16:33:48,696 - INFO - input_processor - Done processing training file, batch size is 7092\n",
      "2022-11-22 16:33:48,696 - INFO - input_processor - Processing training file 4 of 25: ./training/train.bucket9.txt\n",
      "2022-11-22 16:33:48,703 - INFO - input_processor - Done processing training file, batch size is 9456\n",
      "2022-11-22 16:33:48,704 - INFO - input_processor - Processing training file 5 of 25: ./training/train.bucket21.txt\n",
      "2022-11-22 16:33:48,711 - INFO - input_processor - Done processing training file, batch size is 11820\n",
      "2022-11-22 16:33:48,711 - INFO - input_processor - Processing training file 6 of 25: ./training/train.bucket24.txt\n",
      "2022-11-22 16:33:48,719 - INFO - input_processor - Done processing training file, batch size is 14184\n",
      "2022-11-22 16:33:48,720 - INFO - input_processor - Processing training file 7 of 25: ./training/train.bucket5.txt\n",
      "2022-11-22 16:33:48,728 - INFO - input_processor - Done processing training file, batch size is 16548\n",
      "2022-11-22 16:33:48,729 - INFO - input_processor - Processing training file 8 of 25: ./training/train.bucket1.txt\n",
      "2022-11-22 16:33:48,739 - INFO - input_processor - Done processing training file, batch size is 18912\n",
      "2022-11-22 16:33:48,739 - INFO - input_processor - Processing training file 9 of 25: ./training/train.bucket14.txt\n",
      "2022-11-22 16:33:48,750 - INFO - input_processor - Done processing training file, batch size is 21276\n",
      "2022-11-22 16:33:48,751 - INFO - input_processor - Processing training file 10 of 25: ./training/train.bucket18.txt\n",
      "2022-11-22 16:33:48,762 - INFO - input_processor - Done processing training file, batch size is 23640\n",
      "2022-11-22 16:33:48,763 - INFO - input_processor - Processing training file 11 of 25: ./training/train.bucket7.txt\n",
      "2022-11-22 16:33:48,776 - INFO - input_processor - Done processing training file, batch size is 26004\n",
      "2022-11-22 16:33:48,777 - INFO - input_processor - Processing training file 12 of 25: ./training/train.bucket16.txt\n",
      "2022-11-22 16:33:48,790 - INFO - input_processor - Done processing training file, batch size is 28368\n",
      "2022-11-22 16:33:48,791 - INFO - input_processor - Processing training file 13 of 25: ./training/train.bucket0.txt\n",
      "2022-11-22 16:33:48,805 - INFO - input_processor - Done processing training file, batch size is 30732\n",
      "2022-11-22 16:33:48,806 - INFO - input_processor - Processing training file 14 of 25: ./training/train.bucket3.txt\n",
      "2022-11-22 16:33:48,821 - INFO - input_processor - Done processing training file, batch size is 33096\n",
      "2022-11-22 16:33:48,821 - INFO - input_processor - Processing training file 15 of 25: ./training/train.bucket20.txt\n",
      "2022-11-22 16:33:48,838 - INFO - input_processor - Done processing training file, batch size is 35460\n",
      "2022-11-22 16:33:48,839 - INFO - input_processor - Processing training file 16 of 25: ./training/train.bucket17.txt\n",
      "2022-11-22 16:33:48,856 - INFO - input_processor - Done processing training file, batch size is 37824\n",
      "2022-11-22 16:33:48,856 - INFO - input_processor - Processing training file 17 of 25: ./training/train.bucket6.txt\n",
      "2022-11-22 16:33:48,875 - INFO - input_processor - Done processing training file, batch size is 40188\n",
      "2022-11-22 16:33:48,875 - INFO - input_processor - Processing training file 18 of 25: ./training/train.bucket11.txt\n",
      "2022-11-22 16:33:48,894 - INFO - input_processor - Done processing training file, batch size is 42552\n",
      "2022-11-22 16:33:48,895 - INFO - input_processor - Processing training file 19 of 25: ./training/train.bucket13.txt\n",
      "2022-11-22 16:33:48,915 - INFO - input_processor - Done processing training file, batch size is 44916\n",
      "2022-11-22 16:33:48,916 - INFO - input_processor - Processing training file 20 of 25: ./training/train.bucket8.txt\n",
      "2022-11-22 16:33:48,937 - INFO - input_processor - Done processing training file, batch size is 47280\n",
      "2022-11-22 16:33:48,937 - INFO - input_processor - Processing training file 21 of 25: ./training/train.bucket12.txt\n",
      "2022-11-22 16:33:48,960 - INFO - input_processor - Done processing training file, batch size is 49644\n",
      "2022-11-22 16:33:48,960 - INFO - input_processor - Processing training file 22 of 25: ./training/train.bucket22.txt\n",
      "2022-11-22 16:33:48,983 - INFO - input_processor - Done processing training file, batch size is 52008\n",
      "2022-11-22 16:33:48,984 - INFO - input_processor - Processing training file 23 of 25: ./training/train.bucket10.txt\n",
      "2022-11-22 16:33:49,008 - INFO - input_processor - Done processing training file, batch size is 54372\n",
      "2022-11-22 16:33:49,008 - INFO - input_processor - Processing training file 24 of 25: ./training/train.bucket19.txt\n",
      "2022-11-22 16:33:49,032 - INFO - input_processor - Done processing training file, batch size is 56736\n",
      "2022-11-22 16:33:49,033 - INFO - input_processor - Processing training file 25 of 25: ./training/train.bucket4.txt\n",
      "2022-11-22 16:33:49,058 - INFO - input_processor - Done processing training file, batch size is 59100\n",
      "2022-11-22 16:33:49,577 - INFO - train - Step: 100\tLoss: 0.01659533490426839\n",
      "2022-11-22 16:33:50,107 - INFO - train - Step: 200\tLoss: 0.01649240905418992\n",
      "2022-11-22 16:33:50,623 - INFO - train - Step: 300\tLoss: 0.01644357366487384\n",
      "2022-11-22 16:33:51,134 - INFO - train - Step: 400\tLoss: 0.01645659371279180\n",
      "2022-11-22 16:33:51,648 - INFO - train - Step: 500\tLoss: 0.01657846192829311\n",
      "2022-11-22 16:33:52,160 - INFO - train - Step: 600\tLoss: 0.01637555620633066\n",
      "2022-11-22 16:33:52,672 - INFO - train - Step: 700\tLoss: 0.01667628128081560\n",
      "2022-11-22 16:33:53,184 - INFO - train - Step: 800\tLoss: 0.01657225958071649\n",
      "2022-11-22 16:33:53,705 - INFO - train - Step: 900\tLoss: 0.01649473030120134\n",
      "2022-11-22 16:33:54,243 - INFO - train - Step: 1000\tLoss: 0.01662517025135457\n",
      "2022-11-22 16:33:54,775 - INFO - train - Step: 1100\tLoss: 0.01661587209440768\n",
      "2022-11-22 16:33:55,289 - INFO - train - Step: 1200\tLoss: 0.01619740717113018\n",
      "2022-11-22 16:33:55,801 - INFO - train - Step: 1300\tLoss: 0.01644180922769010\n",
      "2022-11-22 16:33:56,313 - INFO - train - Step: 1400\tLoss: 0.01627383654005825\n",
      "2022-11-22 16:33:56,825 - INFO - train - Step: 1500\tLoss: 0.01653912837617099\n",
      "2022-11-22 16:33:57,338 - INFO - train - Step: 1600\tLoss: 0.01621468725614250\n",
      "2022-11-22 16:33:57,850 - INFO - train - Step: 1700\tLoss: 0.01647239116951823\n",
      "2022-11-22 16:33:58,363 - INFO - train - Step: 1800\tLoss: 0.01637191380374134\n",
      "2022-11-22 16:33:58,594 - INFO - input_processor - Reached the end of the dataset\n",
      "2022-11-22 16:33:58,594 - INFO - train - Done with epoch 6\n",
      "Epoch:  35%|███▌      | 7/20 [01:09<02:08,  9.91s/it]2022-11-22 16:33:58,595 - INFO - input_processor - Processing training file 1 of 25: ./training/train.bucket5.txt\n",
      "2022-11-22 16:33:58,599 - INFO - input_processor - Done processing training file, batch size is 2364\n",
      "2022-11-22 16:33:58,600 - INFO - input_processor - Processing training file 2 of 25: ./training/train.bucket23.txt\n",
      "2022-11-22 16:33:58,605 - INFO - input_processor - Done processing training file, batch size is 4728\n",
      "2022-11-22 16:33:58,605 - INFO - input_processor - Processing training file 3 of 25: ./training/train.bucket20.txt\n",
      "2022-11-22 16:33:58,610 - INFO - input_processor - Done processing training file, batch size is 7092\n",
      "2022-11-22 16:33:58,611 - INFO - input_processor - Processing training file 4 of 25: ./training/train.bucket8.txt\n",
      "2022-11-22 16:33:58,617 - INFO - input_processor - Done processing training file, batch size is 9456\n",
      "2022-11-22 16:33:58,618 - INFO - input_processor - Processing training file 5 of 25: ./training/train.bucket6.txt\n",
      "2022-11-22 16:33:58,625 - INFO - input_processor - Done processing training file, batch size is 11820\n",
      "2022-11-22 16:33:58,625 - INFO - input_processor - Processing training file 6 of 25: ./training/train.bucket0.txt\n",
      "2022-11-22 16:33:58,633 - INFO - input_processor - Done processing training file, batch size is 14184\n",
      "2022-11-22 16:33:58,634 - INFO - input_processor - Processing training file 7 of 25: ./training/train.bucket11.txt\n",
      "2022-11-22 16:33:58,643 - INFO - input_processor - Done processing training file, batch size is 16548\n",
      "2022-11-22 16:33:58,643 - INFO - input_processor - Processing training file 8 of 25: ./training/train.bucket2.txt\n",
      "2022-11-22 16:33:58,653 - INFO - input_processor - Done processing training file, batch size is 18912\n",
      "2022-11-22 16:33:58,654 - INFO - input_processor - Processing training file 9 of 25: ./training/train.bucket21.txt\n",
      "2022-11-22 16:33:58,664 - INFO - input_processor - Done processing training file, batch size is 21276\n",
      "2022-11-22 16:33:58,665 - INFO - input_processor - Processing training file 10 of 25: ./training/train.bucket4.txt\n",
      "2022-11-22 16:33:58,677 - INFO - input_processor - Done processing training file, batch size is 23640\n",
      "2022-11-22 16:33:58,677 - INFO - input_processor - Processing training file 11 of 25: ./training/train.bucket13.txt\n",
      "2022-11-22 16:33:58,690 - INFO - input_processor - Done processing training file, batch size is 26004\n",
      "2022-11-22 16:33:58,691 - INFO - input_processor - Processing training file 12 of 25: ./training/train.bucket1.txt\n",
      "2022-11-22 16:33:58,704 - INFO - input_processor - Done processing training file, batch size is 28368\n",
      "2022-11-22 16:33:58,705 - INFO - input_processor - Processing training file 13 of 25: ./training/train.bucket18.txt\n",
      "2022-11-22 16:33:58,719 - INFO - input_processor - Done processing training file, batch size is 30732\n",
      "2022-11-22 16:33:58,720 - INFO - input_processor - Processing training file 14 of 25: ./training/train.bucket15.txt\n",
      "2022-11-22 16:33:58,735 - INFO - input_processor - Done processing training file, batch size is 33096\n",
      "2022-11-22 16:33:58,735 - INFO - input_processor - Processing training file 15 of 25: ./training/train.bucket22.txt\n",
      "2022-11-22 16:33:58,752 - INFO - input_processor - Done processing training file, batch size is 35460\n",
      "2022-11-22 16:33:58,753 - INFO - input_processor - Processing training file 16 of 25: ./training/train.bucket17.txt\n",
      "2022-11-22 16:33:58,770 - INFO - input_processor - Done processing training file, batch size is 37824\n",
      "2022-11-22 16:33:58,770 - INFO - input_processor - Processing training file 17 of 25: ./training/train.bucket7.txt\n",
      "2022-11-22 16:33:58,789 - INFO - input_processor - Done processing training file, batch size is 40188\n",
      "2022-11-22 16:33:58,789 - INFO - input_processor - Processing training file 18 of 25: ./training/train.bucket19.txt\n",
      "2022-11-22 16:33:58,809 - INFO - input_processor - Done processing training file, batch size is 42552\n",
      "2022-11-22 16:33:58,809 - INFO - input_processor - Processing training file 19 of 25: ./training/train.bucket12.txt\n",
      "2022-11-22 16:33:58,830 - INFO - input_processor - Done processing training file, batch size is 44916\n",
      "2022-11-22 16:33:58,830 - INFO - input_processor - Processing training file 20 of 25: ./training/train.bucket9.txt\n",
      "2022-11-22 16:33:58,852 - INFO - input_processor - Done processing training file, batch size is 47280\n",
      "2022-11-22 16:33:58,852 - INFO - input_processor - Processing training file 21 of 25: ./training/train.bucket3.txt\n",
      "2022-11-22 16:33:58,874 - INFO - input_processor - Done processing training file, batch size is 49644\n",
      "2022-11-22 16:33:58,875 - INFO - input_processor - Processing training file 22 of 25: ./training/train.bucket24.txt\n",
      "2022-11-22 16:33:58,898 - INFO - input_processor - Done processing training file, batch size is 52008\n",
      "2022-11-22 16:33:58,899 - INFO - input_processor - Processing training file 23 of 25: ./training/train.bucket14.txt\n",
      "2022-11-22 16:33:58,922 - INFO - input_processor - Done processing training file, batch size is 54372\n",
      "2022-11-22 16:33:58,923 - INFO - input_processor - Processing training file 24 of 25: ./training/train.bucket10.txt\n",
      "2022-11-22 16:33:58,948 - INFO - input_processor - Done processing training file, batch size is 56736\n",
      "2022-11-22 16:33:58,948 - INFO - input_processor - Processing training file 25 of 25: ./training/train.bucket16.txt\n",
      "2022-11-22 16:33:58,974 - INFO - input_processor - Done processing training file, batch size is 59100\n",
      "2022-11-22 16:33:59,492 - INFO - train - Step: 100\tLoss: 0.01689387726597488\n",
      "2022-11-22 16:34:00,005 - INFO - train - Step: 200\tLoss: 0.01659535810351372\n",
      "2022-11-22 16:34:00,555 - INFO - train - Step: 300\tLoss: 0.01635274526663125\n",
      "2022-11-22 16:34:01,078 - INFO - train - Step: 400\tLoss: 0.01634562423452735\n",
      "2022-11-22 16:34:01,590 - INFO - train - Step: 500\tLoss: 0.01641113409772515\n",
      "2022-11-22 16:34:02,103 - INFO - train - Step: 600\tLoss: 0.01632495221681893\n",
      "2022-11-22 16:34:02,615 - INFO - train - Step: 700\tLoss: 0.01621518805623055\n",
      "2022-11-22 16:34:03,127 - INFO - train - Step: 800\tLoss: 0.01630125082097948\n",
      "2022-11-22 16:34:03,639 - INFO - train - Step: 900\tLoss: 0.01644111438654363\n",
      "2022-11-22 16:34:04,152 - INFO - train - Step: 1000\tLoss: 0.01638543698936701\n",
      "2022-11-22 16:34:04,669 - INFO - train - Step: 1100\tLoss: 0.01646064454689622\n",
      "2022-11-22 16:34:05,187 - INFO - train - Step: 1200\tLoss: 0.01665563463233411\n",
      "2022-11-22 16:34:05,700 - INFO - train - Step: 1300\tLoss: 0.01610588735900819\n",
      "2022-11-22 16:34:06,211 - INFO - train - Step: 1400\tLoss: 0.01647443843074143\n",
      "2022-11-22 16:34:06,723 - INFO - train - Step: 1500\tLoss: 0.01625506933778524\n",
      "2022-11-22 16:34:07,236 - INFO - train - Step: 1600\tLoss: 0.01641904802061617\n",
      "2022-11-22 16:34:07,749 - INFO - train - Step: 1700\tLoss: 0.01630634340457618\n",
      "2022-11-22 16:34:08,260 - INFO - train - Step: 1800\tLoss: 0.01638991124927998\n",
      "2022-11-22 16:34:08,492 - INFO - input_processor - Reached the end of the dataset\n",
      "2022-11-22 16:34:08,492 - INFO - train - Done with epoch 7\n",
      "Epoch:  40%|████      | 8/20 [01:19<01:58,  9.90s/it]2022-11-22 16:34:08,493 - INFO - input_processor - Processing training file 1 of 25: ./training/train.bucket24.txt\n",
      "2022-11-22 16:34:08,497 - INFO - input_processor - Done processing training file, batch size is 2364\n",
      "2022-11-22 16:34:08,498 - INFO - input_processor - Processing training file 2 of 25: ./training/train.bucket16.txt\n",
      "2022-11-22 16:34:08,502 - INFO - input_processor - Done processing training file, batch size is 4728\n",
      "2022-11-22 16:34:08,503 - INFO - input_processor - Processing training file 3 of 25: ./training/train.bucket9.txt\n",
      "2022-11-22 16:34:08,508 - INFO - input_processor - Done processing training file, batch size is 7092\n",
      "2022-11-22 16:34:08,508 - INFO - input_processor - Processing training file 4 of 25: ./training/train.bucket7.txt\n",
      "2022-11-22 16:34:08,516 - INFO - input_processor - Done processing training file, batch size is 9456\n",
      "2022-11-22 16:34:08,516 - INFO - input_processor - Processing training file 5 of 25: ./training/train.bucket3.txt\n",
      "2022-11-22 16:34:08,523 - INFO - input_processor - Done processing training file, batch size is 11820\n",
      "2022-11-22 16:34:08,524 - INFO - input_processor - Processing training file 6 of 25: ./training/train.bucket21.txt\n",
      "2022-11-22 16:34:08,532 - INFO - input_processor - Done processing training file, batch size is 14184\n",
      "2022-11-22 16:34:08,533 - INFO - input_processor - Processing training file 7 of 25: ./training/train.bucket17.txt\n",
      "2022-11-22 16:34:08,542 - INFO - input_processor - Done processing training file, batch size is 16548\n",
      "2022-11-22 16:34:08,542 - INFO - input_processor - Processing training file 8 of 25: ./training/train.bucket14.txt\n",
      "2022-11-22 16:34:08,552 - INFO - input_processor - Done processing training file, batch size is 18912\n",
      "2022-11-22 16:34:08,553 - INFO - input_processor - Processing training file 9 of 25: ./training/train.bucket13.txt\n",
      "2022-11-22 16:34:08,564 - INFO - input_processor - Done processing training file, batch size is 21276\n",
      "2022-11-22 16:34:08,565 - INFO - input_processor - Processing training file 10 of 25: ./training/train.bucket20.txt\n",
      "2022-11-22 16:34:08,577 - INFO - input_processor - Done processing training file, batch size is 23640\n",
      "2022-11-22 16:34:08,577 - INFO - input_processor - Processing training file 11 of 25: ./training/train.bucket0.txt\n",
      "2022-11-22 16:34:08,590 - INFO - input_processor - Done processing training file, batch size is 26004\n",
      "2022-11-22 16:34:08,591 - INFO - input_processor - Processing training file 12 of 25: ./training/train.bucket5.txt\n",
      "2022-11-22 16:34:08,605 - INFO - input_processor - Done processing training file, batch size is 28368\n",
      "2022-11-22 16:34:08,605 - INFO - input_processor - Processing training file 13 of 25: ./training/train.bucket2.txt\n",
      "2022-11-22 16:34:08,619 - INFO - input_processor - Done processing training file, batch size is 30732\n",
      "2022-11-22 16:34:08,620 - INFO - input_processor - Processing training file 14 of 25: ./training/train.bucket15.txt\n",
      "2022-11-22 16:34:08,635 - INFO - input_processor - Done processing training file, batch size is 33096\n",
      "2022-11-22 16:34:08,636 - INFO - input_processor - Processing training file 15 of 25: ./training/train.bucket19.txt\n",
      "2022-11-22 16:34:08,652 - INFO - input_processor - Done processing training file, batch size is 35460\n",
      "2022-11-22 16:34:08,653 - INFO - input_processor - Processing training file 16 of 25: ./training/train.bucket6.txt\n",
      "2022-11-22 16:34:08,670 - INFO - input_processor - Done processing training file, batch size is 37824\n",
      "2022-11-22 16:34:08,670 - INFO - input_processor - Processing training file 17 of 25: ./training/train.bucket8.txt\n",
      "2022-11-22 16:34:08,689 - INFO - input_processor - Done processing training file, batch size is 40188\n",
      "2022-11-22 16:34:08,690 - INFO - input_processor - Processing training file 18 of 25: ./training/train.bucket10.txt\n",
      "2022-11-22 16:34:08,709 - INFO - input_processor - Done processing training file, batch size is 42552\n",
      "2022-11-22 16:34:08,710 - INFO - input_processor - Processing training file 19 of 25: ./training/train.bucket11.txt\n",
      "2022-11-22 16:34:08,730 - INFO - input_processor - Done processing training file, batch size is 44916\n",
      "2022-11-22 16:34:08,731 - INFO - input_processor - Processing training file 20 of 25: ./training/train.bucket18.txt\n",
      "2022-11-22 16:34:08,752 - INFO - input_processor - Done processing training file, batch size is 47280\n",
      "2022-11-22 16:34:08,752 - INFO - input_processor - Processing training file 21 of 25: ./training/train.bucket4.txt\n",
      "2022-11-22 16:34:08,774 - INFO - input_processor - Done processing training file, batch size is 49644\n",
      "2022-11-22 16:34:08,775 - INFO - input_processor - Processing training file 22 of 25: ./training/train.bucket1.txt\n",
      "2022-11-22 16:34:08,798 - INFO - input_processor - Done processing training file, batch size is 52008\n",
      "2022-11-22 16:34:08,799 - INFO - input_processor - Processing training file 23 of 25: ./training/train.bucket23.txt\n",
      "2022-11-22 16:34:08,823 - INFO - input_processor - Done processing training file, batch size is 54372\n",
      "2022-11-22 16:34:08,823 - INFO - input_processor - Processing training file 24 of 25: ./training/train.bucket12.txt\n",
      "2022-11-22 16:34:08,848 - INFO - input_processor - Done processing training file, batch size is 56736\n",
      "2022-11-22 16:34:08,848 - INFO - input_processor - Processing training file 25 of 25: ./training/train.bucket22.txt\n",
      "2022-11-22 16:34:08,874 - INFO - input_processor - Done processing training file, batch size is 59100\n",
      "2022-11-22 16:34:09,392 - INFO - train - Step: 100\tLoss: 0.01652141514234245\n",
      "2022-11-22 16:34:09,905 - INFO - train - Step: 200\tLoss: 0.01645272483117878\n",
      "2022-11-22 16:34:10,417 - INFO - train - Step: 300\tLoss: 0.01653119419701397\n",
      "2022-11-22 16:34:10,930 - INFO - train - Step: 400\tLoss: 0.01632990140467882\n",
      "2022-11-22 16:34:11,442 - INFO - train - Step: 500\tLoss: 0.01634338549338281\n",
      "2022-11-22 16:34:11,954 - INFO - train - Step: 600\tLoss: 0.01614165389910340\n",
      "2022-11-22 16:34:12,466 - INFO - train - Step: 700\tLoss: 0.01622821527533233\n",
      "2022-11-22 16:34:12,980 - INFO - train - Step: 800\tLoss: 0.01636395038105547\n",
      "2022-11-22 16:34:13,491 - INFO - train - Step: 900\tLoss: 0.01651991378515959\n",
      "2022-11-22 16:34:14,003 - INFO - train - Step: 1000\tLoss: 0.01615949283353984\n",
      "2022-11-22 16:34:14,514 - INFO - train - Step: 1100\tLoss: 0.01626798765733838\n",
      "2022-11-22 16:34:15,026 - INFO - train - Step: 1200\tLoss: 0.01629500296898186\n",
      "2022-11-22 16:34:15,538 - INFO - train - Step: 1300\tLoss: 0.01666180097498000\n",
      "2022-11-22 16:34:16,050 - INFO - train - Step: 1400\tLoss: 0.01674808501265943\n",
      "2022-11-22 16:34:16,561 - INFO - train - Step: 1500\tLoss: 0.01659853002056479\n",
      "2022-11-22 16:34:17,072 - INFO - train - Step: 1600\tLoss: 0.01648078415542841\n",
      "2022-11-22 16:34:17,584 - INFO - train - Step: 1700\tLoss: 0.01662572186440230\n",
      "2022-11-22 16:34:18,095 - INFO - train - Step: 1800\tLoss: 0.01655881566926837\n",
      "2022-11-22 16:34:18,326 - INFO - input_processor - Reached the end of the dataset\n",
      "2022-11-22 16:34:18,326 - INFO - train - Done with epoch 8\n",
      "Epoch:  45%|████▌     | 9/20 [01:29<01:48,  9.88s/it]2022-11-22 16:34:18,327 - INFO - input_processor - Processing training file 1 of 25: ./training/train.bucket16.txt\n",
      "2022-11-22 16:34:18,331 - INFO - input_processor - Done processing training file, batch size is 2364\n",
      "2022-11-22 16:34:18,332 - INFO - input_processor - Processing training file 2 of 25: ./training/train.bucket21.txt\n",
      "2022-11-22 16:34:18,336 - INFO - input_processor - Done processing training file, batch size is 4728\n",
      "2022-11-22 16:34:18,337 - INFO - input_processor - Processing training file 3 of 25: ./training/train.bucket9.txt\n",
      "2022-11-22 16:34:18,342 - INFO - input_processor - Done processing training file, batch size is 7092\n",
      "2022-11-22 16:34:18,343 - INFO - input_processor - Processing training file 4 of 25: ./training/train.bucket2.txt\n",
      "2022-11-22 16:34:18,349 - INFO - input_processor - Done processing training file, batch size is 9456\n",
      "2022-11-22 16:34:18,349 - INFO - input_processor - Processing training file 5 of 25: ./training/train.bucket6.txt\n",
      "2022-11-22 16:34:18,357 - INFO - input_processor - Done processing training file, batch size is 11820\n",
      "2022-11-22 16:34:18,357 - INFO - input_processor - Processing training file 6 of 25: ./training/train.bucket22.txt\n",
      "2022-11-22 16:34:18,365 - INFO - input_processor - Done processing training file, batch size is 14184\n",
      "2022-11-22 16:34:18,365 - INFO - input_processor - Processing training file 7 of 25: ./training/train.bucket3.txt\n",
      "2022-11-22 16:34:18,374 - INFO - input_processor - Done processing training file, batch size is 16548\n",
      "2022-11-22 16:34:18,375 - INFO - input_processor - Processing training file 8 of 25: ./training/train.bucket14.txt\n",
      "2022-11-22 16:34:18,385 - INFO - input_processor - Done processing training file, batch size is 18912\n",
      "2022-11-22 16:34:18,385 - INFO - input_processor - Processing training file 9 of 25: ./training/train.bucket12.txt\n",
      "2022-11-22 16:34:18,396 - INFO - input_processor - Done processing training file, batch size is 21276\n",
      "2022-11-22 16:34:18,396 - INFO - input_processor - Processing training file 10 of 25: ./training/train.bucket7.txt\n",
      "2022-11-22 16:34:18,409 - INFO - input_processor - Done processing training file, batch size is 23640\n",
      "2022-11-22 16:34:18,410 - INFO - input_processor - Processing training file 11 of 25: ./training/train.bucket4.txt\n",
      "2022-11-22 16:34:18,422 - INFO - input_processor - Done processing training file, batch size is 26004\n",
      "2022-11-22 16:34:18,423 - INFO - input_processor - Processing training file 12 of 25: ./training/train.bucket15.txt\n",
      "2022-11-22 16:34:18,436 - INFO - input_processor - Done processing training file, batch size is 28368\n",
      "2022-11-22 16:34:18,437 - INFO - input_processor - Processing training file 13 of 25: ./training/train.bucket18.txt\n",
      "2022-11-22 16:34:18,452 - INFO - input_processor - Done processing training file, batch size is 30732\n",
      "2022-11-22 16:34:18,452 - INFO - input_processor - Processing training file 14 of 25: ./training/train.bucket20.txt\n",
      "2022-11-22 16:34:18,468 - INFO - input_processor - Done processing training file, batch size is 33096\n",
      "2022-11-22 16:34:18,468 - INFO - input_processor - Processing training file 15 of 25: ./training/train.bucket1.txt\n",
      "2022-11-22 16:34:18,484 - INFO - input_processor - Done processing training file, batch size is 35460\n",
      "2022-11-22 16:34:18,485 - INFO - input_processor - Processing training file 16 of 25: ./training/train.bucket19.txt\n",
      "2022-11-22 16:34:18,502 - INFO - input_processor - Done processing training file, batch size is 37824\n",
      "2022-11-22 16:34:18,503 - INFO - input_processor - Processing training file 17 of 25: ./training/train.bucket13.txt\n",
      "2022-11-22 16:34:18,521 - INFO - input_processor - Done processing training file, batch size is 40188\n",
      "2022-11-22 16:34:18,521 - INFO - input_processor - Processing training file 18 of 25: ./training/train.bucket24.txt\n",
      "2022-11-22 16:34:18,541 - INFO - input_processor - Done processing training file, batch size is 42552\n",
      "2022-11-22 16:34:18,542 - INFO - input_processor - Processing training file 19 of 25: ./training/train.bucket0.txt\n",
      "2022-11-22 16:34:18,562 - INFO - input_processor - Done processing training file, batch size is 44916\n",
      "2022-11-22 16:34:18,563 - INFO - input_processor - Processing training file 20 of 25: ./training/train.bucket17.txt\n",
      "2022-11-22 16:34:18,585 - INFO - input_processor - Done processing training file, batch size is 47280\n",
      "2022-11-22 16:34:18,586 - INFO - input_processor - Processing training file 21 of 25: ./training/train.bucket23.txt\n",
      "2022-11-22 16:34:18,609 - INFO - input_processor - Done processing training file, batch size is 49644\n",
      "2022-11-22 16:34:18,609 - INFO - input_processor - Processing training file 22 of 25: ./training/train.bucket10.txt\n",
      "2022-11-22 16:34:18,633 - INFO - input_processor - Done processing training file, batch size is 52008\n",
      "2022-11-22 16:34:18,634 - INFO - input_processor - Processing training file 23 of 25: ./training/train.bucket11.txt\n",
      "2022-11-22 16:34:18,659 - INFO - input_processor - Done processing training file, batch size is 54372\n",
      "2022-11-22 16:34:18,659 - INFO - input_processor - Processing training file 24 of 25: ./training/train.bucket5.txt\n",
      "2022-11-22 16:34:18,685 - INFO - input_processor - Done processing training file, batch size is 56736\n",
      "2022-11-22 16:34:18,686 - INFO - input_processor - Processing training file 25 of 25: ./training/train.bucket8.txt\n",
      "2022-11-22 16:34:18,713 - INFO - input_processor - Done processing training file, batch size is 59100\n",
      "2022-11-22 16:34:19,239 - INFO - train - Step: 100\tLoss: 0.01663196680136025\n",
      "2022-11-22 16:34:19,764 - INFO - train - Step: 200\tLoss: 0.01650591730140150\n",
      "2022-11-22 16:34:20,312 - INFO - train - Step: 300\tLoss: 0.01659077766351402\n",
      "2022-11-22 16:34:20,837 - INFO - train - Step: 400\tLoss: 0.01643360889516771\n",
      "2022-11-22 16:34:21,349 - INFO - train - Step: 500\tLoss: 0.01651862231083214\n",
      "2022-11-22 16:34:21,860 - INFO - train - Step: 600\tLoss: 0.01642015495337546\n",
      "2022-11-22 16:34:22,372 - INFO - train - Step: 700\tLoss: 0.01638415489345789\n",
      "2022-11-22 16:34:22,885 - INFO - train - Step: 800\tLoss: 0.01655278998427093\n",
      "2022-11-22 16:34:23,398 - INFO - train - Step: 900\tLoss: 0.01619432996027172\n",
      "2022-11-22 16:34:23,910 - INFO - train - Step: 1000\tLoss: 0.01634015505202115\n",
      "2022-11-22 16:34:24,421 - INFO - train - Step: 1100\tLoss: 0.01694670462980866\n",
      "2022-11-22 16:34:24,933 - INFO - train - Step: 1200\tLoss: 0.01634728181175888\n",
      "2022-11-22 16:34:25,445 - INFO - train - Step: 1300\tLoss: 0.01634050616063178\n",
      "2022-11-22 16:34:25,958 - INFO - train - Step: 1400\tLoss: 0.01641254401765764\n",
      "2022-11-22 16:34:26,475 - INFO - train - Step: 1500\tLoss: 0.01653589129447937\n",
      "2022-11-22 16:34:26,988 - INFO - train - Step: 1600\tLoss: 0.01642689269036055\n",
      "2022-11-22 16:34:27,501 - INFO - train - Step: 1700\tLoss: 0.01642433119937778\n",
      "2022-11-22 16:34:28,013 - INFO - train - Step: 1800\tLoss: 0.01617423827759921\n",
      "2022-11-22 16:34:28,244 - INFO - input_processor - Reached the end of the dataset\n",
      "2022-11-22 16:34:28,245 - INFO - train - Done with epoch 9\n",
      "Epoch:  50%|█████     | 10/20 [01:39<01:38,  9.90s/it]2022-11-22 16:34:28,253 - INFO - input_processor - Processing training file 1 of 25: ./training/train.bucket13.txt\n",
      "2022-11-22 16:34:28,257 - INFO - input_processor - Done processing training file, batch size is 2364\n",
      "2022-11-22 16:34:28,258 - INFO - input_processor - Processing training file 2 of 25: ./training/train.bucket21.txt\n",
      "2022-11-22 16:34:28,262 - INFO - input_processor - Done processing training file, batch size is 4728\n",
      "2022-11-22 16:34:28,263 - INFO - input_processor - Processing training file 3 of 25: ./training/train.bucket8.txt\n",
      "2022-11-22 16:34:28,269 - INFO - input_processor - Done processing training file, batch size is 7092\n",
      "2022-11-22 16:34:28,269 - INFO - input_processor - Processing training file 4 of 25: ./training/train.bucket6.txt\n",
      "2022-11-22 16:34:28,276 - INFO - input_processor - Done processing training file, batch size is 9456\n",
      "2022-11-22 16:34:28,276 - INFO - input_processor - Processing training file 5 of 25: ./training/train.bucket14.txt\n",
      "2022-11-22 16:34:28,283 - INFO - input_processor - Done processing training file, batch size is 11820\n",
      "2022-11-22 16:34:28,284 - INFO - input_processor - Processing training file 6 of 25: ./training/train.bucket4.txt\n",
      "2022-11-22 16:34:28,292 - INFO - input_processor - Done processing training file, batch size is 14184\n",
      "2022-11-22 16:34:28,293 - INFO - input_processor - Processing training file 7 of 25: ./training/train.bucket23.txt\n",
      "2022-11-22 16:34:28,302 - INFO - input_processor - Done processing training file, batch size is 16548\n",
      "2022-11-22 16:34:28,302 - INFO - input_processor - Processing training file 8 of 25: ./training/train.bucket7.txt\n",
      "2022-11-22 16:34:28,313 - INFO - input_processor - Done processing training file, batch size is 18912\n",
      "2022-11-22 16:34:28,313 - INFO - input_processor - Processing training file 9 of 25: ./training/train.bucket20.txt\n",
      "2022-11-22 16:34:28,324 - INFO - input_processor - Done processing training file, batch size is 21276\n",
      "2022-11-22 16:34:28,325 - INFO - input_processor - Processing training file 10 of 25: ./training/train.bucket16.txt\n",
      "2022-11-22 16:34:28,337 - INFO - input_processor - Done processing training file, batch size is 23640\n",
      "2022-11-22 16:34:28,337 - INFO - input_processor - Processing training file 11 of 25: ./training/train.bucket9.txt\n",
      "2022-11-22 16:34:28,349 - INFO - input_processor - Done processing training file, batch size is 26004\n",
      "2022-11-22 16:34:28,350 - INFO - input_processor - Processing training file 12 of 25: ./training/train.bucket1.txt\n",
      "2022-11-22 16:34:28,363 - INFO - input_processor - Done processing training file, batch size is 28368\n",
      "2022-11-22 16:34:28,364 - INFO - input_processor - Processing training file 13 of 25: ./training/train.bucket11.txt\n",
      "2022-11-22 16:34:28,379 - INFO - input_processor - Done processing training file, batch size is 30732\n",
      "2022-11-22 16:34:28,379 - INFO - input_processor - Processing training file 14 of 25: ./training/train.bucket15.txt\n",
      "2022-11-22 16:34:28,395 - INFO - input_processor - Done processing training file, batch size is 33096\n",
      "2022-11-22 16:34:28,395 - INFO - input_processor - Processing training file 15 of 25: ./training/train.bucket5.txt\n",
      "2022-11-22 16:34:28,412 - INFO - input_processor - Done processing training file, batch size is 35460\n",
      "2022-11-22 16:34:28,412 - INFO - input_processor - Processing training file 16 of 25: ./training/train.bucket17.txt\n",
      "2022-11-22 16:34:28,432 - INFO - input_processor - Done processing training file, batch size is 37824\n",
      "2022-11-22 16:34:28,432 - INFO - input_processor - Processing training file 17 of 25: ./training/train.bucket10.txt\n",
      "2022-11-22 16:34:28,451 - INFO - input_processor - Done processing training file, batch size is 40188\n",
      "2022-11-22 16:34:28,451 - INFO - input_processor - Processing training file 18 of 25: ./training/train.bucket24.txt\n",
      "2022-11-22 16:34:28,470 - INFO - input_processor - Done processing training file, batch size is 42552\n",
      "2022-11-22 16:34:28,470 - INFO - input_processor - Processing training file 19 of 25: ./training/train.bucket2.txt\n",
      "2022-11-22 16:34:28,490 - INFO - input_processor - Done processing training file, batch size is 44916\n",
      "2022-11-22 16:34:28,491 - INFO - input_processor - Processing training file 20 of 25: ./training/train.bucket12.txt\n",
      "2022-11-22 16:34:28,513 - INFO - input_processor - Done processing training file, batch size is 47280\n",
      "2022-11-22 16:34:28,513 - INFO - input_processor - Processing training file 21 of 25: ./training/train.bucket18.txt\n",
      "2022-11-22 16:34:28,535 - INFO - input_processor - Done processing training file, batch size is 49644\n",
      "2022-11-22 16:34:28,535 - INFO - input_processor - Processing training file 22 of 25: ./training/train.bucket19.txt\n",
      "2022-11-22 16:34:28,559 - INFO - input_processor - Done processing training file, batch size is 52008\n",
      "2022-11-22 16:34:28,559 - INFO - input_processor - Processing training file 23 of 25: ./training/train.bucket3.txt\n",
      "2022-11-22 16:34:28,584 - INFO - input_processor - Done processing training file, batch size is 54372\n",
      "2022-11-22 16:34:28,584 - INFO - input_processor - Processing training file 24 of 25: ./training/train.bucket0.txt\n",
      "2022-11-22 16:34:28,608 - INFO - input_processor - Done processing training file, batch size is 56736\n",
      "2022-11-22 16:34:28,609 - INFO - input_processor - Processing training file 25 of 25: ./training/train.bucket22.txt\n",
      "2022-11-22 16:34:28,634 - INFO - input_processor - Done processing training file, batch size is 59100\n",
      "2022-11-22 16:34:29,154 - INFO - train - Step: 100\tLoss: 0.01663130435161293\n",
      "2022-11-22 16:34:29,697 - INFO - train - Step: 200\tLoss: 0.01657918018288911\n",
      "2022-11-22 16:34:30,227 - INFO - train - Step: 300\tLoss: 0.01641499263234436\n",
      "2022-11-22 16:34:30,756 - INFO - train - Step: 400\tLoss: 0.01631156776100397\n",
      "2022-11-22 16:34:31,274 - INFO - train - Step: 500\tLoss: 0.01634819613769651\n",
      "2022-11-22 16:34:31,811 - INFO - train - Step: 600\tLoss: 0.01645045164972544\n",
      "2022-11-22 16:34:32,349 - INFO - train - Step: 700\tLoss: 0.01655924691818654\n",
      "2022-11-22 16:34:32,874 - INFO - train - Step: 800\tLoss: 0.01616660595871508\n",
      "2022-11-22 16:34:33,411 - INFO - train - Step: 900\tLoss: 0.01637368531897664\n",
      "2022-11-22 16:34:33,931 - INFO - train - Step: 1000\tLoss: 0.01646350561641157\n",
      "2022-11-22 16:34:34,453 - INFO - train - Step: 1100\tLoss: 0.01656751366332173\n",
      "2022-11-22 16:34:34,974 - INFO - train - Step: 1200\tLoss: 0.01622407321818173\n",
      "2022-11-22 16:34:35,495 - INFO - train - Step: 1300\tLoss: 0.01655997108668089\n",
      "2022-11-22 16:34:36,015 - INFO - train - Step: 1400\tLoss: 0.01642422222532332\n",
      "2022-11-22 16:34:36,533 - INFO - train - Step: 1500\tLoss: 0.01613372730091214\n",
      "2022-11-22 16:34:37,051 - INFO - train - Step: 1600\tLoss: 0.01653633560985327\n",
      "2022-11-22 16:34:37,570 - INFO - train - Step: 1700\tLoss: 0.01676138919778168\n",
      "2022-11-22 16:34:38,088 - INFO - train - Step: 1800\tLoss: 0.01629730723798275\n",
      "2022-11-22 16:34:38,322 - INFO - input_processor - Reached the end of the dataset\n",
      "2022-11-22 16:34:38,323 - INFO - train - Done with epoch 10\n",
      "Epoch:  55%|█████▌    | 11/20 [01:49<01:29,  9.95s/it]2022-11-22 16:34:38,324 - INFO - input_processor - Processing training file 1 of 25: ./training/train.bucket1.txt\n",
      "2022-11-22 16:34:38,328 - INFO - input_processor - Done processing training file, batch size is 2364\n",
      "2022-11-22 16:34:38,328 - INFO - input_processor - Processing training file 2 of 25: ./training/train.bucket24.txt\n",
      "2022-11-22 16:34:38,333 - INFO - input_processor - Done processing training file, batch size is 4728\n",
      "2022-11-22 16:34:38,333 - INFO - input_processor - Processing training file 3 of 25: ./training/train.bucket20.txt\n",
      "2022-11-22 16:34:38,339 - INFO - input_processor - Done processing training file, batch size is 7092\n",
      "2022-11-22 16:34:38,339 - INFO - input_processor - Processing training file 4 of 25: ./training/train.bucket23.txt\n",
      "2022-11-22 16:34:38,346 - INFO - input_processor - Done processing training file, batch size is 9456\n",
      "2022-11-22 16:34:38,346 - INFO - input_processor - Processing training file 5 of 25: ./training/train.bucket13.txt\n",
      "2022-11-22 16:34:38,353 - INFO - input_processor - Done processing training file, batch size is 11820\n",
      "2022-11-22 16:34:38,354 - INFO - input_processor - Processing training file 6 of 25: ./training/train.bucket15.txt\n",
      "2022-11-22 16:34:38,362 - INFO - input_processor - Done processing training file, batch size is 14184\n",
      "2022-11-22 16:34:38,362 - INFO - input_processor - Processing training file 7 of 25: ./training/train.bucket22.txt\n",
      "2022-11-22 16:34:38,372 - INFO - input_processor - Done processing training file, batch size is 16548\n",
      "2022-11-22 16:34:38,372 - INFO - input_processor - Processing training file 8 of 25: ./training/train.bucket7.txt\n",
      "2022-11-22 16:34:38,382 - INFO - input_processor - Done processing training file, batch size is 18912\n",
      "2022-11-22 16:34:38,383 - INFO - input_processor - Processing training file 9 of 25: ./training/train.bucket14.txt\n",
      "2022-11-22 16:34:38,394 - INFO - input_processor - Done processing training file, batch size is 21276\n",
      "2022-11-22 16:34:38,396 - INFO - input_processor - Processing training file 10 of 25: ./training/train.bucket4.txt\n",
      "2022-11-22 16:34:38,408 - INFO - input_processor - Done processing training file, batch size is 23640\n",
      "2022-11-22 16:34:38,408 - INFO - input_processor - Processing training file 11 of 25: ./training/train.bucket2.txt\n",
      "2022-11-22 16:34:38,421 - INFO - input_processor - Done processing training file, batch size is 26004\n",
      "2022-11-22 16:34:38,422 - INFO - input_processor - Processing training file 12 of 25: ./training/train.bucket21.txt\n",
      "2022-11-22 16:34:38,436 - INFO - input_processor - Done processing training file, batch size is 28368\n",
      "2022-11-22 16:34:38,436 - INFO - input_processor - Processing training file 13 of 25: ./training/train.bucket10.txt\n",
      "2022-11-22 16:34:38,451 - INFO - input_processor - Done processing training file, batch size is 30732\n",
      "2022-11-22 16:34:38,451 - INFO - input_processor - Processing training file 14 of 25: ./training/train.bucket11.txt\n",
      "2022-11-22 16:34:38,467 - INFO - input_processor - Done processing training file, batch size is 33096\n",
      "2022-11-22 16:34:38,468 - INFO - input_processor - Processing training file 15 of 25: ./training/train.bucket3.txt\n",
      "2022-11-22 16:34:38,485 - INFO - input_processor - Done processing training file, batch size is 35460\n",
      "2022-11-22 16:34:38,485 - INFO - input_processor - Processing training file 16 of 25: ./training/train.bucket5.txt\n",
      "2022-11-22 16:34:38,503 - INFO - input_processor - Done processing training file, batch size is 37824\n",
      "2022-11-22 16:34:38,503 - INFO - input_processor - Processing training file 17 of 25: ./training/train.bucket18.txt\n",
      "2022-11-22 16:34:38,521 - INFO - input_processor - Done processing training file, batch size is 40188\n",
      "2022-11-22 16:34:38,522 - INFO - input_processor - Processing training file 18 of 25: ./training/train.bucket19.txt\n",
      "2022-11-22 16:34:38,541 - INFO - input_processor - Done processing training file, batch size is 42552\n",
      "2022-11-22 16:34:38,542 - INFO - input_processor - Processing training file 19 of 25: ./training/train.bucket8.txt\n",
      "2022-11-22 16:34:38,562 - INFO - input_processor - Done processing training file, batch size is 44916\n",
      "2022-11-22 16:34:38,563 - INFO - input_processor - Processing training file 20 of 25: ./training/train.bucket12.txt\n",
      "2022-11-22 16:34:38,584 - INFO - input_processor - Done processing training file, batch size is 47280\n",
      "2022-11-22 16:34:38,585 - INFO - input_processor - Processing training file 21 of 25: ./training/train.bucket0.txt\n",
      "2022-11-22 16:34:38,607 - INFO - input_processor - Done processing training file, batch size is 49644\n",
      "2022-11-22 16:34:38,607 - INFO - input_processor - Processing training file 22 of 25: ./training/train.bucket16.txt\n",
      "2022-11-22 16:34:38,630 - INFO - input_processor - Done processing training file, batch size is 52008\n",
      "2022-11-22 16:34:38,631 - INFO - input_processor - Processing training file 23 of 25: ./training/train.bucket6.txt\n",
      "2022-11-22 16:34:38,654 - INFO - input_processor - Done processing training file, batch size is 54372\n",
      "2022-11-22 16:34:38,655 - INFO - input_processor - Processing training file 24 of 25: ./training/train.bucket9.txt\n",
      "2022-11-22 16:34:38,680 - INFO - input_processor - Done processing training file, batch size is 56736\n",
      "2022-11-22 16:34:38,680 - INFO - input_processor - Processing training file 25 of 25: ./training/train.bucket17.txt\n",
      "2022-11-22 16:34:38,706 - INFO - input_processor - Done processing training file, batch size is 59100\n",
      "2022-11-22 16:34:39,232 - INFO - train - Step: 100\tLoss: 0.01680166333913803\n",
      "2022-11-22 16:34:39,750 - INFO - train - Step: 200\tLoss: 0.01669865206815302\n",
      "2022-11-22 16:34:40,268 - INFO - train - Step: 300\tLoss: 0.01644291151314974\n",
      "2022-11-22 16:34:40,785 - INFO - train - Step: 400\tLoss: 0.01638813665136695\n",
      "2022-11-22 16:34:41,303 - INFO - train - Step: 500\tLoss: 0.01637809230014682\n",
      "2022-11-22 16:34:41,821 - INFO - train - Step: 600\tLoss: 0.01622959740459919\n",
      "2022-11-22 16:34:42,341 - INFO - train - Step: 700\tLoss: 0.01631988656707108\n",
      "2022-11-22 16:34:42,860 - INFO - train - Step: 800\tLoss: 0.01660361207090318\n",
      "2022-11-22 16:34:43,378 - INFO - train - Step: 900\tLoss: 0.01625807086005807\n",
      "2022-11-22 16:34:43,896 - INFO - train - Step: 1000\tLoss: 0.01586696945130825\n",
      "2022-11-22 16:34:44,415 - INFO - train - Step: 1100\tLoss: 0.01638308737426996\n",
      "2022-11-22 16:34:44,934 - INFO - train - Step: 1200\tLoss: 0.01620540994219482\n",
      "2022-11-22 16:34:45,454 - INFO - train - Step: 1300\tLoss: 0.01644579678773880\n",
      "2022-11-22 16:34:45,972 - INFO - train - Step: 1400\tLoss: 0.01673665582202375\n",
      "2022-11-22 16:34:46,490 - INFO - train - Step: 1500\tLoss: 0.01648133398965001\n",
      "2022-11-22 16:34:47,007 - INFO - train - Step: 1600\tLoss: 0.01660331824794412\n",
      "2022-11-22 16:34:47,525 - INFO - train - Step: 1700\tLoss: 0.01651429224759340\n",
      "2022-11-22 16:34:48,043 - INFO - train - Step: 1800\tLoss: 0.01648362201638520\n",
      "2022-11-22 16:34:48,278 - INFO - input_processor - Reached the end of the dataset\n",
      "2022-11-22 16:34:48,279 - INFO - train - Done with epoch 11\n",
      "Epoch:  60%|██████    | 12/20 [01:59<01:19,  9.95s/it]2022-11-22 16:34:48,280 - INFO - input_processor - Processing training file 1 of 25: ./training/train.bucket11.txt\n",
      "2022-11-22 16:34:48,284 - INFO - input_processor - Done processing training file, batch size is 2364\n",
      "2022-11-22 16:34:48,284 - INFO - input_processor - Processing training file 2 of 25: ./training/train.bucket8.txt\n",
      "2022-11-22 16:34:48,289 - INFO - input_processor - Done processing training file, batch size is 4728\n",
      "2022-11-22 16:34:48,289 - INFO - input_processor - Processing training file 3 of 25: ./training/train.bucket7.txt\n",
      "2022-11-22 16:34:48,294 - INFO - input_processor - Done processing training file, batch size is 7092\n",
      "2022-11-22 16:34:48,295 - INFO - input_processor - Processing training file 4 of 25: ./training/train.bucket15.txt\n",
      "2022-11-22 16:34:48,301 - INFO - input_processor - Done processing training file, batch size is 9456\n",
      "2022-11-22 16:34:48,302 - INFO - input_processor - Processing training file 5 of 25: ./training/train.bucket16.txt\n",
      "2022-11-22 16:34:48,309 - INFO - input_processor - Done processing training file, batch size is 11820\n",
      "2022-11-22 16:34:48,310 - INFO - input_processor - Processing training file 6 of 25: ./training/train.bucket10.txt\n",
      "2022-11-22 16:34:48,318 - INFO - input_processor - Done processing training file, batch size is 14184\n",
      "2022-11-22 16:34:48,319 - INFO - input_processor - Processing training file 7 of 25: ./training/train.bucket13.txt\n",
      "2022-11-22 16:34:48,328 - INFO - input_processor - Done processing training file, batch size is 16548\n",
      "2022-11-22 16:34:48,328 - INFO - input_processor - Processing training file 8 of 25: ./training/train.bucket0.txt\n",
      "2022-11-22 16:34:48,338 - INFO - input_processor - Done processing training file, batch size is 18912\n",
      "2022-11-22 16:34:48,339 - INFO - input_processor - Processing training file 9 of 25: ./training/train.bucket21.txt\n",
      "2022-11-22 16:34:48,350 - INFO - input_processor - Done processing training file, batch size is 21276\n",
      "2022-11-22 16:34:48,351 - INFO - input_processor - Processing training file 10 of 25: ./training/train.bucket4.txt\n",
      "2022-11-22 16:34:48,362 - INFO - input_processor - Done processing training file, batch size is 23640\n",
      "2022-11-22 16:34:48,363 - INFO - input_processor - Processing training file 11 of 25: ./training/train.bucket6.txt\n",
      "2022-11-22 16:34:48,376 - INFO - input_processor - Done processing training file, batch size is 26004\n",
      "2022-11-22 16:34:48,376 - INFO - input_processor - Processing training file 12 of 25: ./training/train.bucket12.txt\n",
      "2022-11-22 16:34:48,390 - INFO - input_processor - Done processing training file, batch size is 28368\n",
      "2022-11-22 16:34:48,391 - INFO - input_processor - Processing training file 13 of 25: ./training/train.bucket1.txt\n",
      "2022-11-22 16:34:48,405 - INFO - input_processor - Done processing training file, batch size is 30732\n",
      "2022-11-22 16:34:48,406 - INFO - input_processor - Processing training file 14 of 25: ./training/train.bucket19.txt\n",
      "2022-11-22 16:34:48,421 - INFO - input_processor - Done processing training file, batch size is 33096\n",
      "2022-11-22 16:34:48,422 - INFO - input_processor - Processing training file 15 of 25: ./training/train.bucket9.txt\n",
      "2022-11-22 16:34:48,438 - INFO - input_processor - Done processing training file, batch size is 35460\n",
      "2022-11-22 16:34:48,439 - INFO - input_processor - Processing training file 16 of 25: ./training/train.bucket23.txt\n",
      "2022-11-22 16:34:48,456 - INFO - input_processor - Done processing training file, batch size is 37824\n",
      "2022-11-22 16:34:48,457 - INFO - input_processor - Processing training file 17 of 25: ./training/train.bucket17.txt\n",
      "2022-11-22 16:34:48,475 - INFO - input_processor - Done processing training file, batch size is 40188\n",
      "2022-11-22 16:34:48,475 - INFO - input_processor - Processing training file 18 of 25: ./training/train.bucket2.txt\n",
      "2022-11-22 16:34:48,495 - INFO - input_processor - Done processing training file, batch size is 42552\n",
      "2022-11-22 16:34:48,495 - INFO - input_processor - Processing training file 19 of 25: ./training/train.bucket14.txt\n",
      "2022-11-22 16:34:48,516 - INFO - input_processor - Done processing training file, batch size is 44916\n",
      "2022-11-22 16:34:48,516 - INFO - input_processor - Processing training file 20 of 25: ./training/train.bucket3.txt\n",
      "2022-11-22 16:34:48,537 - INFO - input_processor - Done processing training file, batch size is 47280\n",
      "2022-11-22 16:34:48,538 - INFO - input_processor - Processing training file 21 of 25: ./training/train.bucket18.txt\n",
      "2022-11-22 16:34:48,560 - INFO - input_processor - Done processing training file, batch size is 49644\n",
      "2022-11-22 16:34:48,560 - INFO - input_processor - Processing training file 22 of 25: ./training/train.bucket20.txt\n",
      "2022-11-22 16:34:48,583 - INFO - input_processor - Done processing training file, batch size is 52008\n",
      "2022-11-22 16:34:48,584 - INFO - input_processor - Processing training file 23 of 25: ./training/train.bucket5.txt\n",
      "2022-11-22 16:34:48,608 - INFO - input_processor - Done processing training file, batch size is 54372\n",
      "2022-11-22 16:34:48,608 - INFO - input_processor - Processing training file 24 of 25: ./training/train.bucket22.txt\n",
      "2022-11-22 16:34:48,633 - INFO - input_processor - Done processing training file, batch size is 56736\n",
      "2022-11-22 16:34:48,633 - INFO - input_processor - Processing training file 25 of 25: ./training/train.bucket24.txt\n",
      "2022-11-22 16:34:48,659 - INFO - input_processor - Done processing training file, batch size is 59100\n",
      "2022-11-22 16:34:49,183 - INFO - train - Step: 100\tLoss: 0.01654372749850154\n",
      "2022-11-22 16:34:49,716 - INFO - train - Step: 200\tLoss: 0.01683306046761572\n",
      "2022-11-22 16:34:50,273 - INFO - train - Step: 300\tLoss: 0.01630156713537872\n",
      "2022-11-22 16:34:50,822 - INFO - train - Step: 400\tLoss: 0.01672092421911657\n",
      "2022-11-22 16:34:51,341 - INFO - train - Step: 500\tLoss: 0.01648291761986911\n",
      "2022-11-22 16:34:51,859 - INFO - train - Step: 600\tLoss: 0.01650349154137075\n",
      "2022-11-22 16:34:52,377 - INFO - train - Step: 700\tLoss: 0.01645132266916335\n",
      "2022-11-22 16:34:52,894 - INFO - train - Step: 800\tLoss: 0.01639006614685059\n",
      "2022-11-22 16:34:53,413 - INFO - train - Step: 900\tLoss: 0.01652480167336762\n",
      "2022-11-22 16:34:53,938 - INFO - train - Step: 1000\tLoss: 0.01639261065982282\n",
      "2022-11-22 16:34:54,456 - INFO - train - Step: 1100\tLoss: 0.01649440824054181\n",
      "2022-11-22 16:34:54,976 - INFO - train - Step: 1200\tLoss: 0.01653463832102716\n",
      "2022-11-22 16:34:55,493 - INFO - train - Step: 1300\tLoss: 0.01656381078995764\n",
      "2022-11-22 16:34:56,011 - INFO - train - Step: 1400\tLoss: 0.01626514077186585\n",
      "2022-11-22 16:34:56,529 - INFO - train - Step: 1500\tLoss: 0.01618225459940732\n",
      "2022-11-22 16:34:57,046 - INFO - train - Step: 1600\tLoss: 0.01608058172278106\n",
      "2022-11-22 16:34:57,563 - INFO - train - Step: 1700\tLoss: 0.01613532103598118\n",
      "2022-11-22 16:34:58,082 - INFO - train - Step: 1800\tLoss: 0.01657719362527132\n",
      "2022-11-22 16:34:58,315 - INFO - input_processor - Reached the end of the dataset\n",
      "2022-11-22 16:34:58,316 - INFO - train - Done with epoch 12\n",
      "Epoch:  65%|██████▌   | 13/20 [02:09<01:09,  9.98s/it]2022-11-22 16:34:58,317 - INFO - input_processor - Processing training file 1 of 25: ./training/train.bucket17.txt\n",
      "2022-11-22 16:34:58,321 - INFO - input_processor - Done processing training file, batch size is 2364\n",
      "2022-11-22 16:34:58,321 - INFO - input_processor - Processing training file 2 of 25: ./training/train.bucket9.txt\n",
      "2022-11-22 16:34:58,326 - INFO - input_processor - Done processing training file, batch size is 4728\n",
      "2022-11-22 16:34:58,326 - INFO - input_processor - Processing training file 3 of 25: ./training/train.bucket21.txt\n",
      "2022-11-22 16:34:58,331 - INFO - input_processor - Done processing training file, batch size is 7092\n",
      "2022-11-22 16:34:58,332 - INFO - input_processor - Processing training file 4 of 25: ./training/train.bucket7.txt\n",
      "2022-11-22 16:34:58,338 - INFO - input_processor - Done processing training file, batch size is 9456\n",
      "2022-11-22 16:34:58,338 - INFO - input_processor - Processing training file 5 of 25: ./training/train.bucket1.txt\n",
      "2022-11-22 16:34:58,347 - INFO - input_processor - Done processing training file, batch size is 11820\n",
      "2022-11-22 16:34:58,347 - INFO - input_processor - Processing training file 6 of 25: ./training/train.bucket13.txt\n",
      "2022-11-22 16:34:58,355 - INFO - input_processor - Done processing training file, batch size is 14184\n",
      "2022-11-22 16:34:58,355 - INFO - input_processor - Processing training file 7 of 25: ./training/train.bucket12.txt\n",
      "2022-11-22 16:34:58,364 - INFO - input_processor - Done processing training file, batch size is 16548\n",
      "2022-11-22 16:34:58,364 - INFO - input_processor - Processing training file 8 of 25: ./training/train.bucket4.txt\n",
      "2022-11-22 16:34:58,375 - INFO - input_processor - Done processing training file, batch size is 18912\n",
      "2022-11-22 16:34:58,375 - INFO - input_processor - Processing training file 9 of 25: ./training/train.bucket2.txt\n",
      "2022-11-22 16:34:58,386 - INFO - input_processor - Done processing training file, batch size is 21276\n",
      "2022-11-22 16:34:58,387 - INFO - input_processor - Processing training file 10 of 25: ./training/train.bucket5.txt\n",
      "2022-11-22 16:34:58,398 - INFO - input_processor - Done processing training file, batch size is 23640\n",
      "2022-11-22 16:34:58,399 - INFO - input_processor - Processing training file 11 of 25: ./training/train.bucket22.txt\n",
      "2022-11-22 16:34:58,411 - INFO - input_processor - Done processing training file, batch size is 26004\n",
      "2022-11-22 16:34:58,412 - INFO - input_processor - Processing training file 12 of 25: ./training/train.bucket6.txt\n",
      "2022-11-22 16:34:58,426 - INFO - input_processor - Done processing training file, batch size is 28368\n",
      "2022-11-22 16:34:58,427 - INFO - input_processor - Processing training file 13 of 25: ./training/train.bucket10.txt\n",
      "2022-11-22 16:34:58,441 - INFO - input_processor - Done processing training file, batch size is 30732\n",
      "2022-11-22 16:34:58,442 - INFO - input_processor - Processing training file 14 of 25: ./training/train.bucket20.txt\n",
      "2022-11-22 16:34:58,457 - INFO - input_processor - Done processing training file, batch size is 33096\n",
      "2022-11-22 16:34:58,458 - INFO - input_processor - Processing training file 15 of 25: ./training/train.bucket11.txt\n",
      "2022-11-22 16:34:58,474 - INFO - input_processor - Done processing training file, batch size is 35460\n",
      "2022-11-22 16:34:58,474 - INFO - input_processor - Processing training file 16 of 25: ./training/train.bucket0.txt\n",
      "2022-11-22 16:34:58,492 - INFO - input_processor - Done processing training file, batch size is 37824\n",
      "2022-11-22 16:34:58,492 - INFO - input_processor - Processing training file 17 of 25: ./training/train.bucket16.txt\n",
      "2022-11-22 16:34:58,511 - INFO - input_processor - Done processing training file, batch size is 40188\n",
      "2022-11-22 16:34:58,511 - INFO - input_processor - Processing training file 18 of 25: ./training/train.bucket15.txt\n",
      "2022-11-22 16:34:58,530 - INFO - input_processor - Done processing training file, batch size is 42552\n",
      "2022-11-22 16:34:58,531 - INFO - input_processor - Processing training file 19 of 25: ./training/train.bucket23.txt\n",
      "2022-11-22 16:34:58,551 - INFO - input_processor - Done processing training file, batch size is 44916\n",
      "2022-11-22 16:34:58,551 - INFO - input_processor - Processing training file 20 of 25: ./training/train.bucket14.txt\n",
      "2022-11-22 16:34:58,573 - INFO - input_processor - Done processing training file, batch size is 47280\n",
      "2022-11-22 16:34:58,573 - INFO - input_processor - Processing training file 21 of 25: ./training/train.bucket24.txt\n",
      "2022-11-22 16:34:58,595 - INFO - input_processor - Done processing training file, batch size is 49644\n",
      "2022-11-22 16:34:58,596 - INFO - input_processor - Processing training file 22 of 25: ./training/train.bucket19.txt\n",
      "2022-11-22 16:34:58,619 - INFO - input_processor - Done processing training file, batch size is 52008\n",
      "2022-11-22 16:34:58,619 - INFO - input_processor - Processing training file 23 of 25: ./training/train.bucket8.txt\n",
      "2022-11-22 16:34:58,643 - INFO - input_processor - Done processing training file, batch size is 54372\n",
      "2022-11-22 16:34:58,643 - INFO - input_processor - Processing training file 24 of 25: ./training/train.bucket3.txt\n",
      "2022-11-22 16:34:58,668 - INFO - input_processor - Done processing training file, batch size is 56736\n",
      "2022-11-22 16:34:58,668 - INFO - input_processor - Processing training file 25 of 25: ./training/train.bucket18.txt\n",
      "2022-11-22 16:34:58,694 - INFO - input_processor - Done processing training file, batch size is 59100\n",
      "2022-11-22 16:34:59,218 - INFO - train - Step: 100\tLoss: 0.01693945575505495\n",
      "2022-11-22 16:34:59,734 - INFO - train - Step: 200\tLoss: 0.01633343432098627\n",
      "2022-11-22 16:35:00,259 - INFO - train - Step: 300\tLoss: 0.01641179339960218\n",
      "2022-11-22 16:35:00,803 - INFO - train - Step: 400\tLoss: 0.01640219322405756\n",
      "2022-11-22 16:35:01,332 - INFO - train - Step: 500\tLoss: 0.01627306574955583\n",
      "2022-11-22 16:35:01,850 - INFO - train - Step: 600\tLoss: 0.01635252560488880\n",
      "2022-11-22 16:35:02,367 - INFO - train - Step: 700\tLoss: 0.01613096865825355\n",
      "2022-11-22 16:35:02,886 - INFO - train - Step: 800\tLoss: 0.01633788846433163\n",
      "2022-11-22 16:35:03,403 - INFO - train - Step: 900\tLoss: 0.01654435564763844\n",
      "2022-11-22 16:35:03,921 - INFO - train - Step: 1000\tLoss: 0.01644613047130406\n",
      "2022-11-22 16:35:04,439 - INFO - train - Step: 1100\tLoss: 0.01635081436485052\n",
      "2022-11-22 16:35:04,983 - INFO - train - Step: 1200\tLoss: 0.01675968158990145\n",
      "2022-11-22 16:35:05,513 - INFO - train - Step: 1300\tLoss: 0.01650620456784964\n",
      "2022-11-22 16:35:06,031 - INFO - train - Step: 1400\tLoss: 0.01626372235827148\n",
      "2022-11-22 16:35:06,548 - INFO - train - Step: 1500\tLoss: 0.01646448411978781\n",
      "2022-11-22 16:35:07,067 - INFO - train - Step: 1600\tLoss: 0.01622730560600757\n",
      "2022-11-22 16:35:07,587 - INFO - train - Step: 1700\tLoss: 0.01633418269455433\n",
      "2022-11-22 16:35:08,105 - INFO - train - Step: 1800\tLoss: 0.01627159127965570\n",
      "2022-11-22 16:35:08,339 - INFO - input_processor - Reached the end of the dataset\n",
      "2022-11-22 16:35:08,339 - INFO - train - Done with epoch 13\n",
      "Epoch:  70%|███████   | 14/20 [02:19<00:59,  9.99s/it]2022-11-22 16:35:08,340 - INFO - input_processor - Processing training file 1 of 25: ./training/train.bucket21.txt\n",
      "2022-11-22 16:35:08,345 - INFO - input_processor - Done processing training file, batch size is 2364\n",
      "2022-11-22 16:35:08,345 - INFO - input_processor - Processing training file 2 of 25: ./training/train.bucket4.txt\n",
      "2022-11-22 16:35:08,350 - INFO - input_processor - Done processing training file, batch size is 4728\n",
      "2022-11-22 16:35:08,350 - INFO - input_processor - Processing training file 3 of 25: ./training/train.bucket3.txt\n",
      "2022-11-22 16:35:08,355 - INFO - input_processor - Done processing training file, batch size is 7092\n",
      "2022-11-22 16:35:08,356 - INFO - input_processor - Processing training file 4 of 25: ./training/train.bucket13.txt\n",
      "2022-11-22 16:35:08,362 - INFO - input_processor - Done processing training file, batch size is 9456\n",
      "2022-11-22 16:35:08,362 - INFO - input_processor - Processing training file 5 of 25: ./training/train.bucket7.txt\n",
      "2022-11-22 16:35:08,370 - INFO - input_processor - Done processing training file, batch size is 11820\n",
      "2022-11-22 16:35:08,370 - INFO - input_processor - Processing training file 6 of 25: ./training/train.bucket2.txt\n",
      "2022-11-22 16:35:08,378 - INFO - input_processor - Done processing training file, batch size is 14184\n",
      "2022-11-22 16:35:08,378 - INFO - input_processor - Processing training file 7 of 25: ./training/train.bucket22.txt\n",
      "2022-11-22 16:35:08,387 - INFO - input_processor - Done processing training file, batch size is 16548\n",
      "2022-11-22 16:35:08,388 - INFO - input_processor - Processing training file 8 of 25: ./training/train.bucket1.txt\n",
      "2022-11-22 16:35:08,398 - INFO - input_processor - Done processing training file, batch size is 18912\n",
      "2022-11-22 16:35:08,399 - INFO - input_processor - Processing training file 9 of 25: ./training/train.bucket9.txt\n",
      "2022-11-22 16:35:08,410 - INFO - input_processor - Done processing training file, batch size is 21276\n",
      "2022-11-22 16:35:08,411 - INFO - input_processor - Processing training file 10 of 25: ./training/train.bucket20.txt\n",
      "2022-11-22 16:35:08,422 - INFO - input_processor - Done processing training file, batch size is 23640\n",
      "2022-11-22 16:35:08,423 - INFO - input_processor - Processing training file 11 of 25: ./training/train.bucket24.txt\n",
      "2022-11-22 16:35:08,435 - INFO - input_processor - Done processing training file, batch size is 26004\n",
      "2022-11-22 16:35:08,436 - INFO - input_processor - Processing training file 12 of 25: ./training/train.bucket15.txt\n",
      "2022-11-22 16:35:08,449 - INFO - input_processor - Done processing training file, batch size is 28368\n",
      "2022-11-22 16:35:08,450 - INFO - input_processor - Processing training file 13 of 25: ./training/train.bucket19.txt\n",
      "2022-11-22 16:35:08,464 - INFO - input_processor - Done processing training file, batch size is 30732\n",
      "2022-11-22 16:35:08,465 - INFO - input_processor - Processing training file 14 of 25: ./training/train.bucket5.txt\n",
      "2022-11-22 16:35:08,480 - INFO - input_processor - Done processing training file, batch size is 33096\n",
      "2022-11-22 16:35:08,480 - INFO - input_processor - Processing training file 15 of 25: ./training/train.bucket16.txt\n",
      "2022-11-22 16:35:08,497 - INFO - input_processor - Done processing training file, batch size is 35460\n",
      "2022-11-22 16:35:08,497 - INFO - input_processor - Processing training file 16 of 25: ./training/train.bucket0.txt\n",
      "2022-11-22 16:35:08,515 - INFO - input_processor - Done processing training file, batch size is 37824\n",
      "2022-11-22 16:35:08,515 - INFO - input_processor - Processing training file 17 of 25: ./training/train.bucket23.txt\n",
      "2022-11-22 16:35:08,533 - INFO - input_processor - Done processing training file, batch size is 40188\n",
      "2022-11-22 16:35:08,534 - INFO - input_processor - Processing training file 18 of 25: ./training/train.bucket18.txt\n",
      "2022-11-22 16:35:08,553 - INFO - input_processor - Done processing training file, batch size is 42552\n",
      "2022-11-22 16:35:08,553 - INFO - input_processor - Processing training file 19 of 25: ./training/train.bucket14.txt\n",
      "2022-11-22 16:35:08,574 - INFO - input_processor - Done processing training file, batch size is 44916\n",
      "2022-11-22 16:35:08,574 - INFO - input_processor - Processing training file 20 of 25: ./training/train.bucket6.txt\n",
      "2022-11-22 16:35:08,596 - INFO - input_processor - Done processing training file, batch size is 47280\n",
      "2022-11-22 16:35:08,596 - INFO - input_processor - Processing training file 21 of 25: ./training/train.bucket8.txt\n",
      "2022-11-22 16:35:08,618 - INFO - input_processor - Done processing training file, batch size is 49644\n",
      "2022-11-22 16:35:08,619 - INFO - input_processor - Processing training file 22 of 25: ./training/train.bucket17.txt\n",
      "2022-11-22 16:35:08,642 - INFO - input_processor - Done processing training file, batch size is 52008\n",
      "2022-11-22 16:35:08,643 - INFO - input_processor - Processing training file 23 of 25: ./training/train.bucket11.txt\n",
      "2022-11-22 16:35:08,668 - INFO - input_processor - Done processing training file, batch size is 54372\n",
      "2022-11-22 16:35:08,668 - INFO - input_processor - Processing training file 24 of 25: ./training/train.bucket10.txt\n",
      "2022-11-22 16:35:08,694 - INFO - input_processor - Done processing training file, batch size is 56736\n",
      "2022-11-22 16:35:08,695 - INFO - input_processor - Processing training file 25 of 25: ./training/train.bucket12.txt\n",
      "2022-11-22 16:35:08,722 - INFO - input_processor - Done processing training file, batch size is 59100\n",
      "2022-11-22 16:35:09,270 - INFO - train - Step: 100\tLoss: 0.01681037928909063\n",
      "2022-11-22 16:35:09,787 - INFO - train - Step: 200\tLoss: 0.01681371689774096\n",
      "2022-11-22 16:35:10,306 - INFO - train - Step: 300\tLoss: 0.01657846494577825\n",
      "2022-11-22 16:35:10,824 - INFO - train - Step: 400\tLoss: 0.01607689942233264\n",
      "2022-11-22 16:35:11,343 - INFO - train - Step: 500\tLoss: 0.01643823821097612\n",
      "2022-11-22 16:35:11,861 - INFO - train - Step: 600\tLoss: 0.01606912658549845\n",
      "2022-11-22 16:35:12,380 - INFO - train - Step: 700\tLoss: 0.01624116736464203\n",
      "2022-11-22 16:35:12,911 - INFO - train - Step: 800\tLoss: 0.01639374637976289\n",
      "2022-11-22 16:35:13,454 - INFO - train - Step: 900\tLoss: 0.01663217629306018\n",
      "2022-11-22 16:35:13,985 - INFO - train - Step: 1000\tLoss: 0.01641510976478457\n",
      "2022-11-22 16:35:14,516 - INFO - train - Step: 1100\tLoss: 0.01655239407904446\n",
      "2022-11-22 16:35:15,037 - INFO - train - Step: 1200\tLoss: 0.01646465156227350\n",
      "2022-11-22 16:35:15,553 - INFO - train - Step: 1300\tLoss: 0.01625608277507126\n",
      "2022-11-22 16:35:16,070 - INFO - train - Step: 1400\tLoss: 0.01637700906954706\n",
      "2022-11-22 16:35:16,591 - INFO - train - Step: 1500\tLoss: 0.01674083044752479\n",
      "2022-11-22 16:35:17,108 - INFO - train - Step: 1600\tLoss: 0.01641976426355541\n",
      "2022-11-22 16:35:17,625 - INFO - train - Step: 1700\tLoss: 0.01643976784311235\n",
      "2022-11-22 16:35:18,149 - INFO - train - Step: 1800\tLoss: 0.01629214775748551\n",
      "2022-11-22 16:35:18,389 - INFO - input_processor - Reached the end of the dataset\n",
      "2022-11-22 16:35:18,389 - INFO - train - Done with epoch 14\n",
      "Epoch:  75%|███████▌  | 15/20 [02:29<00:50, 10.01s/it]2022-11-22 16:35:18,391 - INFO - input_processor - Processing training file 1 of 25: ./training/train.bucket3.txt\n",
      "2022-11-22 16:35:18,395 - INFO - input_processor - Done processing training file, batch size is 2364\n",
      "2022-11-22 16:35:18,395 - INFO - input_processor - Processing training file 2 of 25: ./training/train.bucket22.txt\n",
      "2022-11-22 16:35:18,400 - INFO - input_processor - Done processing training file, batch size is 4728\n",
      "2022-11-22 16:35:18,400 - INFO - input_processor - Processing training file 3 of 25: ./training/train.bucket7.txt\n",
      "2022-11-22 16:35:18,406 - INFO - input_processor - Done processing training file, batch size is 7092\n",
      "2022-11-22 16:35:18,406 - INFO - input_processor - Processing training file 4 of 25: ./training/train.bucket6.txt\n",
      "2022-11-22 16:35:18,413 - INFO - input_processor - Done processing training file, batch size is 9456\n",
      "2022-11-22 16:35:18,414 - INFO - input_processor - Processing training file 5 of 25: ./training/train.bucket17.txt\n",
      "2022-11-22 16:35:18,422 - INFO - input_processor - Done processing training file, batch size is 11820\n",
      "2022-11-22 16:35:18,422 - INFO - input_processor - Processing training file 6 of 25: ./training/train.bucket24.txt\n",
      "2022-11-22 16:35:18,431 - INFO - input_processor - Done processing training file, batch size is 14184\n",
      "2022-11-22 16:35:18,431 - INFO - input_processor - Processing training file 7 of 25: ./training/train.bucket13.txt\n",
      "2022-11-22 16:35:18,442 - INFO - input_processor - Done processing training file, batch size is 16548\n",
      "2022-11-22 16:35:18,442 - INFO - input_processor - Processing training file 8 of 25: ./training/train.bucket10.txt\n",
      "2022-11-22 16:35:18,452 - INFO - input_processor - Done processing training file, batch size is 18912\n",
      "2022-11-22 16:35:18,453 - INFO - input_processor - Processing training file 9 of 25: ./training/train.bucket15.txt\n",
      "2022-11-22 16:35:18,464 - INFO - input_processor - Done processing training file, batch size is 21276\n",
      "2022-11-22 16:35:18,465 - INFO - input_processor - Processing training file 10 of 25: ./training/train.bucket2.txt\n",
      "2022-11-22 16:35:18,477 - INFO - input_processor - Done processing training file, batch size is 23640\n",
      "2022-11-22 16:35:18,478 - INFO - input_processor - Processing training file 11 of 25: ./training/train.bucket14.txt\n",
      "2022-11-22 16:35:18,491 - INFO - input_processor - Done processing training file, batch size is 26004\n",
      "2022-11-22 16:35:18,491 - INFO - input_processor - Processing training file 12 of 25: ./training/train.bucket4.txt\n",
      "2022-11-22 16:35:18,506 - INFO - input_processor - Done processing training file, batch size is 28368\n",
      "2022-11-22 16:35:18,507 - INFO - input_processor - Processing training file 13 of 25: ./training/train.bucket12.txt\n",
      "2022-11-22 16:35:18,522 - INFO - input_processor - Done processing training file, batch size is 30732\n",
      "2022-11-22 16:35:18,522 - INFO - input_processor - Processing training file 14 of 25: ./training/train.bucket1.txt\n",
      "2022-11-22 16:35:18,538 - INFO - input_processor - Done processing training file, batch size is 33096\n",
      "2022-11-22 16:35:18,539 - INFO - input_processor - Processing training file 15 of 25: ./training/train.bucket9.txt\n",
      "2022-11-22 16:35:18,556 - INFO - input_processor - Done processing training file, batch size is 35460\n",
      "2022-11-22 16:35:18,556 - INFO - input_processor - Processing training file 16 of 25: ./training/train.bucket8.txt\n",
      "2022-11-22 16:35:18,575 - INFO - input_processor - Done processing training file, batch size is 37824\n",
      "2022-11-22 16:35:18,575 - INFO - input_processor - Processing training file 17 of 25: ./training/train.bucket11.txt\n",
      "2022-11-22 16:35:18,595 - INFO - input_processor - Done processing training file, batch size is 40188\n",
      "2022-11-22 16:35:18,595 - INFO - input_processor - Processing training file 18 of 25: ./training/train.bucket21.txt\n",
      "2022-11-22 16:35:18,615 - INFO - input_processor - Done processing training file, batch size is 42552\n",
      "2022-11-22 16:35:18,616 - INFO - input_processor - Processing training file 19 of 25: ./training/train.bucket20.txt\n",
      "2022-11-22 16:35:18,638 - INFO - input_processor - Done processing training file, batch size is 44916\n",
      "2022-11-22 16:35:18,639 - INFO - input_processor - Processing training file 20 of 25: ./training/train.bucket18.txt\n",
      "2022-11-22 16:35:18,660 - INFO - input_processor - Done processing training file, batch size is 47280\n",
      "2022-11-22 16:35:18,661 - INFO - input_processor - Processing training file 21 of 25: ./training/train.bucket16.txt\n",
      "2022-11-22 16:35:18,683 - INFO - input_processor - Done processing training file, batch size is 49644\n",
      "2022-11-22 16:35:18,684 - INFO - input_processor - Processing training file 22 of 25: ./training/train.bucket19.txt\n",
      "2022-11-22 16:35:18,707 - INFO - input_processor - Done processing training file, batch size is 52008\n",
      "2022-11-22 16:35:18,707 - INFO - input_processor - Processing training file 23 of 25: ./training/train.bucket23.txt\n",
      "2022-11-22 16:35:18,731 - INFO - input_processor - Done processing training file, batch size is 54372\n",
      "2022-11-22 16:35:18,732 - INFO - input_processor - Processing training file 24 of 25: ./training/train.bucket0.txt\n",
      "2022-11-22 16:35:18,756 - INFO - input_processor - Done processing training file, batch size is 56736\n",
      "2022-11-22 16:35:18,757 - INFO - input_processor - Processing training file 25 of 25: ./training/train.bucket5.txt\n",
      "2022-11-22 16:35:18,782 - INFO - input_processor - Done processing training file, batch size is 59100\n",
      "2022-11-22 16:35:19,305 - INFO - train - Step: 100\tLoss: 0.01668710356578231\n",
      "2022-11-22 16:35:19,841 - INFO - train - Step: 200\tLoss: 0.01638013207353652\n",
      "2022-11-22 16:35:20,385 - INFO - train - Step: 300\tLoss: 0.01680945227853954\n",
      "2022-11-22 16:35:20,916 - INFO - train - Step: 400\tLoss: 0.01623189140111208\n",
      "2022-11-22 16:35:21,452 - INFO - train - Step: 500\tLoss: 0.01616859813220799\n",
      "2022-11-22 16:35:21,968 - INFO - train - Step: 600\tLoss: 0.01654157391749322\n",
      "2022-11-22 16:35:22,485 - INFO - train - Step: 700\tLoss: 0.01661952603608370\n",
      "2022-11-22 16:35:23,003 - INFO - train - Step: 800\tLoss: 0.01658823509700596\n",
      "2022-11-22 16:35:23,520 - INFO - train - Step: 900\tLoss: 0.01616785547696054\n",
      "2022-11-22 16:35:24,036 - INFO - train - Step: 1000\tLoss: 0.01648775909096003\n",
      "2022-11-22 16:35:24,553 - INFO - train - Step: 1100\tLoss: 0.01646276094019413\n",
      "2022-11-22 16:35:25,068 - INFO - train - Step: 1200\tLoss: 0.01668602624908090\n",
      "2022-11-22 16:35:25,584 - INFO - train - Step: 1300\tLoss: 0.01641876379959285\n",
      "2022-11-22 16:35:26,100 - INFO - train - Step: 1400\tLoss: 0.01614212507382035\n",
      "2022-11-22 16:35:26,618 - INFO - train - Step: 1500\tLoss: 0.01640587477944791\n",
      "2022-11-22 16:35:27,154 - INFO - train - Step: 1600\tLoss: 0.01652051863260567\n",
      "2022-11-22 16:35:27,671 - INFO - train - Step: 1700\tLoss: 0.01619932223111391\n",
      "2022-11-22 16:35:28,187 - INFO - train - Step: 1800\tLoss: 0.01654315805062652\n",
      "2022-11-22 16:35:28,421 - INFO - input_processor - Reached the end of the dataset\n",
      "2022-11-22 16:35:28,421 - INFO - train - Done with epoch 15\n",
      "Epoch:  80%|████████  | 16/20 [02:39<00:40, 10.02s/it]2022-11-22 16:35:28,422 - INFO - input_processor - Processing training file 1 of 25: ./training/train.bucket2.txt\n",
      "2022-11-22 16:35:28,426 - INFO - input_processor - Done processing training file, batch size is 2364\n",
      "2022-11-22 16:35:28,427 - INFO - input_processor - Processing training file 2 of 25: ./training/train.bucket14.txt\n",
      "2022-11-22 16:35:28,431 - INFO - input_processor - Done processing training file, batch size is 4728\n",
      "2022-11-22 16:35:28,432 - INFO - input_processor - Processing training file 3 of 25: ./training/train.bucket6.txt\n",
      "2022-11-22 16:35:28,437 - INFO - input_processor - Done processing training file, batch size is 7092\n",
      "2022-11-22 16:35:28,437 - INFO - input_processor - Processing training file 4 of 25: ./training/train.bucket3.txt\n",
      "2022-11-22 16:35:28,444 - INFO - input_processor - Done processing training file, batch size is 9456\n",
      "2022-11-22 16:35:28,445 - INFO - input_processor - Processing training file 5 of 25: ./training/train.bucket23.txt\n",
      "2022-11-22 16:35:28,454 - INFO - input_processor - Done processing training file, batch size is 11820\n",
      "2022-11-22 16:35:28,454 - INFO - input_processor - Processing training file 6 of 25: ./training/train.bucket24.txt\n",
      "2022-11-22 16:35:28,463 - INFO - input_processor - Done processing training file, batch size is 14184\n",
      "2022-11-22 16:35:28,463 - INFO - input_processor - Processing training file 7 of 25: ./training/train.bucket1.txt\n",
      "2022-11-22 16:35:28,473 - INFO - input_processor - Done processing training file, batch size is 16548\n",
      "2022-11-22 16:35:28,473 - INFO - input_processor - Processing training file 8 of 25: ./training/train.bucket21.txt\n",
      "2022-11-22 16:35:28,483 - INFO - input_processor - Done processing training file, batch size is 18912\n",
      "2022-11-22 16:35:28,484 - INFO - input_processor - Processing training file 9 of 25: ./training/train.bucket4.txt\n",
      "2022-11-22 16:35:28,495 - INFO - input_processor - Done processing training file, batch size is 21276\n",
      "2022-11-22 16:35:28,496 - INFO - input_processor - Processing training file 10 of 25: ./training/train.bucket18.txt\n",
      "2022-11-22 16:35:28,508 - INFO - input_processor - Done processing training file, batch size is 23640\n",
      "2022-11-22 16:35:28,508 - INFO - input_processor - Processing training file 11 of 25: ./training/train.bucket17.txt\n",
      "2022-11-22 16:35:28,521 - INFO - input_processor - Done processing training file, batch size is 26004\n",
      "2022-11-22 16:35:28,522 - INFO - input_processor - Processing training file 12 of 25: ./training/train.bucket15.txt\n",
      "2022-11-22 16:35:28,535 - INFO - input_processor - Done processing training file, batch size is 28368\n",
      "2022-11-22 16:35:28,536 - INFO - input_processor - Processing training file 13 of 25: ./training/train.bucket19.txt\n",
      "2022-11-22 16:35:28,551 - INFO - input_processor - Done processing training file, batch size is 30732\n",
      "2022-11-22 16:35:28,551 - INFO - input_processor - Processing training file 14 of 25: ./training/train.bucket22.txt\n",
      "2022-11-22 16:35:28,567 - INFO - input_processor - Done processing training file, batch size is 33096\n",
      "2022-11-22 16:35:28,567 - INFO - input_processor - Processing training file 15 of 25: ./training/train.bucket13.txt\n",
      "2022-11-22 16:35:28,584 - INFO - input_processor - Done processing training file, batch size is 35460\n",
      "2022-11-22 16:35:28,584 - INFO - input_processor - Processing training file 16 of 25: ./training/train.bucket12.txt\n",
      "2022-11-22 16:35:28,602 - INFO - input_processor - Done processing training file, batch size is 37824\n",
      "2022-11-22 16:35:28,602 - INFO - input_processor - Processing training file 17 of 25: ./training/train.bucket7.txt\n",
      "2022-11-22 16:35:28,620 - INFO - input_processor - Done processing training file, batch size is 40188\n",
      "2022-11-22 16:35:28,621 - INFO - input_processor - Processing training file 18 of 25: ./training/train.bucket20.txt\n",
      "2022-11-22 16:35:28,640 - INFO - input_processor - Done processing training file, batch size is 42552\n",
      "2022-11-22 16:35:28,641 - INFO - input_processor - Processing training file 19 of 25: ./training/train.bucket5.txt\n",
      "2022-11-22 16:35:28,661 - INFO - input_processor - Done processing training file, batch size is 44916\n",
      "2022-11-22 16:35:28,662 - INFO - input_processor - Processing training file 20 of 25: ./training/train.bucket9.txt\n",
      "2022-11-22 16:35:28,683 - INFO - input_processor - Done processing training file, batch size is 47280\n",
      "2022-11-22 16:35:28,684 - INFO - input_processor - Processing training file 21 of 25: ./training/train.bucket0.txt\n",
      "2022-11-22 16:35:28,706 - INFO - input_processor - Done processing training file, batch size is 49644\n",
      "2022-11-22 16:35:28,706 - INFO - input_processor - Processing training file 22 of 25: ./training/train.bucket11.txt\n",
      "2022-11-22 16:35:28,729 - INFO - input_processor - Done processing training file, batch size is 52008\n",
      "2022-11-22 16:35:28,730 - INFO - input_processor - Processing training file 23 of 25: ./training/train.bucket16.txt\n",
      "2022-11-22 16:35:28,753 - INFO - input_processor - Done processing training file, batch size is 54372\n",
      "2022-11-22 16:35:28,754 - INFO - input_processor - Processing training file 24 of 25: ./training/train.bucket10.txt\n",
      "2022-11-22 16:35:28,779 - INFO - input_processor - Done processing training file, batch size is 56736\n",
      "2022-11-22 16:35:28,779 - INFO - input_processor - Processing training file 25 of 25: ./training/train.bucket8.txt\n",
      "2022-11-22 16:35:28,805 - INFO - input_processor - Done processing training file, batch size is 59100\n",
      "2022-11-22 16:35:29,329 - INFO - train - Step: 100\tLoss: 0.01654132110066712\n",
      "2022-11-22 16:35:29,845 - INFO - train - Step: 200\tLoss: 0.01637567405588925\n",
      "2022-11-22 16:35:30,361 - INFO - train - Step: 300\tLoss: 0.01664615419693291\n",
      "2022-11-22 16:35:30,877 - INFO - train - Step: 400\tLoss: 0.01631326959468424\n",
      "2022-11-22 16:35:31,393 - INFO - train - Step: 500\tLoss: 0.01629951518028975\n",
      "2022-11-22 16:35:31,909 - INFO - train - Step: 600\tLoss: 0.01629519253037870\n",
      "2022-11-22 16:35:32,425 - INFO - train - Step: 700\tLoss: 0.01663014929741621\n",
      "2022-11-22 16:35:32,942 - INFO - train - Step: 800\tLoss: 0.01632061094976962\n",
      "2022-11-22 16:35:33,458 - INFO - train - Step: 900\tLoss: 0.01617664219811559\n",
      "2022-11-22 16:35:33,973 - INFO - train - Step: 1000\tLoss: 0.01678287304006517\n",
      "2022-11-22 16:35:34,489 - INFO - train - Step: 1100\tLoss: 0.01656070536002517\n",
      "2022-11-22 16:35:35,021 - INFO - train - Step: 1200\tLoss: 0.01605371790938079\n",
      "2022-11-22 16:35:35,539 - INFO - train - Step: 1300\tLoss: 0.01639389920048416\n",
      "2022-11-22 16:35:36,057 - INFO - train - Step: 1400\tLoss: 0.01633391429670155\n",
      "2022-11-22 16:35:36,573 - INFO - train - Step: 1500\tLoss: 0.01662328757345677\n",
      "2022-11-22 16:35:37,089 - INFO - train - Step: 1600\tLoss: 0.01659866929054260\n",
      "2022-11-22 16:35:37,605 - INFO - train - Step: 1700\tLoss: 0.01631393161602318\n",
      "2022-11-22 16:35:38,144 - INFO - train - Step: 1800\tLoss: 0.01663880916312337\n",
      "2022-11-22 16:35:38,390 - INFO - input_processor - Reached the end of the dataset\n",
      "2022-11-22 16:35:38,390 - INFO - train - Done with epoch 16\n",
      "Epoch:  85%|████████▌ | 17/20 [02:49<00:30, 10.00s/it]2022-11-22 16:35:38,391 - INFO - input_processor - Processing training file 1 of 25: ./training/train.bucket6.txt\n",
      "2022-11-22 16:35:38,395 - INFO - input_processor - Done processing training file, batch size is 2364\n",
      "2022-11-22 16:35:38,396 - INFO - input_processor - Processing training file 2 of 25: ./training/train.bucket11.txt\n",
      "2022-11-22 16:35:38,400 - INFO - input_processor - Done processing training file, batch size is 4728\n",
      "2022-11-22 16:35:38,401 - INFO - input_processor - Processing training file 3 of 25: ./training/train.bucket15.txt\n",
      "2022-11-22 16:35:38,407 - INFO - input_processor - Done processing training file, batch size is 7092\n",
      "2022-11-22 16:35:38,408 - INFO - input_processor - Processing training file 4 of 25: ./training/train.bucket2.txt\n",
      "2022-11-22 16:35:38,415 - INFO - input_processor - Done processing training file, batch size is 9456\n",
      "2022-11-22 16:35:38,416 - INFO - input_processor - Processing training file 5 of 25: ./training/train.bucket17.txt\n",
      "2022-11-22 16:35:38,423 - INFO - input_processor - Done processing training file, batch size is 11820\n",
      "2022-11-22 16:35:38,424 - INFO - input_processor - Processing training file 6 of 25: ./training/train.bucket18.txt\n",
      "2022-11-22 16:35:38,432 - INFO - input_processor - Done processing training file, batch size is 14184\n",
      "2022-11-22 16:35:38,433 - INFO - input_processor - Processing training file 7 of 25: ./training/train.bucket21.txt\n",
      "2022-11-22 16:35:38,442 - INFO - input_processor - Done processing training file, batch size is 16548\n",
      "2022-11-22 16:35:38,443 - INFO - input_processor - Processing training file 8 of 25: ./training/train.bucket16.txt\n",
      "2022-11-22 16:35:38,453 - INFO - input_processor - Done processing training file, batch size is 18912\n",
      "2022-11-22 16:35:38,454 - INFO - input_processor - Processing training file 9 of 25: ./training/train.bucket3.txt\n",
      "2022-11-22 16:35:38,465 - INFO - input_processor - Done processing training file, batch size is 21276\n",
      "2022-11-22 16:35:38,465 - INFO - input_processor - Processing training file 10 of 25: ./training/train.bucket22.txt\n",
      "2022-11-22 16:35:38,478 - INFO - input_processor - Done processing training file, batch size is 23640\n",
      "2022-11-22 16:35:38,478 - INFO - input_processor - Processing training file 11 of 25: ./training/train.bucket8.txt\n",
      "2022-11-22 16:35:38,492 - INFO - input_processor - Done processing training file, batch size is 26004\n",
      "2022-11-22 16:35:38,492 - INFO - input_processor - Processing training file 12 of 25: ./training/train.bucket24.txt\n",
      "2022-11-22 16:35:38,508 - INFO - input_processor - Done processing training file, batch size is 28368\n",
      "2022-11-22 16:35:38,508 - INFO - input_processor - Processing training file 13 of 25: ./training/train.bucket13.txt\n",
      "2022-11-22 16:35:38,524 - INFO - input_processor - Done processing training file, batch size is 30732\n",
      "2022-11-22 16:35:38,524 - INFO - input_processor - Processing training file 14 of 25: ./training/train.bucket7.txt\n",
      "2022-11-22 16:35:38,540 - INFO - input_processor - Done processing training file, batch size is 33096\n",
      "2022-11-22 16:35:38,541 - INFO - input_processor - Processing training file 15 of 25: ./training/train.bucket14.txt\n",
      "2022-11-22 16:35:38,558 - INFO - input_processor - Done processing training file, batch size is 35460\n",
      "2022-11-22 16:35:38,558 - INFO - input_processor - Processing training file 16 of 25: ./training/train.bucket23.txt\n",
      "2022-11-22 16:35:38,576 - INFO - input_processor - Done processing training file, batch size is 37824\n",
      "2022-11-22 16:35:38,577 - INFO - input_processor - Processing training file 17 of 25: ./training/train.bucket4.txt\n",
      "2022-11-22 16:35:38,596 - INFO - input_processor - Done processing training file, batch size is 40188\n",
      "2022-11-22 16:35:38,597 - INFO - input_processor - Processing training file 18 of 25: ./training/train.bucket5.txt\n",
      "2022-11-22 16:35:38,617 - INFO - input_processor - Done processing training file, batch size is 42552\n",
      "2022-11-22 16:35:38,618 - INFO - input_processor - Processing training file 19 of 25: ./training/train.bucket1.txt\n",
      "2022-11-22 16:35:38,639 - INFO - input_processor - Done processing training file, batch size is 44916\n",
      "2022-11-22 16:35:38,640 - INFO - input_processor - Processing training file 20 of 25: ./training/train.bucket20.txt\n",
      "2022-11-22 16:35:38,662 - INFO - input_processor - Done processing training file, batch size is 47280\n",
      "2022-11-22 16:35:38,663 - INFO - input_processor - Processing training file 21 of 25: ./training/train.bucket9.txt\n",
      "2022-11-22 16:35:38,686 - INFO - input_processor - Done processing training file, batch size is 49644\n",
      "2022-11-22 16:35:38,686 - INFO - input_processor - Processing training file 22 of 25: ./training/train.bucket19.txt\n",
      "2022-11-22 16:35:38,709 - INFO - input_processor - Done processing training file, batch size is 52008\n",
      "2022-11-22 16:35:38,709 - INFO - input_processor - Processing training file 23 of 25: ./training/train.bucket10.txt\n",
      "2022-11-22 16:35:38,733 - INFO - input_processor - Done processing training file, batch size is 54372\n",
      "2022-11-22 16:35:38,734 - INFO - input_processor - Processing training file 24 of 25: ./training/train.bucket0.txt\n",
      "2022-11-22 16:35:38,758 - INFO - input_processor - Done processing training file, batch size is 56736\n",
      "2022-11-22 16:35:38,759 - INFO - input_processor - Processing training file 25 of 25: ./training/train.bucket12.txt\n",
      "2022-11-22 16:35:38,785 - INFO - input_processor - Done processing training file, batch size is 59100\n",
      "2022-11-22 16:35:39,310 - INFO - train - Step: 100\tLoss: 0.01643867035396397\n",
      "2022-11-22 16:35:39,826 - INFO - train - Step: 200\tLoss: 0.01632518408820033\n",
      "2022-11-22 16:35:40,343 - INFO - train - Step: 300\tLoss: 0.01667812626808882\n",
      "2022-11-22 16:35:40,860 - INFO - train - Step: 400\tLoss: 0.01629820911213756\n",
      "2022-11-22 16:35:41,377 - INFO - train - Step: 500\tLoss: 0.01649715931154788\n",
      "2022-11-22 16:35:41,893 - INFO - train - Step: 600\tLoss: 0.01614173188805580\n",
      "2022-11-22 16:35:42,411 - INFO - train - Step: 700\tLoss: 0.01614351551048458\n",
      "2022-11-22 16:35:42,927 - INFO - train - Step: 800\tLoss: 0.01644795281812549\n",
      "2022-11-22 16:35:43,444 - INFO - train - Step: 900\tLoss: 0.01642165992408991\n",
      "2022-11-22 16:35:43,961 - INFO - train - Step: 1000\tLoss: 0.01645268567837775\n",
      "2022-11-22 16:35:44,477 - INFO - train - Step: 1100\tLoss: 0.01632566596381366\n",
      "2022-11-22 16:35:44,994 - INFO - train - Step: 1200\tLoss: 0.01624560826458037\n",
      "2022-11-22 16:35:45,511 - INFO - train - Step: 1300\tLoss: 0.01630043627694249\n",
      "2022-11-22 16:35:46,028 - INFO - train - Step: 1400\tLoss: 0.01657173668034375\n",
      "2022-11-22 16:35:46,545 - INFO - train - Step: 1500\tLoss: 0.01660544426180422\n",
      "2022-11-22 16:35:47,062 - INFO - train - Step: 1600\tLoss: 0.01673116506077349\n",
      "2022-11-22 16:35:47,578 - INFO - train - Step: 1700\tLoss: 0.01648162820376456\n",
      "2022-11-22 16:35:48,094 - INFO - train - Step: 1800\tLoss: 0.01657184343785047\n",
      "2022-11-22 16:35:48,329 - INFO - input_processor - Reached the end of the dataset\n",
      "2022-11-22 16:35:48,329 - INFO - train - Done with epoch 17\n",
      "Epoch:  90%|█████████ | 18/20 [02:59<00:19,  9.98s/it]2022-11-22 16:35:48,331 - INFO - input_processor - Processing training file 1 of 25: ./training/train.bucket12.txt\n",
      "2022-11-22 16:35:48,335 - INFO - input_processor - Done processing training file, batch size is 2364\n",
      "2022-11-22 16:35:48,335 - INFO - input_processor - Processing training file 2 of 25: ./training/train.bucket21.txt\n",
      "2022-11-22 16:35:48,340 - INFO - input_processor - Done processing training file, batch size is 4728\n",
      "2022-11-22 16:35:48,340 - INFO - input_processor - Processing training file 3 of 25: ./training/train.bucket10.txt\n",
      "2022-11-22 16:35:48,345 - INFO - input_processor - Done processing training file, batch size is 7092\n",
      "2022-11-22 16:35:48,346 - INFO - input_processor - Processing training file 4 of 25: ./training/train.bucket14.txt\n",
      "2022-11-22 16:35:48,353 - INFO - input_processor - Done processing training file, batch size is 9456\n",
      "2022-11-22 16:35:48,353 - INFO - input_processor - Processing training file 5 of 25: ./training/train.bucket9.txt\n",
      "2022-11-22 16:35:48,360 - INFO - input_processor - Done processing training file, batch size is 11820\n",
      "2022-11-22 16:35:48,361 - INFO - input_processor - Processing training file 6 of 25: ./training/train.bucket11.txt\n",
      "2022-11-22 16:35:48,369 - INFO - input_processor - Done processing training file, batch size is 14184\n",
      "2022-11-22 16:35:48,370 - INFO - input_processor - Processing training file 7 of 25: ./training/train.bucket16.txt\n",
      "2022-11-22 16:35:48,379 - INFO - input_processor - Done processing training file, batch size is 16548\n",
      "2022-11-22 16:35:48,379 - INFO - input_processor - Processing training file 8 of 25: ./training/train.bucket1.txt\n",
      "2022-11-22 16:35:48,389 - INFO - input_processor - Done processing training file, batch size is 18912\n",
      "2022-11-22 16:35:48,390 - INFO - input_processor - Processing training file 9 of 25: ./training/train.bucket6.txt\n",
      "2022-11-22 16:35:48,401 - INFO - input_processor - Done processing training file, batch size is 21276\n",
      "2022-11-22 16:35:48,401 - INFO - input_processor - Processing training file 10 of 25: ./training/train.bucket17.txt\n",
      "2022-11-22 16:35:48,414 - INFO - input_processor - Done processing training file, batch size is 23640\n",
      "2022-11-22 16:35:48,414 - INFO - input_processor - Processing training file 11 of 25: ./training/train.bucket3.txt\n",
      "2022-11-22 16:35:48,427 - INFO - input_processor - Done processing training file, batch size is 26004\n",
      "2022-11-22 16:35:48,427 - INFO - input_processor - Processing training file 12 of 25: ./training/train.bucket19.txt\n",
      "2022-11-22 16:35:48,441 - INFO - input_processor - Done processing training file, batch size is 28368\n",
      "2022-11-22 16:35:48,441 - INFO - input_processor - Processing training file 13 of 25: ./training/train.bucket8.txt\n",
      "2022-11-22 16:35:48,456 - INFO - input_processor - Done processing training file, batch size is 30732\n",
      "2022-11-22 16:35:48,456 - INFO - input_processor - Processing training file 14 of 25: ./training/train.bucket18.txt\n",
      "2022-11-22 16:35:48,472 - INFO - input_processor - Done processing training file, batch size is 33096\n",
      "2022-11-22 16:35:48,473 - INFO - input_processor - Processing training file 15 of 25: ./training/train.bucket23.txt\n",
      "2022-11-22 16:35:48,489 - INFO - input_processor - Done processing training file, batch size is 35460\n",
      "2022-11-22 16:35:48,490 - INFO - input_processor - Processing training file 16 of 25: ./training/train.bucket24.txt\n",
      "2022-11-22 16:35:48,507 - INFO - input_processor - Done processing training file, batch size is 37824\n",
      "2022-11-22 16:35:48,508 - INFO - input_processor - Processing training file 17 of 25: ./training/train.bucket2.txt\n",
      "2022-11-22 16:35:48,526 - INFO - input_processor - Done processing training file, batch size is 40188\n",
      "2022-11-22 16:35:48,526 - INFO - input_processor - Processing training file 18 of 25: ./training/train.bucket5.txt\n",
      "2022-11-22 16:35:48,546 - INFO - input_processor - Done processing training file, batch size is 42552\n",
      "2022-11-22 16:35:48,546 - INFO - input_processor - Processing training file 19 of 25: ./training/train.bucket20.txt\n",
      "2022-11-22 16:35:48,566 - INFO - input_processor - Done processing training file, batch size is 44916\n",
      "2022-11-22 16:35:48,567 - INFO - input_processor - Processing training file 20 of 25: ./training/train.bucket4.txt\n",
      "2022-11-22 16:35:48,588 - INFO - input_processor - Done processing training file, batch size is 47280\n",
      "2022-11-22 16:35:48,589 - INFO - input_processor - Processing training file 21 of 25: ./training/train.bucket7.txt\n",
      "2022-11-22 16:35:48,611 - INFO - input_processor - Done processing training file, batch size is 49644\n",
      "2022-11-22 16:35:48,611 - INFO - input_processor - Processing training file 22 of 25: ./training/train.bucket0.txt\n",
      "2022-11-22 16:35:48,634 - INFO - input_processor - Done processing training file, batch size is 52008\n",
      "2022-11-22 16:35:48,635 - INFO - input_processor - Processing training file 23 of 25: ./training/train.bucket13.txt\n",
      "2022-11-22 16:35:48,659 - INFO - input_processor - Done processing training file, batch size is 54372\n",
      "2022-11-22 16:35:48,659 - INFO - input_processor - Processing training file 24 of 25: ./training/train.bucket22.txt\n",
      "2022-11-22 16:35:48,684 - INFO - input_processor - Done processing training file, batch size is 56736\n",
      "2022-11-22 16:35:48,685 - INFO - input_processor - Processing training file 25 of 25: ./training/train.bucket15.txt\n",
      "2022-11-22 16:35:48,710 - INFO - input_processor - Done processing training file, batch size is 59100\n",
      "2022-11-22 16:35:49,232 - INFO - train - Step: 100\tLoss: 0.01640570264309645\n",
      "2022-11-22 16:35:49,762 - INFO - train - Step: 200\tLoss: 0.01652191172353923\n",
      "2022-11-22 16:35:50,299 - INFO - train - Step: 300\tLoss: 0.01655941020697355\n",
      "2022-11-22 16:35:50,815 - INFO - train - Step: 400\tLoss: 0.01640573695302010\n",
      "2022-11-22 16:35:51,332 - INFO - train - Step: 500\tLoss: 0.01629359400831163\n",
      "2022-11-22 16:35:51,849 - INFO - train - Step: 600\tLoss: 0.01623159975744784\n",
      "2022-11-22 16:35:52,366 - INFO - train - Step: 700\tLoss: 0.01642351189628244\n",
      "2022-11-22 16:35:52,882 - INFO - train - Step: 800\tLoss: 0.01668860044330359\n",
      "2022-11-22 16:35:53,398 - INFO - train - Step: 900\tLoss: 0.01656241787597537\n",
      "2022-11-22 16:35:53,914 - INFO - train - Step: 1000\tLoss: 0.01637773056514561\n",
      "2022-11-22 16:35:54,430 - INFO - train - Step: 1100\tLoss: 0.01640352633781731\n",
      "2022-11-22 16:35:54,947 - INFO - train - Step: 1200\tLoss: 0.01627163439989090\n",
      "2022-11-22 16:35:55,464 - INFO - train - Step: 1300\tLoss: 0.01626866487786174\n",
      "2022-11-22 16:35:55,981 - INFO - train - Step: 1400\tLoss: 0.01665611179545522\n",
      "2022-11-22 16:35:56,496 - INFO - train - Step: 1500\tLoss: 0.01642654648981988\n",
      "2022-11-22 16:35:57,012 - INFO - train - Step: 1600\tLoss: 0.01628368411213160\n",
      "2022-11-22 16:35:57,527 - INFO - train - Step: 1700\tLoss: 0.01639355477876961\n",
      "2022-11-22 16:35:58,045 - INFO - train - Step: 1800\tLoss: 0.01647700817324221\n",
      "2022-11-22 16:35:58,278 - INFO - input_processor - Reached the end of the dataset\n",
      "2022-11-22 16:35:58,278 - INFO - train - Done with epoch 18\n",
      "Epoch:  95%|█████████▌| 19/20 [03:09<00:09,  9.97s/it]2022-11-22 16:35:58,280 - INFO - input_processor - Processing training file 1 of 25: ./training/train.bucket3.txt\n",
      "2022-11-22 16:35:58,284 - INFO - input_processor - Done processing training file, batch size is 2364\n",
      "2022-11-22 16:35:58,284 - INFO - input_processor - Processing training file 2 of 25: ./training/train.bucket24.txt\n",
      "2022-11-22 16:35:58,289 - INFO - input_processor - Done processing training file, batch size is 4728\n",
      "2022-11-22 16:35:58,289 - INFO - input_processor - Processing training file 3 of 25: ./training/train.bucket13.txt\n",
      "2022-11-22 16:35:58,295 - INFO - input_processor - Done processing training file, batch size is 7092\n",
      "2022-11-22 16:35:58,295 - INFO - input_processor - Processing training file 4 of 25: ./training/train.bucket17.txt\n",
      "2022-11-22 16:35:58,301 - INFO - input_processor - Done processing training file, batch size is 9456\n",
      "2022-11-22 16:35:58,302 - INFO - input_processor - Processing training file 5 of 25: ./training/train.bucket9.txt\n",
      "2022-11-22 16:35:58,309 - INFO - input_processor - Done processing training file, batch size is 11820\n",
      "2022-11-22 16:35:58,310 - INFO - input_processor - Processing training file 6 of 25: ./training/train.bucket23.txt\n",
      "2022-11-22 16:35:58,318 - INFO - input_processor - Done processing training file, batch size is 14184\n",
      "2022-11-22 16:35:58,318 - INFO - input_processor - Processing training file 7 of 25: ./training/train.bucket8.txt\n",
      "2022-11-22 16:35:58,327 - INFO - input_processor - Done processing training file, batch size is 16548\n",
      "2022-11-22 16:35:58,328 - INFO - input_processor - Processing training file 8 of 25: ./training/train.bucket2.txt\n",
      "2022-11-22 16:35:58,338 - INFO - input_processor - Done processing training file, batch size is 18912\n",
      "2022-11-22 16:35:58,338 - INFO - input_processor - Processing training file 9 of 25: ./training/train.bucket16.txt\n",
      "2022-11-22 16:35:58,349 - INFO - input_processor - Done processing training file, batch size is 21276\n",
      "2022-11-22 16:35:58,350 - INFO - input_processor - Processing training file 10 of 25: ./training/train.bucket22.txt\n",
      "2022-11-22 16:35:58,363 - INFO - input_processor - Done processing training file, batch size is 23640\n",
      "2022-11-22 16:35:58,363 - INFO - input_processor - Processing training file 11 of 25: ./training/train.bucket18.txt\n",
      "2022-11-22 16:35:58,376 - INFO - input_processor - Done processing training file, batch size is 26004\n",
      "2022-11-22 16:35:58,377 - INFO - input_processor - Processing training file 12 of 25: ./training/train.bucket12.txt\n",
      "2022-11-22 16:35:58,391 - INFO - input_processor - Done processing training file, batch size is 28368\n",
      "2022-11-22 16:35:58,391 - INFO - input_processor - Processing training file 13 of 25: ./training/train.bucket19.txt\n",
      "2022-11-22 16:35:58,406 - INFO - input_processor - Done processing training file, batch size is 30732\n",
      "2022-11-22 16:35:58,406 - INFO - input_processor - Processing training file 14 of 25: ./training/train.bucket0.txt\n",
      "2022-11-22 16:35:58,422 - INFO - input_processor - Done processing training file, batch size is 33096\n",
      "2022-11-22 16:35:58,422 - INFO - input_processor - Processing training file 15 of 25: ./training/train.bucket6.txt\n",
      "2022-11-22 16:35:58,439 - INFO - input_processor - Done processing training file, batch size is 35460\n",
      "2022-11-22 16:35:58,439 - INFO - input_processor - Processing training file 16 of 25: ./training/train.bucket10.txt\n",
      "2022-11-22 16:35:58,457 - INFO - input_processor - Done processing training file, batch size is 37824\n",
      "2022-11-22 16:35:58,457 - INFO - input_processor - Processing training file 17 of 25: ./training/train.bucket5.txt\n",
      "2022-11-22 16:35:58,476 - INFO - input_processor - Done processing training file, batch size is 40188\n",
      "2022-11-22 16:35:58,476 - INFO - input_processor - Processing training file 18 of 25: ./training/train.bucket14.txt\n",
      "2022-11-22 16:35:58,496 - INFO - input_processor - Done processing training file, batch size is 42552\n",
      "2022-11-22 16:35:58,497 - INFO - input_processor - Processing training file 19 of 25: ./training/train.bucket7.txt\n",
      "2022-11-22 16:35:58,517 - INFO - input_processor - Done processing training file, batch size is 44916\n",
      "2022-11-22 16:35:58,518 - INFO - input_processor - Processing training file 20 of 25: ./training/train.bucket11.txt\n",
      "2022-11-22 16:35:58,539 - INFO - input_processor - Done processing training file, batch size is 47280\n",
      "2022-11-22 16:35:58,539 - INFO - input_processor - Processing training file 21 of 25: ./training/train.bucket4.txt\n",
      "2022-11-22 16:35:58,562 - INFO - input_processor - Done processing training file, batch size is 49644\n",
      "2022-11-22 16:35:58,562 - INFO - input_processor - Processing training file 22 of 25: ./training/train.bucket1.txt\n",
      "2022-11-22 16:35:58,585 - INFO - input_processor - Done processing training file, batch size is 52008\n",
      "2022-11-22 16:35:58,586 - INFO - input_processor - Processing training file 23 of 25: ./training/train.bucket15.txt\n",
      "2022-11-22 16:35:58,610 - INFO - input_processor - Done processing training file, batch size is 54372\n",
      "2022-11-22 16:35:58,610 - INFO - input_processor - Processing training file 24 of 25: ./training/train.bucket21.txt\n",
      "2022-11-22 16:35:58,635 - INFO - input_processor - Done processing training file, batch size is 56736\n",
      "2022-11-22 16:35:58,636 - INFO - input_processor - Processing training file 25 of 25: ./training/train.bucket20.txt\n",
      "2022-11-22 16:35:58,661 - INFO - input_processor - Done processing training file, batch size is 59100\n",
      "2022-11-22 16:35:59,184 - INFO - train - Step: 100\tLoss: 0.01667946489527821\n",
      "2022-11-22 16:35:59,700 - INFO - train - Step: 200\tLoss: 0.01668167845346034\n",
      "2022-11-22 16:36:00,232 - INFO - train - Step: 300\tLoss: 0.01639657083898783\n",
      "2022-11-22 16:36:00,769 - INFO - train - Step: 400\tLoss: 0.01629257263615727\n",
      "2022-11-22 16:36:01,287 - INFO - train - Step: 500\tLoss: 0.01658445066772401\n",
      "2022-11-22 16:36:01,804 - INFO - train - Step: 600\tLoss: 0.01659595113247633\n",
      "2022-11-22 16:36:02,320 - INFO - train - Step: 700\tLoss: 0.01628595830872655\n",
      "2022-11-22 16:36:02,838 - INFO - train - Step: 800\tLoss: 0.01621772911399603\n",
      "2022-11-22 16:36:03,355 - INFO - train - Step: 900\tLoss: 0.01644356071949005\n",
      "2022-11-22 16:36:03,871 - INFO - train - Step: 1000\tLoss: 0.01652069716714323\n",
      "2022-11-22 16:36:04,389 - INFO - train - Step: 1100\tLoss: 0.01619167256169021\n",
      "2022-11-22 16:36:04,909 - INFO - train - Step: 1200\tLoss: 0.01660052013583481\n",
      "2022-11-22 16:36:05,427 - INFO - train - Step: 1300\tLoss: 0.01626188564114273\n",
      "2022-11-22 16:36:05,943 - INFO - train - Step: 1400\tLoss: 0.01645949242636562\n",
      "2022-11-22 16:36:06,460 - INFO - train - Step: 1500\tLoss: 0.01642828001640737\n",
      "2022-11-22 16:36:06,976 - INFO - train - Step: 1600\tLoss: 0.01614090944640338\n",
      "2022-11-22 16:36:07,499 - INFO - train - Step: 1700\tLoss: 0.01626498002558947\n",
      "2022-11-22 16:36:08,017 - INFO - train - Step: 1800\tLoss: 0.01635959567502141\n",
      "2022-11-22 16:36:08,249 - INFO - input_processor - Reached the end of the dataset\n",
      "2022-11-22 16:36:08,250 - INFO - train - Done with epoch 19\n",
      "Epoch: 100%|██████████| 20/20 [03:19<00:00,  9.97s/it]\n"
     ]
    }
   ],
   "source": [
    "%run train.py \\\n",
    "   --model_cls 'bert' \\\n",
    "   --bert_model 'bert-base-uncased' \\\n",
    "   --output_dir ./outputs/BERT_form \\\n",
    "   --train_dir ./training/ \\\n",
    "   --vocab ./training/train.vwc100 \\\n",
    "   --emb_file ./fcm/wordEmbeddings/glove.6B.50d.txt \\\n",
    "   --num_train_epochs 20 \\\n",
    "   --emb_dim 768 \\\n",
    "   --train_batch_size 32 \\\n",
    "   --smin 1 \\\n",
    "   --smax 1 \\\n",
    "   --max_seq_length 10 \\\n",
    "   --mode 'form' \\\n",
    "   --learning_rate 0.01 \\\n",
    "   --dropout 0.1 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3eb907",
   "metadata": {},
   "source": [
    "### Combine models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec0c4cb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-22 17:47:35.860638: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-22 17:47:39.162829: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-11-22 17:47:51.764854: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /share/apps/rc/software/CUDA/9.2.88-GCC-7.3.0-2.30/extras/CUPTI/lib64:/share/apps/rc/software/CUDA/9.2.88-GCC-7.3.0-2.30/lib64:/share/apps/rc/software/binutils/2.30-GCCcore-7.3.0/lib:/share/apps/rc/software/GCCcore/7.3.0/lib64:/share/apps/rc/software/GCCcore/7.3.0/lib:/cm/shared/apps/cuda92/toolkit/9.2.88/extras/CUPTI/lib64:/cm/local/apps/cuda/libs/current/lib64:/cm/shared/apps/cuda92/toolkit/9.2.88/targets/x86_64-linux/lib:/cm/shared/apps/slurm/18.08.9/lib64/slurm:/cm/shared/apps/slurm/18.08.9/lib64\n",
      "2022-11-22 17:47:51.765045: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /share/apps/rc/software/CUDA/9.2.88-GCC-7.3.0-2.30/extras/CUPTI/lib64:/share/apps/rc/software/CUDA/9.2.88-GCC-7.3.0-2.30/lib64:/share/apps/rc/software/binutils/2.30-GCCcore-7.3.0/lib:/share/apps/rc/software/GCCcore/7.3.0/lib64:/share/apps/rc/software/GCCcore/7.3.0/lib:/cm/shared/apps/cuda92/toolkit/9.2.88/extras/CUPTI/lib64:/cm/local/apps/cuda/libs/current/lib64:/cm/shared/apps/cuda92/toolkit/9.2.88/targets/x86_64-linux/lib:/cm/shared/apps/slurm/18.08.9/lib64/slurm:/cm/shared/apps/slurm/18.08.9/lib64\n",
      "2022-11-22 17:47:51.765058: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2022-11-22 17:48:17.057799: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /share/apps/rc/software/CUDA/9.2.88-GCC-7.3.0-2.30/extras/CUPTI/lib64:/share/apps/rc/software/CUDA/9.2.88-GCC-7.3.0-2.30/lib64:/share/apps/rc/software/binutils/2.30-GCCcore-7.3.0/lib:/share/apps/rc/software/GCCcore/7.3.0/lib64:/share/apps/rc/software/GCCcore/7.3.0/lib:/cm/shared/apps/cuda92/toolkit/9.2.88/extras/CUPTI/lib64:/cm/local/apps/cuda/libs/current/lib64:/cm/shared/apps/cuda92/toolkit/9.2.88/targets/x86_64-linux/lib:/cm/shared/apps/slurm/18.08.9/lib64/slurm:/cm/shared/apps/slurm/18.08.9/lib64\n",
      "2022-11-22 17:48:17.058003: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /share/apps/rc/software/CUDA/9.2.88-GCC-7.3.0-2.30/extras/CUPTI/lib64:/share/apps/rc/software/CUDA/9.2.88-GCC-7.3.0-2.30/lib64:/share/apps/rc/software/binutils/2.30-GCCcore-7.3.0/lib:/share/apps/rc/software/GCCcore/7.3.0/lib64:/share/apps/rc/software/GCCcore/7.3.0/lib:/cm/shared/apps/cuda92/toolkit/9.2.88/extras/CUPTI/lib64:/cm/local/apps/cuda/libs/current/lib64:/cm/shared/apps/cuda92/toolkit/9.2.88/targets/x86_64-linux/lib:/cm/shared/apps/slurm/18.08.9/lib64/slurm:/cm/shared/apps/slurm/18.08.9/lib64\n",
      "2022-11-22 17:48:17.058164: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /share/apps/rc/software/CUDA/9.2.88-GCC-7.3.0-2.30/extras/CUPTI/lib64:/share/apps/rc/software/CUDA/9.2.88-GCC-7.3.0-2.30/lib64:/share/apps/rc/software/binutils/2.30-GCCcore-7.3.0/lib:/share/apps/rc/software/GCCcore/7.3.0/lib64:/share/apps/rc/software/GCCcore/7.3.0/lib:/cm/shared/apps/cuda92/toolkit/9.2.88/extras/CUPTI/lib64:/cm/local/apps/cuda/libs/current/lib64:/cm/shared/apps/cuda92/toolkit/9.2.88/targets/x86_64-linux/lib:/cm/shared/apps/slurm/18.08.9/lib64/slurm:/cm/shared/apps/slurm/18.08.9/lib64\n",
      "2022-11-22 17:48:17.058336: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /share/apps/rc/software/CUDA/9.2.88-GCC-7.3.0-2.30/extras/CUPTI/lib64:/share/apps/rc/software/CUDA/9.2.88-GCC-7.3.0-2.30/lib64:/share/apps/rc/software/binutils/2.30-GCCcore-7.3.0/lib:/share/apps/rc/software/GCCcore/7.3.0/lib64:/share/apps/rc/software/GCCcore/7.3.0/lib:/cm/shared/apps/cuda92/toolkit/9.2.88/extras/CUPTI/lib64:/cm/local/apps/cuda/libs/current/lib64:/cm/shared/apps/cuda92/toolkit/9.2.88/targets/x86_64-linux/lib:/cm/shared/apps/slurm/18.08.9/lib64/slurm:/cm/shared/apps/slurm/18.08.9/lib64\n",
      "2022-11-22 17:48:17.058409: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-11-22 17:48:17.059428: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-22 17:48:21,464 - INFO - tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/sungman/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "2022-11-22 17:48:21,544 - INFO - ngram_models - Found 712 ngrams with min count 4 and (nmin,nmax)=(3,5), first 10: ['UNK', 'PAD', 'ed<S>', 'ng<S>', 'ing', 'er<S>', 'ing<S>', 'on<S>', '<S>co', 'ion'], last 10: ['tly', 'tly<S>', 'cor', 'anc', 'ance', 'ance<S>', '<S>des', 'des', 'sio', 'sion']\n",
      "2022-11-22 17:48:21,545 - INFO - utils - Loading embeddings from ./fcm/wordEmbeddings/glove.6B.50d.txt\n",
      "2022-11-22 18:07:41,916 - INFO - utils - Done loading embeddings\n",
      "2022-11-22 18:07:41,923 - INFO - configuration_utils - loading configuration file ./outputs/BERT_form/config.json\n",
      "2022-11-22 18:07:41,924 - INFO - configuration_utils - Model config {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "2022-11-22 18:07:41,926 - INFO - modeling_utils - loading weights file ./outputs/BERT_form/pytorch_model.bin\n",
      "2022-11-22 18:07:42,205 - INFO - tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/sungman/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "2022-11-22 18:07:42,273 - INFO - ngram_models - Found 712 ngrams with min count 4 and (nmin,nmax)=(3,5), first 10: ['UNK', 'PAD', 'ed<S>', 'ng<S>', 'ing', 'er<S>', 'ing<S>', 'on<S>', '<S>co', 'ion'], last 10: ['tly', 'tly<S>', 'cor', 'anc', 'ance', 'ance<S>', '<S>des', 'des', 'sio', 'sion']\n",
      "2022-11-22 18:07:42,275 - INFO - utils - Loading embeddings from ./fcm/wordEmbeddings/glove.6B.50d.txt\n",
      "2022-11-22 18:27:36,090 - INFO - utils - Done loading embeddings\n",
      "2022-11-22 18:27:36,099 - INFO - configuration_utils - loading configuration file ./outputs/BERT_context/config.json\n",
      "2022-11-22 18:27:36,100 - INFO - configuration_utils - Model config {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "2022-11-22 18:27:36,102 - INFO - modeling_utils - loading weights file ./outputs/BERT_context/pytorch_model.bin\n",
      "2022-11-22 18:27:39,522 - INFO - tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/sungman/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "2022-11-22 18:27:39,574 - INFO - ngram_models - Found 712 ngrams with min count 4 and (nmin,nmax)=(3,5), first 10: ['UNK', 'PAD', 'ed<S>', 'ng<S>', 'ing', 'er<S>', 'ing<S>', 'on<S>', '<S>co', 'ion'], last 10: ['tly', 'tly<S>', 'cor', 'anc', 'ance', 'ance<S>', '<S>des', 'des', 'sio', 'sion']\n",
      "2022-11-22 18:27:39,576 - INFO - utils - Loading embeddings from ./fcm/wordEmbeddings/glove.6B.50d.txt\n",
      "2022-11-22 18:47:49,291 - INFO - utils - Done loading embeddings\n"
     ]
    }
   ],
   "source": [
    "%run fuse_models.py --form_model ./outputs/BERT_form --context_model ./outputs/BERT_context --mode 'add' --output ./outputs/BERT_fused\n",
    "#%run fuse_models.py --form_model ./outputs/BERT_form --context_model ./outputs/BERT_context --mode 'replace' --output ./outputs/BERT_fused"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69950260",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-22 18:56:40,964 - INFO - train - Initializing pretrained BERTRAM instance from ./outputs/BERT_fused.\n",
      "2022-11-22 18:56:41,081 - INFO - tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/sungman/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "2022-11-22 18:56:41,117 - INFO - ngram_models - Found 712 ngrams with min count 4 and (nmin,nmax)=(3,5), first 10: ['UNK', 'PAD', 'ed<S>', 'ng<S>', 'ing', 'er<S>', 'ing<S>', 'on<S>', '<S>co', 'ion'], last 10: ['tly', 'tly<S>', 'cor', 'anc', 'ance', 'ance<S>', '<S>des', 'des', 'sio', 'sion']\n",
      "2022-11-22 18:56:41,118 - INFO - utils - Loading embeddings from ./fcm/wordEmbeddings/glove.6B.50d.txt\n",
      "2022-11-22 19:16:51,989 - INFO - utils - Done loading embeddings\n",
      "2022-11-22 19:16:51,992 - INFO - configuration_utils - loading configuration file ./outputs/BERT_fused/config.json\n",
      "2022-11-22 19:16:51,993 - INFO - configuration_utils - Model config {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "2022-11-22 19:16:51,994 - INFO - modeling_utils - loading weights file ./outputs/BERT_fused/pytorch_model.bin\n",
      "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]2022-11-22 19:17:06,187 - INFO - input_processor - Processing training file 1 of 25: ./training/train.bucket21.txt\n",
      "2022-11-22 19:17:06,226 - INFO - input_processor - Done processing training file, batch size is 106\n",
      "2022-11-22 19:17:06,227 - INFO - input_processor - Processing training file 2 of 25: ./training/train.bucket7.txt\n",
      "2022-11-22 19:17:06,263 - INFO - input_processor - Done processing training file, batch size is 208\n",
      "2022-11-22 19:17:06,263 - INFO - input_processor - Processing training file 3 of 25: ./training/train.bucket4.txt\n",
      "2022-11-22 19:17:06,297 - INFO - input_processor - Done processing training file, batch size is 309\n",
      "2022-11-22 19:17:06,298 - INFO - input_processor - Processing training file 4 of 25: ./training/train.bucket11.txt\n",
      "2022-11-22 19:17:06,346 - INFO - input_processor - Done processing training file, batch size is 387\n",
      "2022-11-22 19:17:06,346 - INFO - input_processor - Processing training file 5 of 25: ./training/train.bucket6.txt\n",
      "2022-11-22 19:17:06,374 - INFO - input_processor - Done processing training file, batch size is 482\n",
      "2022-11-22 19:17:06,375 - INFO - input_processor - Processing training file 6 of 25: ./training/train.bucket3.txt\n",
      "2022-11-22 19:17:06,401 - INFO - input_processor - Done processing training file, batch size is 570\n",
      "2022-11-22 19:17:06,401 - INFO - input_processor - Processing training file 7 of 25: ./training/train.bucket5.txt\n",
      "2022-11-22 19:17:06,460 - INFO - input_processor - Done processing training file, batch size is 667\n",
      "2022-11-22 19:17:06,461 - INFO - input_processor - Processing training file 8 of 25: ./training/train.bucket17.txt\n",
      "2022-11-22 19:17:06,498 - INFO - input_processor - Done processing training file, batch size is 739\n",
      "2022-11-22 19:17:06,499 - INFO - input_processor - Processing training file 9 of 25: ./training/train.bucket2.txt\n",
      "2022-11-22 19:17:06,536 - INFO - input_processor - Done processing training file, batch size is 823\n",
      "2022-11-22 19:17:06,536 - INFO - input_processor - Processing training file 10 of 25: ./training/train.bucket20.txt\n",
      "2022-11-22 19:17:06,581 - INFO - input_processor - Done processing training file, batch size is 931\n",
      "2022-11-22 19:17:06,582 - INFO - input_processor - Processing training file 11 of 25: ./training/train.bucket23.txt\n",
      "2022-11-22 19:17:06,623 - INFO - input_processor - Done processing training file, batch size is 1033\n",
      "2022-11-22 19:17:06,624 - INFO - input_processor - Processing training file 12 of 25: ./training/train.bucket14.txt\n",
      "2022-11-22 19:17:06,659 - INFO - input_processor - Done processing training file, batch size is 1125\n",
      "2022-11-22 19:17:06,660 - INFO - input_processor - Processing training file 13 of 25: ./training/train.bucket24.txt\n",
      "2022-11-22 19:17:06,722 - INFO - input_processor - Done processing training file, batch size is 1234\n",
      "2022-11-22 19:17:06,722 - INFO - input_processor - Processing training file 14 of 25: ./training/train.bucket16.txt\n",
      "2022-11-22 19:17:06,755 - INFO - input_processor - Done processing training file, batch size is 1312\n",
      "2022-11-22 19:17:06,756 - INFO - input_processor - Processing training file 15 of 25: ./training/train.bucket0.txt\n",
      "2022-11-22 19:17:06,783 - INFO - input_processor - Done processing training file, batch size is 1391\n",
      "2022-11-22 19:17:06,784 - INFO - input_processor - Processing training file 16 of 25: ./training/train.bucket12.txt\n",
      "2022-11-22 19:17:06,823 - INFO - input_processor - Done processing training file, batch size is 1498\n",
      "2022-11-22 19:17:06,823 - INFO - input_processor - Processing training file 17 of 25: ./training/train.bucket9.txt\n",
      "2022-11-22 19:17:06,857 - INFO - input_processor - Done processing training file, batch size is 1590\n",
      "2022-11-22 19:17:06,858 - INFO - input_processor - Processing training file 18 of 25: ./training/train.bucket13.txt\n",
      "2022-11-22 19:17:06,895 - INFO - input_processor - Done processing training file, batch size is 1668\n",
      "2022-11-22 19:17:06,896 - INFO - input_processor - Processing training file 19 of 25: ./training/train.bucket18.txt\n",
      "2022-11-22 19:17:06,933 - INFO - input_processor - Done processing training file, batch size is 1758\n",
      "2022-11-22 19:17:06,933 - INFO - input_processor - Processing training file 20 of 25: ./training/train.bucket1.txt\n",
      "2022-11-22 19:17:06,977 - INFO - input_processor - Done processing training file, batch size is 1871\n",
      "2022-11-22 19:17:06,978 - INFO - input_processor - Processing training file 21 of 25: ./training/train.bucket15.txt\n",
      "2022-11-22 19:17:07,006 - INFO - input_processor - Done processing training file, batch size is 1945\n",
      "2022-11-22 19:17:07,007 - INFO - input_processor - Processing training file 22 of 25: ./training/train.bucket22.txt\n",
      "2022-11-22 19:17:07,044 - INFO - input_processor - Done processing training file, batch size is 2033\n",
      "2022-11-22 19:17:07,044 - INFO - input_processor - Processing training file 23 of 25: ./training/train.bucket10.txt\n",
      "2022-11-22 19:17:07,087 - INFO - input_processor - Done processing training file, batch size is 2135\n",
      "2022-11-22 19:17:07,088 - INFO - input_processor - Processing training file 24 of 25: ./training/train.bucket8.txt\n",
      "2022-11-22 19:17:07,132 - INFO - input_processor - Done processing training file, batch size is 2226\n",
      "2022-11-22 19:17:07,149 - INFO - input_processor - Processing training file 25 of 25: ./training/train.bucket19.txt\n",
      "2022-11-22 19:17:07,185 - INFO - input_processor - Done processing training file, batch size is 2315\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/user/home/sungman/test/NLP Group Project/bertram-master/train.py:309: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(loss, requires_grad = True)\n",
      "/home/sungman/.local/lib/python3.9/site-packages/transformers/optimization.py:146: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1420.)\n",
      "  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-22 19:17:42,942 - INFO - train - Step: 100\tLoss: 0.02622995749115944\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-22 19:18:14,557 - INFO - train - Step: 200\tLoss: 0.12951394397765398\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-22 19:18:46,150 - INFO - train - Step: 300\tLoss: 0.16632862687110900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-22 19:19:17,758 - INFO - train - Step: 400\tLoss: 0.13990258827805518\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-22 19:19:49,474 - INFO - train - Step: 500\tLoss: 0.08392910823225976\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-22 19:20:21,227 - INFO - train - Step: 600\tLoss: 0.06114456575363875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-22 19:20:52,981 - INFO - train - Step: 700\tLoss: 0.05240389514714480\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-22 19:21:14,260 - INFO - input_processor - Reached the end of the dataset\n",
      "2022-11-22 19:21:14,261 - INFO - train - Done with epoch 0\n",
      "Epoch: 100%|██████████| 1/1 [04:08<00:00, 248.52s/it]\n"
     ]
    }
   ],
   "source": [
    "%run train.py \\\n",
    "   --model_cls 'bert' \\\n",
    "   --bert_model ./outputs/BERT_fused \\\n",
    "   --output_dir ./outputs/BERT_fused_test \\\n",
    "   --train_dir ./training/ \\\n",
    "   --vocab ./training/train.vwc100 \\\n",
    "   --emb_file ./fcm/wordEmbeddings/glove.6B.50d.txt \\\n",
    "   --emb_dim 768 \\\n",
    "   --mode 'add' \\\n",
    "   --train_batch_size 32 \\\n",
    "   --max_seq_length 10 \\\n",
    "   --num_train_epochs 1 \\\n",
    "   --smin 4 \\\n",
    "   --smax 32 \\\n",
    "   --optimize_only_combinator \\\n",
    "   --learning_rate 0.01 \\\n",
    "   --dropout 0.1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ce51930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting evaluate\n",
      "  Downloading evaluate-0.3.0-py3-none-any.whl (72 kB)\n",
      "\u001b[K     |████████████████████████████████| 72 kB 1.1 MB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /data/rc/apps/rc/software/Anaconda3/2022.05/lib/python3.9/site-packages (from evaluate) (1.21.5)\n",
      "Requirement already satisfied: pandas in /data/rc/apps/rc/software/Anaconda3/2022.05/lib/python3.9/site-packages (from evaluate) (1.4.2)\n",
      "Requirement already satisfied: dill in /data/user/home/sungman/.local/lib/python3.9/site-packages (from evaluate) (0.3.6)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /data/user/home/sungman/.local/lib/python3.9/site-packages (from evaluate) (0.10.1)\n",
      "Requirement already satisfied: responses<0.19 in /data/user/home/sungman/.local/lib/python3.9/site-packages (from evaluate) (0.18.0)\n",
      "Requirement already satisfied: xxhash in /data/user/home/sungman/.local/lib/python3.9/site-packages (from evaluate) (3.1.0)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /data/rc/apps/rc/software/Anaconda3/2022.05/lib/python3.9/site-packages (from evaluate) (2022.2.0)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /data/user/home/sungman/.local/lib/python3.9/site-packages (from evaluate) (2.7.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /data/user/home/sungman/.local/lib/python3.9/site-packages (from evaluate) (2.28.1)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /data/rc/apps/rc/software/Anaconda3/2022.05/lib/python3.9/site-packages (from evaluate) (4.64.0)\n",
      "Requirement already satisfied: multiprocess in /data/user/home/sungman/.local/lib/python3.9/site-packages (from evaluate) (0.70.14)\n",
      "Requirement already satisfied: packaging in /data/rc/apps/rc/software/Anaconda3/2022.05/lib/python3.9/site-packages (from evaluate) (21.3)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /data/user/home/sungman/.local/lib/python3.9/site-packages (from datasets>=2.0.0->evaluate) (10.0.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /data/rc/apps/rc/software/Anaconda3/2022.05/lib/python3.9/site-packages (from datasets>=2.0.0->evaluate) (6.0)\n",
      "Requirement already satisfied: aiohttp in /data/rc/apps/rc/software/Anaconda3/2022.05/lib/python3.9/site-packages (from datasets>=2.0.0->evaluate) (3.8.1)\n",
      "Requirement already satisfied: filelock in /data/rc/apps/rc/software/Anaconda3/2022.05/lib/python3.9/site-packages (from huggingface-hub>=0.7.0->evaluate) (3.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /data/rc/apps/rc/software/Anaconda3/2022.05/lib/python3.9/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.1.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /data/rc/apps/rc/software/Anaconda3/2022.05/lib/python3.9/site-packages (from packaging->evaluate) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /data/rc/apps/rc/software/Anaconda3/2022.05/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /data/rc/apps/rc/software/Anaconda3/2022.05/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /data/rc/apps/rc/software/Anaconda3/2022.05/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /data/rc/apps/rc/software/Anaconda3/2022.05/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (3.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /data/rc/apps/rc/software/Anaconda3/2022.05/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.6.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /data/rc/apps/rc/software/Anaconda3/2022.05/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /data/rc/apps/rc/software/Anaconda3/2022.05/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (5.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /data/rc/apps/rc/software/Anaconda3/2022.05/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (21.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /data/rc/apps/rc/software/Anaconda3/2022.05/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.2.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /data/rc/apps/rc/software/Anaconda3/2022.05/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /data/rc/apps/rc/software/Anaconda3/2022.05/lib/python3.9/site-packages (from pandas->evaluate) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /data/rc/apps/rc/software/Anaconda3/2022.05/lib/python3.9/site-packages (from pandas->evaluate) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in /data/rc/apps/rc/software/Anaconda3/2022.05/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas->evaluate) (1.16.0)\n",
      "Installing collected packages: evaluate\n",
      "Successfully installed evaluate-0.3.0\n"
     ]
    }
   ],
   "source": [
    "!pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f123bc00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /share/apps/rc/software/Anaconda3/2022.05\n",
      "\n",
      "  added / updated specs:\n",
      "    - transformers\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    conda-22.9.0               |   py39h06a4308_0         884 KB\n",
      "    huggingface_hub-0.11.0     |             py_0         177 KB  huggingface\n",
      "    sacremoses-master          |             py_0         404 KB  huggingface\n",
      "    tokenizers-0.13.0.dev0     |           py39_0         4.5 MB  huggingface\n",
      "    transformers-4.24.0        |             py_0         2.6 MB  huggingface\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:         8.5 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  huggingface_hub    huggingface/noarch::huggingface_hub-0.11.0-py_0\n",
      "  sacremoses         huggingface/noarch::sacremoses-master-py_0\n",
      "  tokenizers         huggingface/linux-64::tokenizers-0.13.0.dev0-py39_0\n",
      "  transformers       huggingface/noarch::transformers-4.24.0-py_0\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  conda                               4.14.0-py39h06a4308_0 --> 22.9.0-py39h06a4308_0\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "tokenizers-0.13.0.de | 4.5 MB    | ##################################### | 100% \n",
      "transformers-4.24.0  | 2.6 MB    | ##################################### | 100% \n",
      "conda-22.9.0         | 884 KB    | ##################################### | 100% \n",
      "sacremoses-master    | 404 KB    | ##################################### | 100% \n",
      "huggingface_hub-0.11 | 177 KB    | ##################################### | 100% \n",
      "Preparing transaction: done\n",
      "Verifying transaction: failed\n",
      "\n",
      "EnvironmentNotWritableError: The current user does not have write permissions to the target environment.\n",
      "  environment location: /share/apps/rc/software/Anaconda3/2022.05\n",
      "  uid: 12129\n",
      "  gid: 12129\n",
      "\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda install -c huggingface transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f923f99b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement transformers==4.25.0.dev0 (from versions: 0.1, 2.0.0, 2.1.0, 2.1.1, 2.2.0, 2.2.1, 2.2.2, 2.3.0, 2.4.0, 2.4.1, 2.5.0, 2.5.1, 2.6.0, 2.7.0, 2.8.0, 2.9.0, 2.9.1, 2.10.0, 2.11.0, 3.0.0, 3.0.1, 3.0.2, 3.1.0, 3.2.0, 3.3.0, 3.3.1, 3.4.0, 3.5.0, 3.5.1, 4.0.0rc1, 4.0.0, 4.0.1, 4.1.0, 4.1.1, 4.2.0, 4.2.1, 4.2.2, 4.3.0rc1, 4.3.0, 4.3.1, 4.3.2, 4.3.3, 4.4.0, 4.4.1, 4.4.2, 4.5.0, 4.5.1, 4.6.0, 4.6.1, 4.7.0, 4.8.0, 4.8.1, 4.8.2, 4.9.0, 4.9.1, 4.9.2, 4.10.0, 4.10.1, 4.10.2, 4.10.3, 4.11.0, 4.11.1, 4.11.2, 4.11.3, 4.12.0, 4.12.1, 4.12.2, 4.12.3, 4.12.4, 4.12.5, 4.13.0, 4.14.0, 4.14.1, 4.15.0, 4.16.0, 4.16.1, 4.16.2, 4.17.0, 4.18.0, 4.19.0, 4.19.1, 4.19.2, 4.19.3, 4.19.4, 4.20.0, 4.20.1, 4.21.0, 4.21.1, 4.21.2, 4.21.3, 4.22.0, 4.22.1, 4.22.2, 4.23.0, 4.23.1, 4.24.0)\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for transformers==4.25.0.dev0\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#pip install transformers==4.25.0.dev0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52adb09b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.24.0\n",
      "1.13.0+cu117\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(transformers.__version__)\n",
    "print(torch. __version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5682a877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/23/2022 21:38:46 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "11/23/2022 21:38:46 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=2e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=passive,\n",
      "log_level_replica=passive,\n",
      "log_on_each_node=True,\n",
      "logging_dir=./outputs/SST-2/runs/Nov23_21-38-46_c0099,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=3.0,\n",
      "optim=adamw_hf,\n",
      "output_dir=./outputs/SST-2/,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=8,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=./outputs/SST-2/,\n",
      "save_on_each_node=False,\n",
      "save_steps=500,\n",
      "save_strategy=steps,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n",
      "11/23/2022 21:38:47 - INFO - datasets.info - Loading Dataset Infos from /home/sungman/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
      "11/23/2022 21:38:47 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n",
      "11/23/2022 21:38:47 - INFO - datasets.info - Loading Dataset info from /home/sungman/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
      "11/23/2022 21:38:47 - WARNING - datasets.builder - Found cached dataset glue (/home/sungman/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "11/23/2022 21:38:47 - INFO - datasets.info - Loading Dataset info from /home/sungman/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d835a98fd894136a49ce5dd6b896c39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bfafde42b0e435f892b47f32e76fd30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:654] 2022-11-23 21:38:48,270 >> loading configuration file config.json from cache at /home/sungman/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/config.json\n",
      "[INFO|configuration_utils.py:706] 2022-11-23 21:38:48,312 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"finetuning_task\": \"sst2\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e5612b7798e46a19600cf505907bca9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:654] 2022-11-23 21:38:48,518 >> loading configuration file config.json from cache at /home/sungman/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/config.json\n",
      "[INFO|configuration_utils.py:706] 2022-11-23 21:38:48,519 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ccb79f9c6974d04ae6314aa973ddde0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b24783cdca64e95996cf9a57182cb56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|tokenization_utils_base.py:1775] 2022-11-23 21:38:49,266 >> loading file vocab.txt from cache at /home/sungman/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/vocab.txt\n",
      "[INFO|tokenization_utils_base.py:1775] 2022-11-23 21:38:49,267 >> loading file tokenizer.json from cache at /home/sungman/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1775] 2022-11-23 21:38:49,267 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1775] 2022-11-23 21:38:49,267 >> loading file special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1775] 2022-11-23 21:38:49,268 >> loading file tokenizer_config.json from cache at /home/sungman/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/tokenizer_config.json\n",
      "[INFO|configuration_utils.py:654] 2022-11-23 21:38:49,269 >> loading configuration file config.json from cache at /home/sungman/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/config.json\n",
      "[INFO|configuration_utils.py:706] 2022-11-23 21:38:49,270 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59487a2c12054c47a2631f6630aec783",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/436M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:2158] 2022-11-23 21:38:55,413 >> loading weights file pytorch_model.bin from cache at /home/sungman/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/pytorch_model.bin\n",
      "[WARNING|modeling_utils.py:2598] 2022-11-23 21:38:56,922 >> Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[WARNING|modeling_utils.py:2610] 2022-11-23 21:38:56,923 >> Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a916984935349ccbb2a5a97d50cf159",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/68 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/23/2022 21:38:57 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/sungman/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-7d09cb53be33ffd9.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b700b5bb6d943269df33da79e270b5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/23/2022 21:39:01 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/sungman/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-8cdcbca3a41c6122.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b16023d9d457466ab085fccdf97e898c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/23/2022 21:39:01 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/sungman/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-39b84fc68d6df6b3.arrow\n",
      "11/23/2022 21:39:01 - INFO - __main__ - Sample 14592 of the training set: {'sentence': 'a great movie ', 'label': 1, 'idx': 14592, 'input_ids': [101, 170, 1632, 2523, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
      "11/23/2022 21:39:01 - INFO - __main__ - Sample 3278 of the training set: {'sentence': 'entertaining , if somewhat standardized , action ', 'label': 1, 'idx': 3278, 'input_ids': [101, 15021, 117, 1191, 4742, 18013, 117, 2168, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
      "11/23/2022 21:39:01 - INFO - __main__ - Sample 36048 of the training set: {'sentence': 'even when there are lulls , the emotions seem authentic , ', 'label': 1, 'idx': 36048, 'input_ids': [101, 1256, 1165, 1175, 1132, 181, 11781, 1116, 117, 1103, 6288, 3166, 16047, 117, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd9d15416be74f7e904c6daebd7f94ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/5.75k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING|training_args.py:1292] 2022-11-23 21:39:08,006 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "[WARNING|training_args.py:1292] 2022-11-23 21:39:08,007 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "[INFO|trainer.py:725] 2022-11-23 21:39:08,007 >> The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "[WARNING|training_args.py:1292] 2022-11-23 21:39:08,011 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "/home/sungman/.local/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "[INFO|trainer.py:1608] 2022-11-23 21:39:08,036 >> ***** Running training *****\n",
      "[INFO|trainer.py:1609] 2022-11-23 21:39:08,036 >>   Num examples = 67349\n",
      "[INFO|trainer.py:1610] 2022-11-23 21:39:08,036 >>   Num Epochs = 3\n",
      "[INFO|trainer.py:1611] 2022-11-23 21:39:08,037 >>   Instantaneous batch size per device = 8\n",
      "[INFO|trainer.py:1612] 2022-11-23 21:39:08,037 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1613] 2022-11-23 21:39:08,037 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1614] 2022-11-23 21:39:08,038 >>   Total optimization steps = 6315\n",
      "[INFO|trainer.py:1615] 2022-11-23 21:39:08,039 >>   Number of trainable parameters = 108311810\n",
      "[WARNING|training_args.py:1292] 2022-11-23 21:39:08,045 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6315' max='6315' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6315/6315 39:48, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.309200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.225500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.187700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.172600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.122000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.107000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.112500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.111600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.080500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.069400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.073400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.072400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:2678] 2022-11-23 21:42:19,903 >> Saving model checkpoint to ./outputs/SST-2/checkpoint-500\n",
      "[INFO|configuration_utils.py:447] 2022-11-23 21:42:19,907 >> Configuration saved in ./outputs/SST-2/checkpoint-500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-23 21:42:20,312 >> Model weights saved in ./outputs/SST-2/checkpoint-500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-23 21:42:20,314 >> tokenizer config file saved in ./outputs/SST-2/checkpoint-500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-23 21:42:20,315 >> Special tokens file saved in ./outputs/SST-2/checkpoint-500/special_tokens_map.json\n",
      "[INFO|trainer.py:2678] 2022-11-23 21:45:30,033 >> Saving model checkpoint to ./outputs/SST-2/checkpoint-1000\n",
      "[INFO|configuration_utils.py:447] 2022-11-23 21:45:30,036 >> Configuration saved in ./outputs/SST-2/checkpoint-1000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-23 21:45:30,440 >> Model weights saved in ./outputs/SST-2/checkpoint-1000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-23 21:45:30,443 >> tokenizer config file saved in ./outputs/SST-2/checkpoint-1000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-23 21:45:30,444 >> Special tokens file saved in ./outputs/SST-2/checkpoint-1000/special_tokens_map.json\n",
      "[INFO|trainer.py:2678] 2022-11-23 21:48:39,481 >> Saving model checkpoint to ./outputs/SST-2/checkpoint-1500\n",
      "[INFO|configuration_utils.py:447] 2022-11-23 21:48:39,485 >> Configuration saved in ./outputs/SST-2/checkpoint-1500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-23 21:48:39,895 >> Model weights saved in ./outputs/SST-2/checkpoint-1500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-23 21:48:39,897 >> tokenizer config file saved in ./outputs/SST-2/checkpoint-1500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-23 21:48:39,898 >> Special tokens file saved in ./outputs/SST-2/checkpoint-1500/special_tokens_map.json\n",
      "[INFO|trainer.py:2678] 2022-11-23 21:51:49,027 >> Saving model checkpoint to ./outputs/SST-2/checkpoint-2000\n",
      "[INFO|configuration_utils.py:447] 2022-11-23 21:51:49,029 >> Configuration saved in ./outputs/SST-2/checkpoint-2000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-23 21:51:49,437 >> Model weights saved in ./outputs/SST-2/checkpoint-2000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-23 21:51:49,439 >> tokenizer config file saved in ./outputs/SST-2/checkpoint-2000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-23 21:51:49,441 >> Special tokens file saved in ./outputs/SST-2/checkpoint-2000/special_tokens_map.json\n",
      "[INFO|trainer.py:2678] 2022-11-23 21:54:58,696 >> Saving model checkpoint to ./outputs/SST-2/checkpoint-2500\n",
      "[INFO|configuration_utils.py:447] 2022-11-23 21:54:58,699 >> Configuration saved in ./outputs/SST-2/checkpoint-2500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-23 21:54:59,110 >> Model weights saved in ./outputs/SST-2/checkpoint-2500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-23 21:54:59,112 >> tokenizer config file saved in ./outputs/SST-2/checkpoint-2500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-23 21:54:59,113 >> Special tokens file saved in ./outputs/SST-2/checkpoint-2500/special_tokens_map.json\n",
      "[INFO|trainer.py:2678] 2022-11-23 21:58:07,900 >> Saving model checkpoint to ./outputs/SST-2/checkpoint-3000\n",
      "[INFO|configuration_utils.py:447] 2022-11-23 21:58:07,903 >> Configuration saved in ./outputs/SST-2/checkpoint-3000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-23 21:58:08,309 >> Model weights saved in ./outputs/SST-2/checkpoint-3000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-23 21:58:08,311 >> tokenizer config file saved in ./outputs/SST-2/checkpoint-3000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-23 21:58:08,312 >> Special tokens file saved in ./outputs/SST-2/checkpoint-3000/special_tokens_map.json\n",
      "[INFO|trainer.py:2678] 2022-11-23 22:01:17,046 >> Saving model checkpoint to ./outputs/SST-2/checkpoint-3500\n",
      "[INFO|configuration_utils.py:447] 2022-11-23 22:01:17,048 >> Configuration saved in ./outputs/SST-2/checkpoint-3500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-23 22:01:17,456 >> Model weights saved in ./outputs/SST-2/checkpoint-3500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-23 22:01:17,458 >> tokenizer config file saved in ./outputs/SST-2/checkpoint-3500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-23 22:01:17,459 >> Special tokens file saved in ./outputs/SST-2/checkpoint-3500/special_tokens_map.json\n",
      "[INFO|trainer.py:2678] 2022-11-23 22:04:26,086 >> Saving model checkpoint to ./outputs/SST-2/checkpoint-4000\n",
      "[INFO|configuration_utils.py:447] 2022-11-23 22:04:26,088 >> Configuration saved in ./outputs/SST-2/checkpoint-4000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-23 22:04:26,497 >> Model weights saved in ./outputs/SST-2/checkpoint-4000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-23 22:04:26,501 >> tokenizer config file saved in ./outputs/SST-2/checkpoint-4000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-23 22:04:26,502 >> Special tokens file saved in ./outputs/SST-2/checkpoint-4000/special_tokens_map.json\n",
      "[INFO|trainer.py:2678] 2022-11-23 22:07:34,918 >> Saving model checkpoint to ./outputs/SST-2/checkpoint-4500\n",
      "[INFO|configuration_utils.py:447] 2022-11-23 22:07:34,920 >> Configuration saved in ./outputs/SST-2/checkpoint-4500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-23 22:07:35,316 >> Model weights saved in ./outputs/SST-2/checkpoint-4500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-23 22:07:35,318 >> tokenizer config file saved in ./outputs/SST-2/checkpoint-4500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-23 22:07:35,319 >> Special tokens file saved in ./outputs/SST-2/checkpoint-4500/special_tokens_map.json\n",
      "[INFO|trainer.py:2678] 2022-11-23 22:10:43,878 >> Saving model checkpoint to ./outputs/SST-2/checkpoint-5000\n",
      "[INFO|configuration_utils.py:447] 2022-11-23 22:10:43,881 >> Configuration saved in ./outputs/SST-2/checkpoint-5000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-23 22:10:44,292 >> Model weights saved in ./outputs/SST-2/checkpoint-5000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-23 22:10:44,294 >> tokenizer config file saved in ./outputs/SST-2/checkpoint-5000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-23 22:10:44,295 >> Special tokens file saved in ./outputs/SST-2/checkpoint-5000/special_tokens_map.json\n",
      "[INFO|trainer.py:2678] 2022-11-23 22:13:52,945 >> Saving model checkpoint to ./outputs/SST-2/checkpoint-5500\n",
      "[INFO|configuration_utils.py:447] 2022-11-23 22:13:52,947 >> Configuration saved in ./outputs/SST-2/checkpoint-5500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-23 22:13:53,351 >> Model weights saved in ./outputs/SST-2/checkpoint-5500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-23 22:13:53,353 >> tokenizer config file saved in ./outputs/SST-2/checkpoint-5500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-23 22:13:53,354 >> Special tokens file saved in ./outputs/SST-2/checkpoint-5500/special_tokens_map.json\n",
      "[INFO|trainer.py:2678] 2022-11-23 22:17:02,000 >> Saving model checkpoint to ./outputs/SST-2/checkpoint-6000\n",
      "[INFO|configuration_utils.py:447] 2022-11-23 22:17:02,003 >> Configuration saved in ./outputs/SST-2/checkpoint-6000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-23 22:17:02,407 >> Model weights saved in ./outputs/SST-2/checkpoint-6000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-23 22:17:02,411 >> tokenizer config file saved in ./outputs/SST-2/checkpoint-6000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-23 22:17:02,412 >> Special tokens file saved in ./outputs/SST-2/checkpoint-6000/special_tokens_map.json\n",
      "[INFO|trainer.py:1859] 2022-11-23 22:19:01,517 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "[INFO|trainer.py:2678] 2022-11-23 22:19:01,521 >> Saving model checkpoint to ./outputs/SST-2/\n",
      "[INFO|configuration_utils.py:447] 2022-11-23 22:19:01,523 >> Configuration saved in ./outputs/SST-2/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-23 22:19:01,919 >> Model weights saved in ./outputs/SST-2/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-23 22:19:01,921 >> tokenizer config file saved in ./outputs/SST-2/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-23 22:19:01,922 >> Special tokens file saved in ./outputs/SST-2/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  epoch                    =        3.0\n",
      "  total_flos               = 12377463GF\n",
      "  train_loss               =     0.1339\n",
      "  train_runtime            = 0:39:53.47\n",
      "  train_samples            =      67349\n",
      "  train_samples_per_second =     84.416\n",
      "  train_steps_per_second   =      2.638\n",
      "11/23/2022 22:19:01 - INFO - __main__ - *** Evaluate ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:725] 2022-11-23 22:19:01,962 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2929] 2022-11-23 22:19:01,966 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2931] 2022-11-23 22:19:01,966 >>   Num examples = 872\n",
      "[INFO|trainer.py:2934] 2022-11-23 22:19:01,967 >>   Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='109' max='109' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [109/109 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING|training_args.py:1292] 2022-11-23 22:19:05,834 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "[WARNING|training_args.py:1292] 2022-11-23 22:19:05,834 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** eval metrics *****\n",
      "  epoch                   =        3.0\n",
      "  eval_accuracy           =     0.9151\n",
      "  eval_loss               =     0.3067\n",
      "  eval_runtime            = 0:00:03.86\n",
      "  eval_samples            =        872\n",
      "  eval_samples_per_second =    225.694\n",
      "  eval_steps_per_second   =     28.212\n"
     ]
    }
   ],
   "source": [
    "%run run_glue.py \\\n",
    "  --model_name_or_path bert-base-cased \\\n",
    "  --task_name sst2 \\\n",
    "  --do_train \\\n",
    "  --do_eval \\\n",
    "  --max_seq_length 128 \\\n",
    "  --per_gpu_train_batch_size 32 \\\n",
    "  --learning_rate 2e-5 \\\n",
    "  --num_train_epochs 3.0 \\\n",
    "  --output_dir ./outputs/SST-2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b043f605",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|training_args.py:1324] 2022-11-23 22:37:57,940 >> PyTorch: setting up devices\n",
      "[INFO|training_args.py:1152] 2022-11-23 22:37:57,941 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/23/2022 22:37:58 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "11/23/2022 22:37:58 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=2e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=passive,\n",
      "log_level_replica=passive,\n",
      "log_on_each_node=True,\n",
      "logging_dir=./outputs/SST-2-Fused/runs/Nov23_22-37-57_c0099,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "optim=adamw_hf,\n",
      "output_dir=./outputs/SST-2-Fused/,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=8,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=./outputs/SST-2-Fused/,\n",
      "save_on_each_node=False,\n",
      "save_steps=500,\n",
      "save_strategy=steps,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n",
      "11/23/2022 22:37:58 - INFO - datasets.info - Loading Dataset Infos from /home/sungman/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
      "11/23/2022 22:37:58 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n",
      "11/23/2022 22:37:58 - INFO - datasets.info - Loading Dataset info from /home/sungman/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
      "11/23/2022 22:37:58 - WARNING - datasets.builder - Found cached dataset glue (/home/sungman/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "11/23/2022 22:37:58 - INFO - datasets.info - Loading Dataset info from /home/sungman/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cb7d849e8d74f7291c460986e0c3c1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:654] 2022-11-23 22:37:58,741 >> loading configuration file config.json from cache at /home/sungman/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/config.json\n",
      "[INFO|configuration_utils.py:706] 2022-11-23 22:37:58,742 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"finetuning_task\": \"sst2\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:654] 2022-11-23 22:37:58,834 >> loading configuration file config.json from cache at /home/sungman/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/config.json\n",
      "[INFO|configuration_utils.py:706] 2022-11-23 22:37:58,835 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1775] 2022-11-23 22:37:58,836 >> loading file vocab.txt from cache at /home/sungman/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/vocab.txt\n",
      "[INFO|tokenization_utils_base.py:1775] 2022-11-23 22:37:58,837 >> loading file tokenizer.json from cache at /home/sungman/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1775] 2022-11-23 22:37:58,837 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1775] 2022-11-23 22:37:58,837 >> loading file special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1775] 2022-11-23 22:37:58,838 >> loading file tokenizer_config.json from cache at /home/sungman/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/tokenizer_config.json\n",
      "[INFO|configuration_utils.py:654] 2022-11-23 22:37:58,839 >> loading configuration file config.json from cache at /home/sungman/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/config.json\n",
      "[INFO|configuration_utils.py:706] 2022-11-23 22:37:58,840 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:2158] 2022-11-23 22:37:58,863 >> loading weights file pytorch_model.bin from cache at /home/sungman/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/pytorch_model.bin\n",
      "[WARNING|modeling_utils.py:2598] 2022-11-23 22:38:00,261 >> Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[WARNING|modeling_utils.py:2610] 2022-11-23 22:38:00,262 >> Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/23/2022 22:38:00 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/sungman/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-7d09cb53be33ffd9.arrow\n",
      "11/23/2022 22:38:00 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/sungman/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-6f9dcc51cab07f9d.arrow\n",
      "11/23/2022 22:38:00 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/sungman/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-2c9eb267e6aa6e1c.arrow\n",
      "11/23/2022 22:38:00 - INFO - __main__ - Sample 14592 of the training set: {'sentence': 'a great movie ', 'label': 1, 'idx': 14592, 'input_ids': [101, 170, 1632, 2523, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
      "11/23/2022 22:38:00 - INFO - __main__ - Sample 3278 of the training set: {'sentence': 'entertaining , if somewhat standardized , action ', 'label': 1, 'idx': 3278, 'input_ids': [101, 15021, 117, 1191, 4742, 18013, 117, 2168, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
      "11/23/2022 22:38:00 - INFO - __main__ - Sample 36048 of the training set: {'sentence': 'even when there are lulls , the emotions seem authentic , ', 'label': 1, 'idx': 36048, 'input_ids': [101, 1256, 1165, 1175, 1132, 181, 11781, 1116, 117, 1103, 6288, 3166, 16047, 117, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING|training_args.py:1292] 2022-11-23 22:38:00,581 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "[WARNING|training_args.py:1292] 2022-11-23 22:38:00,582 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "[INFO|trainer.py:725] 2022-11-23 22:38:00,583 >> The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "[WARNING|training_args.py:1292] 2022-11-23 22:38:00,585 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "/home/sungman/.local/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "[INFO|trainer.py:1608] 2022-11-23 22:38:00,592 >> ***** Running training *****\n",
      "[INFO|trainer.py:1609] 2022-11-23 22:38:00,592 >>   Num examples = 67349\n",
      "[INFO|trainer.py:1610] 2022-11-23 22:38:00,592 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1611] 2022-11-23 22:38:00,593 >>   Instantaneous batch size per device = 8\n",
      "[INFO|trainer.py:1612] 2022-11-23 22:38:00,593 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1613] 2022-11-23 22:38:00,594 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1614] 2022-11-23 22:38:00,594 >>   Total optimization steps = 2105\n",
      "[INFO|trainer.py:1615] 2022-11-23 22:38:00,595 >>   Number of trainable parameters = 108311810\n",
      "[WARNING|training_args.py:1292] 2022-11-23 22:38:00,601 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2105' max='2105' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2105/2105 13:17, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.309400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.223900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.183400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.167900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:2678] 2022-11-23 22:41:11,138 >> Saving model checkpoint to ./outputs/SST-2-Fused/checkpoint-500\n",
      "[INFO|configuration_utils.py:447] 2022-11-23 22:41:11,141 >> Configuration saved in ./outputs/SST-2-Fused/checkpoint-500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-23 22:41:11,551 >> Model weights saved in ./outputs/SST-2-Fused/checkpoint-500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-23 22:41:11,556 >> tokenizer config file saved in ./outputs/SST-2-Fused/checkpoint-500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-23 22:41:11,558 >> Special tokens file saved in ./outputs/SST-2-Fused/checkpoint-500/special_tokens_map.json\n",
      "[INFO|trainer.py:2678] 2022-11-23 22:44:20,180 >> Saving model checkpoint to ./outputs/SST-2-Fused/checkpoint-1000\n",
      "[INFO|configuration_utils.py:447] 2022-11-23 22:44:20,183 >> Configuration saved in ./outputs/SST-2-Fused/checkpoint-1000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-23 22:44:20,578 >> Model weights saved in ./outputs/SST-2-Fused/checkpoint-1000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-23 22:44:20,580 >> tokenizer config file saved in ./outputs/SST-2-Fused/checkpoint-1000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-23 22:44:20,584 >> Special tokens file saved in ./outputs/SST-2-Fused/checkpoint-1000/special_tokens_map.json\n",
      "[INFO|trainer.py:2678] 2022-11-23 22:47:29,145 >> Saving model checkpoint to ./outputs/SST-2-Fused/checkpoint-1500\n",
      "[INFO|configuration_utils.py:447] 2022-11-23 22:47:29,148 >> Configuration saved in ./outputs/SST-2-Fused/checkpoint-1500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-23 22:47:29,542 >> Model weights saved in ./outputs/SST-2-Fused/checkpoint-1500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-23 22:47:29,545 >> tokenizer config file saved in ./outputs/SST-2-Fused/checkpoint-1500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-23 22:47:29,546 >> Special tokens file saved in ./outputs/SST-2-Fused/checkpoint-1500/special_tokens_map.json\n",
      "[INFO|trainer.py:2678] 2022-11-23 22:50:37,901 >> Saving model checkpoint to ./outputs/SST-2-Fused/checkpoint-2000\n",
      "[INFO|configuration_utils.py:447] 2022-11-23 22:50:37,904 >> Configuration saved in ./outputs/SST-2-Fused/checkpoint-2000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-23 22:50:38,315 >> Model weights saved in ./outputs/SST-2-Fused/checkpoint-2000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-23 22:50:38,320 >> tokenizer config file saved in ./outputs/SST-2-Fused/checkpoint-2000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-23 22:50:38,321 >> Special tokens file saved in ./outputs/SST-2-Fused/checkpoint-2000/special_tokens_map.json\n",
      "[INFO|trainer.py:1859] 2022-11-23 22:51:18,424 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "[INFO|trainer.py:2678] 2022-11-23 22:51:18,428 >> Saving model checkpoint to ./outputs/SST-2-Fused/\n",
      "[INFO|configuration_utils.py:447] 2022-11-23 22:51:18,431 >> Configuration saved in ./outputs/SST-2-Fused/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-23 22:51:18,846 >> Model weights saved in ./outputs/SST-2-Fused/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-23 22:51:18,848 >> tokenizer config file saved in ./outputs/SST-2-Fused/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-23 22:51:18,849 >> Special tokens file saved in ./outputs/SST-2-Fused/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  total_flos               =  4125821GF\n",
      "  train_loss               =     0.2185\n",
      "  train_runtime            = 0:13:17.83\n",
      "  train_samples            =      67349\n",
      "  train_samples_per_second =     84.415\n",
      "  train_steps_per_second   =      2.638\n",
      "11/23/2022 22:51:18 - INFO - __main__ - *** Evaluate ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:725] 2022-11-23 22:51:18,892 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2929] 2022-11-23 22:51:18,895 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2931] 2022-11-23 22:51:18,895 >>   Num examples = 872\n",
      "[INFO|trainer.py:2934] 2022-11-23 22:51:18,895 >>   Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='109' max='109' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [109/109 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING|training_args.py:1292] 2022-11-23 22:51:22,684 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "[WARNING|training_args.py:1292] 2022-11-23 22:51:22,685 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** eval metrics *****\n",
      "  epoch                   =        1.0\n",
      "  eval_accuracy           =     0.9163\n",
      "  eval_loss               =     0.2322\n",
      "  eval_runtime            = 0:00:03.78\n",
      "  eval_samples            =        872\n",
      "  eval_samples_per_second =    230.317\n",
      "  eval_steps_per_second   =      28.79\n"
     ]
    }
   ],
   "source": [
    "%run run_glue.py \\\n",
    "  --model_name_or_path bert-base-cased \\\n",
    "  --task_name sst2 \\\n",
    "  --train_file ./outputs/BERT_fused_test/bertram_config.json \\\n",
    "  --do_train \\\n",
    "  --do_eval \\\n",
    "  --max_seq_length 128 \\\n",
    "  --per_gpu_train_batch_size 32 \\\n",
    "  --learning_rate 2e-5 \\\n",
    "  --num_train_epochs 1.0 \\\n",
    "  --output_dir ./outputs/SST-2-Fused/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b623c9b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|training_args.py:1324] 2022-11-23 22:24:36,637 >> PyTorch: setting up devices\n",
      "[INFO|training_args.py:1152] 2022-11-23 22:24:36,637 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/23/2022 22:24:36 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "11/23/2022 22:24:36 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=2e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=passive,\n",
      "log_level_replica=passive,\n",
      "log_on_each_node=True,\n",
      "logging_dir=./outputs/SST-2-form_e10/runs/Nov23_22-24-36_c0099,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "optim=adamw_hf,\n",
      "output_dir=./outputs/SST-2-form_e10/,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=8,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=./outputs/SST-2-form_e10/,\n",
      "save_on_each_node=False,\n",
      "save_steps=500,\n",
      "save_strategy=steps,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n",
      "11/23/2022 22:24:37 - INFO - datasets.info - Loading Dataset Infos from /home/sungman/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
      "11/23/2022 22:24:37 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n",
      "11/23/2022 22:24:37 - INFO - datasets.info - Loading Dataset info from /home/sungman/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
      "11/23/2022 22:24:37 - WARNING - datasets.builder - Found cached dataset glue (/home/sungman/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "11/23/2022 22:24:37 - INFO - datasets.info - Loading Dataset info from /home/sungman/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e296ff7f5e944aba8629abff6794989",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:654] 2022-11-23 22:24:37,431 >> loading configuration file config.json from cache at /home/sungman/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/config.json\n",
      "[INFO|configuration_utils.py:706] 2022-11-23 22:24:37,432 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"finetuning_task\": \"sst2\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:654] 2022-11-23 22:24:37,524 >> loading configuration file config.json from cache at /home/sungman/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/config.json\n",
      "[INFO|configuration_utils.py:706] 2022-11-23 22:24:37,525 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1775] 2022-11-23 22:24:37,526 >> loading file vocab.txt from cache at /home/sungman/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/vocab.txt\n",
      "[INFO|tokenization_utils_base.py:1775] 2022-11-23 22:24:37,527 >> loading file tokenizer.json from cache at /home/sungman/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1775] 2022-11-23 22:24:37,527 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1775] 2022-11-23 22:24:37,527 >> loading file special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1775] 2022-11-23 22:24:37,528 >> loading file tokenizer_config.json from cache at /home/sungman/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/tokenizer_config.json\n",
      "[INFO|configuration_utils.py:654] 2022-11-23 22:24:37,529 >> loading configuration file config.json from cache at /home/sungman/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/config.json\n",
      "[INFO|configuration_utils.py:706] 2022-11-23 22:24:37,530 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:2158] 2022-11-23 22:24:37,554 >> loading weights file pytorch_model.bin from cache at /home/sungman/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/pytorch_model.bin\n",
      "[WARNING|modeling_utils.py:2598] 2022-11-23 22:24:39,040 >> Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[WARNING|modeling_utils.py:2610] 2022-11-23 22:24:39,041 >> Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/23/2022 22:24:39 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/sungman/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-7d09cb53be33ffd9.arrow\n",
      "11/23/2022 22:24:39 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/sungman/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-6f9dcc51cab07f9d.arrow\n",
      "11/23/2022 22:24:39 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/sungman/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-2c9eb267e6aa6e1c.arrow\n",
      "11/23/2022 22:24:39 - INFO - __main__ - Sample 14592 of the training set: {'sentence': 'a great movie ', 'label': 1, 'idx': 14592, 'input_ids': [101, 170, 1632, 2523, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
      "11/23/2022 22:24:39 - INFO - __main__ - Sample 3278 of the training set: {'sentence': 'entertaining , if somewhat standardized , action ', 'label': 1, 'idx': 3278, 'input_ids': [101, 15021, 117, 1191, 4742, 18013, 117, 2168, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
      "11/23/2022 22:24:39 - INFO - __main__ - Sample 36048 of the training set: {'sentence': 'even when there are lulls , the emotions seem authentic , ', 'label': 1, 'idx': 36048, 'input_ids': [101, 1256, 1165, 1175, 1132, 181, 11781, 1116, 117, 1103, 6288, 3166, 16047, 117, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING|training_args.py:1292] 2022-11-23 22:24:39,350 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "[WARNING|training_args.py:1292] 2022-11-23 22:24:39,351 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "[INFO|trainer.py:725] 2022-11-23 22:24:39,352 >> The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "[WARNING|training_args.py:1292] 2022-11-23 22:24:39,354 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "/home/sungman/.local/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "[INFO|trainer.py:1608] 2022-11-23 22:24:39,369 >> ***** Running training *****\n",
      "[INFO|trainer.py:1609] 2022-11-23 22:24:39,369 >>   Num examples = 67349\n",
      "[INFO|trainer.py:1610] 2022-11-23 22:24:39,369 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1611] 2022-11-23 22:24:39,370 >>   Instantaneous batch size per device = 8\n",
      "[INFO|trainer.py:1612] 2022-11-23 22:24:39,370 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1613] 2022-11-23 22:24:39,371 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1614] 2022-11-23 22:24:39,371 >>   Total optimization steps = 2105\n",
      "[INFO|trainer.py:1615] 2022-11-23 22:24:39,372 >>   Number of trainable parameters = 108311810\n",
      "[WARNING|training_args.py:1292] 2022-11-23 22:24:39,378 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2105' max='2105' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2105/2105 13:13, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.309400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.223900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.183400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.167900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:2678] 2022-11-23 22:27:46,601 >> Saving model checkpoint to ./outputs/SST-2-form_e10/checkpoint-500\n",
      "[INFO|configuration_utils.py:447] 2022-11-23 22:27:46,603 >> Configuration saved in ./outputs/SST-2-form_e10/checkpoint-500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-23 22:27:47,015 >> Model weights saved in ./outputs/SST-2-form_e10/checkpoint-500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-23 22:27:47,017 >> tokenizer config file saved in ./outputs/SST-2-form_e10/checkpoint-500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-23 22:27:47,018 >> Special tokens file saved in ./outputs/SST-2-form_e10/checkpoint-500/special_tokens_map.json\n",
      "[INFO|trainer.py:2678] 2022-11-23 22:30:55,421 >> Saving model checkpoint to ./outputs/SST-2-form_e10/checkpoint-1000\n",
      "[INFO|configuration_utils.py:447] 2022-11-23 22:30:55,423 >> Configuration saved in ./outputs/SST-2-form_e10/checkpoint-1000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-23 22:30:55,815 >> Model weights saved in ./outputs/SST-2-form_e10/checkpoint-1000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-23 22:30:55,817 >> tokenizer config file saved in ./outputs/SST-2-form_e10/checkpoint-1000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-23 22:30:55,818 >> Special tokens file saved in ./outputs/SST-2-form_e10/checkpoint-1000/special_tokens_map.json\n",
      "[INFO|trainer.py:2678] 2022-11-23 22:34:04,207 >> Saving model checkpoint to ./outputs/SST-2-form_e10/checkpoint-1500\n",
      "[INFO|configuration_utils.py:447] 2022-11-23 22:34:04,210 >> Configuration saved in ./outputs/SST-2-form_e10/checkpoint-1500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-23 22:34:04,611 >> Model weights saved in ./outputs/SST-2-form_e10/checkpoint-1500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-23 22:34:04,614 >> tokenizer config file saved in ./outputs/SST-2-form_e10/checkpoint-1500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-23 22:34:04,615 >> Special tokens file saved in ./outputs/SST-2-form_e10/checkpoint-1500/special_tokens_map.json\n",
      "[INFO|trainer.py:2678] 2022-11-23 22:37:13,084 >> Saving model checkpoint to ./outputs/SST-2-form_e10/checkpoint-2000\n",
      "[INFO|configuration_utils.py:447] 2022-11-23 22:37:13,086 >> Configuration saved in ./outputs/SST-2-form_e10/checkpoint-2000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-23 22:37:13,480 >> Model weights saved in ./outputs/SST-2-form_e10/checkpoint-2000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-23 22:37:13,483 >> tokenizer config file saved in ./outputs/SST-2-form_e10/checkpoint-2000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-23 22:37:13,484 >> Special tokens file saved in ./outputs/SST-2-form_e10/checkpoint-2000/special_tokens_map.json\n",
      "[INFO|trainer.py:1859] 2022-11-23 22:37:53,529 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "[INFO|trainer.py:2678] 2022-11-23 22:37:53,533 >> Saving model checkpoint to ./outputs/SST-2-form_e10/\n",
      "[INFO|configuration_utils.py:447] 2022-11-23 22:37:53,535 >> Configuration saved in ./outputs/SST-2-form_e10/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-23 22:37:53,932 >> Model weights saved in ./outputs/SST-2-form_e10/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-23 22:37:53,935 >> tokenizer config file saved in ./outputs/SST-2-form_e10/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-23 22:37:53,937 >> Special tokens file saved in ./outputs/SST-2-form_e10/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  total_flos               =  4125821GF\n",
      "  train_loss               =     0.2185\n",
      "  train_runtime            = 0:13:14.15\n",
      "  train_samples            =      67349\n",
      "  train_samples_per_second =     84.806\n",
      "  train_steps_per_second   =      2.651\n",
      "11/23/2022 22:37:53 - INFO - __main__ - *** Evaluate ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:725] 2022-11-23 22:37:53,979 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2929] 2022-11-23 22:37:53,983 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2931] 2022-11-23 22:37:53,984 >>   Num examples = 872\n",
      "[INFO|trainer.py:2934] 2022-11-23 22:37:53,984 >>   Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='109' max='109' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [109/109 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING|training_args.py:1292] 2022-11-23 22:37:57,770 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "[WARNING|training_args.py:1292] 2022-11-23 22:37:57,771 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** eval metrics *****\n",
      "  epoch                   =        1.0\n",
      "  eval_accuracy           =     0.9163\n",
      "  eval_loss               =     0.2322\n",
      "  eval_runtime            = 0:00:03.78\n",
      "  eval_samples            =        872\n",
      "  eval_samples_per_second =    230.515\n",
      "  eval_steps_per_second   =     28.814\n"
     ]
    }
   ],
   "source": [
    "%run run_glue.py \\\n",
    "  --model_name_or_path bert-base-cased \\\n",
    "  --task_name sst2 \\\n",
    "  --train_file ./outputs/BERT_form-e10/bertram_config.json \\\n",
    "  --do_train \\\n",
    "  --do_eval \\\n",
    "  --max_seq_length 128 \\\n",
    "  --per_gpu_train_batch_size 32 \\\n",
    "  --learning_rate 2e-5 \\\n",
    "  --num_train_epochs 1.0 \\\n",
    "  --output_dir ./outputs/SST-2-form_e10/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f807dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
