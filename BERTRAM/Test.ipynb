{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e46d04a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "device='cuda' if torch.cuda.is_available() else 'cpu' \n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26e63d72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-23 01:36:45.095044: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-23 01:36:48.227024: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-11-23 01:37:00.995556: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /share/apps/rc/software/CUDA/9.2.88-GCC-7.3.0-2.30/extras/CUPTI/lib64:/share/apps/rc/software/CUDA/9.2.88-GCC-7.3.0-2.30/lib64:/share/apps/rc/software/binutils/2.30-GCCcore-7.3.0/lib:/share/apps/rc/software/GCCcore/7.3.0/lib64:/share/apps/rc/software/GCCcore/7.3.0/lib:/cm/shared/apps/cuda92/toolkit/9.2.88/extras/CUPTI/lib64:/cm/local/apps/cuda/libs/current/lib64:/cm/shared/apps/cuda92/toolkit/9.2.88/targets/x86_64-linux/lib:/cm/shared/apps/slurm/18.08.9/lib64/slurm:/cm/shared/apps/slurm/18.08.9/lib64\n",
      "2022-11-23 01:37:00.995745: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /share/apps/rc/software/CUDA/9.2.88-GCC-7.3.0-2.30/extras/CUPTI/lib64:/share/apps/rc/software/CUDA/9.2.88-GCC-7.3.0-2.30/lib64:/share/apps/rc/software/binutils/2.30-GCCcore-7.3.0/lib:/share/apps/rc/software/GCCcore/7.3.0/lib64:/share/apps/rc/software/GCCcore/7.3.0/lib:/cm/shared/apps/cuda92/toolkit/9.2.88/extras/CUPTI/lib64:/cm/local/apps/cuda/libs/current/lib64:/cm/shared/apps/cuda92/toolkit/9.2.88/targets/x86_64-linux/lib:/cm/shared/apps/slurm/18.08.9/lib64/slurm:/cm/shared/apps/slurm/18.08.9/lib64\n",
      "2022-11-23 01:37:00.995758: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2022-11-23 01:37:27.053333: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /share/apps/rc/software/CUDA/9.2.88-GCC-7.3.0-2.30/extras/CUPTI/lib64:/share/apps/rc/software/CUDA/9.2.88-GCC-7.3.0-2.30/lib64:/share/apps/rc/software/binutils/2.30-GCCcore-7.3.0/lib:/share/apps/rc/software/GCCcore/7.3.0/lib64:/share/apps/rc/software/GCCcore/7.3.0/lib:/cm/shared/apps/cuda92/toolkit/9.2.88/extras/CUPTI/lib64:/cm/local/apps/cuda/libs/current/lib64:/cm/shared/apps/cuda92/toolkit/9.2.88/targets/x86_64-linux/lib:/cm/shared/apps/slurm/18.08.9/lib64/slurm:/cm/shared/apps/slurm/18.08.9/lib64\n",
      "2022-11-23 01:37:27.053547: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /share/apps/rc/software/CUDA/9.2.88-GCC-7.3.0-2.30/extras/CUPTI/lib64:/share/apps/rc/software/CUDA/9.2.88-GCC-7.3.0-2.30/lib64:/share/apps/rc/software/binutils/2.30-GCCcore-7.3.0/lib:/share/apps/rc/software/GCCcore/7.3.0/lib64:/share/apps/rc/software/GCCcore/7.3.0/lib:/cm/shared/apps/cuda92/toolkit/9.2.88/extras/CUPTI/lib64:/cm/local/apps/cuda/libs/current/lib64:/cm/shared/apps/cuda92/toolkit/9.2.88/targets/x86_64-linux/lib:/cm/shared/apps/slurm/18.08.9/lib64/slurm:/cm/shared/apps/slurm/18.08.9/lib64\n",
      "2022-11-23 01:37:27.053736: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /share/apps/rc/software/CUDA/9.2.88-GCC-7.3.0-2.30/extras/CUPTI/lib64:/share/apps/rc/software/CUDA/9.2.88-GCC-7.3.0-2.30/lib64:/share/apps/rc/software/binutils/2.30-GCCcore-7.3.0/lib:/share/apps/rc/software/GCCcore/7.3.0/lib64:/share/apps/rc/software/GCCcore/7.3.0/lib:/cm/shared/apps/cuda92/toolkit/9.2.88/extras/CUPTI/lib64:/cm/local/apps/cuda/libs/current/lib64:/cm/shared/apps/cuda92/toolkit/9.2.88/targets/x86_64-linux/lib:/cm/shared/apps/slurm/18.08.9/lib64/slurm:/cm/shared/apps/slurm/18.08.9/lib64\n",
      "2022-11-23 01:37:27.053886: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /share/apps/rc/software/CUDA/9.2.88-GCC-7.3.0-2.30/extras/CUPTI/lib64:/share/apps/rc/software/CUDA/9.2.88-GCC-7.3.0-2.30/lib64:/share/apps/rc/software/binutils/2.30-GCCcore-7.3.0/lib:/share/apps/rc/software/GCCcore/7.3.0/lib64:/share/apps/rc/software/GCCcore/7.3.0/lib:/cm/shared/apps/cuda92/toolkit/9.2.88/extras/CUPTI/lib64:/cm/local/apps/cuda/libs/current/lib64:/cm/shared/apps/cuda92/toolkit/9.2.88/targets/x86_64-linux/lib:/cm/shared/apps/slurm/18.08.9/lib64/slurm:/cm/shared/apps/slurm/18.08.9/lib64\n",
      "2022-11-23 01:37:27.053955: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-11-23 01:37:27.054906: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "\n",
    "import torch\n",
    "from transformers import BertForMaskedLM, BertTokenizer, GPT2Tokenizer\n",
    "\n",
    "from bertram import BertramWrapper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4dc82f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(inp: str, model: BertForMaskedLM, tokenizer: BertTokenizer, k: int = 3) -> List[str]:\n",
    "    \"\"\"\n",
    "    Predict the top-k substitutes for an input text containing a single MASK token.\n",
    "    :param inp: the input text\n",
    "    :param model: a masked language model\n",
    "    :param tokenizer: the tokenizer corresponding to the model\n",
    "    :param k: the number of predictions\n",
    "    :return: the list of top-k substitutes for the MASK token\n",
    "    \"\"\"\n",
    "    kwargs = {'add_prefix_space': True} if isinstance(tokenizer, GPT2Tokenizer) else {}\n",
    "    input_ids = tokenizer.encode(inp, add_special_tokens=True, **kwargs)\n",
    "    mask_idx = input_ids.index(tokenizer.mask_token_id)\n",
    "    input_ids = torch.tensor([input_ids])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        (predictions,) = model(input_ids)\n",
    "\n",
    "    predicted_tokens = []\n",
    "    _, predicted_indices = torch.topk(predictions[0, mask_idx], k)\n",
    "\n",
    "    for predicted_index in predicted_indices:\n",
    "        predicted_token = tokenizer.convert_ids_to_tokens([predicted_index.item()])[0]\n",
    "        predicted_tokens.append(predicted_token)\n",
    "    return predicted_tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a06d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-23 01:37:30,995 - INFO - tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/sungman/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "2022-11-23 01:37:31,063 - INFO - ngram_models - Found 712 ngrams with min count 4 and (nmin,nmax)=(3,5), first 10: ['UNK', 'PAD', 'ed<S>', 'ng<S>', 'ing', 'er<S>', 'ing<S>', 'on<S>', '<S>co', 'ion'], last 10: ['tly', 'tly<S>', 'cor', 'anc', 'ance', 'ance<S>', '<S>des', 'des', 'sio', 'sion']\n",
      "2022-11-23 01:37:31,065 - INFO - utils - Loading embeddings from ./fcm/wordEmbeddings/glove.6B.50d.txt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# load a pre-trained BERTRAM model and the corresponding BERT model\n",
    "#bert_config_fused = BertConfig.from_json_file('/data/user/home/sungman/test/NLP Group Project/bertram-master/outputs/BERT_fused_test/bertram_config.json')\n",
    "'''\n",
    "bert_config_fused = BertConfig.from_json_file('/data/user/home/sungman/test/NLP Group Project/bertram-master/outputs/BERT_form-e10/bertram_config.json')\n",
    "\n",
    "bert_config_fused.output_hidden_states = True\n",
    "bert_config_fused.vocab_size = 32000\n",
    "\n",
    "fused_model = BertForSequenceClassification(bert_config_fused)\n",
    "fused_model.to(device=device)\n",
    "'''\n",
    "fused_model = BertramWrapper('/data/user/home/sungman/test/NLP Group Project/bertram-master/outputs/BERT_form-e10/', device=device)\n",
    "\n",
    "bert = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "words_with_contexts = {\n",
    "    'kumquat': ['litchi, pineapple and kumquat is planned for the greenhouse.', 'kumquat and cranberry sherbet'],\n",
    "    'resigntaion': []\n",
    "}\n",
    "\n",
    "# infer a BERTRAM vector for a single word from it's surface form and contexts\n",
    "print(f'BERTRAM vector for \"kumquat\": {fused_model.infer_vector(\"kumquat\", words_with_contexts[\"kumquat\"])[:5]}')\n",
    "\n",
    "# infer BERTRAM vectors for all words and add them to the transformer's embedding matrix\n",
    "# for each word `w`, this creates a new token `<BERTRAM:w>` that can be used like a regular word\n",
    "fused_model.add_word_vectors_to_model(words_with_contexts, tokenizer, bert)\n",
    "\n",
    "inputs_bert = [\"a kumquat is a [MASK].\", \"'resigntaion' is a misspelling of '[MASK]'.\"]\n",
    "inputs_bertram = [\"a <BERTRAM:kumquat> is a [MASK].\", \"'<BERTRAM:resigntaion>' is a misspelling of '[MASK]'.\"]\n",
    "\n",
    "for input_bert, input_bertram in zip(inputs_bert, inputs_bertram):\n",
    "    bert_predictions = predict(input_bert, bert, tokenizer)\n",
    "    bertram_predictions = predict(input_bertram, bert, tokenizer)\n",
    "    print(f'Input: {input_bert} \\n\\tBERT:    {bert_predictions}\\n\\tBERTRAM: {bertram_predictions}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc90a4a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
