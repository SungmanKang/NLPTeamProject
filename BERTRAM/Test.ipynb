{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e46d04a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "device='cuda' if torch.cuda.is_available() else 'cpu' \n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26e63d72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-23 09:09:11.947789: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-23 09:09:15.746388: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-11-23 09:09:27.698358: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /share/apps/rc/software/CUDA/9.2.88-GCC-7.3.0-2.30/extras/CUPTI/lib64:/share/apps/rc/software/CUDA/9.2.88-GCC-7.3.0-2.30/lib64:/share/apps/rc/software/binutils/2.30-GCCcore-7.3.0/lib:/share/apps/rc/software/GCCcore/7.3.0/lib64:/share/apps/rc/software/GCCcore/7.3.0/lib:/cm/shared/apps/cuda92/toolkit/9.2.88/extras/CUPTI/lib64:/cm/local/apps/cuda/libs/current/lib64:/cm/shared/apps/cuda92/toolkit/9.2.88/targets/x86_64-linux/lib:/cm/shared/apps/slurm/18.08.9/lib64/slurm:/cm/shared/apps/slurm/18.08.9/lib64\n",
      "2022-11-23 09:09:27.698558: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /share/apps/rc/software/CUDA/9.2.88-GCC-7.3.0-2.30/extras/CUPTI/lib64:/share/apps/rc/software/CUDA/9.2.88-GCC-7.3.0-2.30/lib64:/share/apps/rc/software/binutils/2.30-GCCcore-7.3.0/lib:/share/apps/rc/software/GCCcore/7.3.0/lib64:/share/apps/rc/software/GCCcore/7.3.0/lib:/cm/shared/apps/cuda92/toolkit/9.2.88/extras/CUPTI/lib64:/cm/local/apps/cuda/libs/current/lib64:/cm/shared/apps/cuda92/toolkit/9.2.88/targets/x86_64-linux/lib:/cm/shared/apps/slurm/18.08.9/lib64/slurm:/cm/shared/apps/slurm/18.08.9/lib64\n",
      "2022-11-23 09:09:27.698572: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2022-11-23 09:09:53.404589: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /share/apps/rc/software/CUDA/9.2.88-GCC-7.3.0-2.30/extras/CUPTI/lib64:/share/apps/rc/software/CUDA/9.2.88-GCC-7.3.0-2.30/lib64:/share/apps/rc/software/binutils/2.30-GCCcore-7.3.0/lib:/share/apps/rc/software/GCCcore/7.3.0/lib64:/share/apps/rc/software/GCCcore/7.3.0/lib:/cm/shared/apps/cuda92/toolkit/9.2.88/extras/CUPTI/lib64:/cm/local/apps/cuda/libs/current/lib64:/cm/shared/apps/cuda92/toolkit/9.2.88/targets/x86_64-linux/lib:/cm/shared/apps/slurm/18.08.9/lib64/slurm:/cm/shared/apps/slurm/18.08.9/lib64\n",
      "2022-11-23 09:09:53.404809: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /share/apps/rc/software/CUDA/9.2.88-GCC-7.3.0-2.30/extras/CUPTI/lib64:/share/apps/rc/software/CUDA/9.2.88-GCC-7.3.0-2.30/lib64:/share/apps/rc/software/binutils/2.30-GCCcore-7.3.0/lib:/share/apps/rc/software/GCCcore/7.3.0/lib64:/share/apps/rc/software/GCCcore/7.3.0/lib:/cm/shared/apps/cuda92/toolkit/9.2.88/extras/CUPTI/lib64:/cm/local/apps/cuda/libs/current/lib64:/cm/shared/apps/cuda92/toolkit/9.2.88/targets/x86_64-linux/lib:/cm/shared/apps/slurm/18.08.9/lib64/slurm:/cm/shared/apps/slurm/18.08.9/lib64\n",
      "2022-11-23 09:09:53.404990: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /share/apps/rc/software/CUDA/9.2.88-GCC-7.3.0-2.30/extras/CUPTI/lib64:/share/apps/rc/software/CUDA/9.2.88-GCC-7.3.0-2.30/lib64:/share/apps/rc/software/binutils/2.30-GCCcore-7.3.0/lib:/share/apps/rc/software/GCCcore/7.3.0/lib64:/share/apps/rc/software/GCCcore/7.3.0/lib:/cm/shared/apps/cuda92/toolkit/9.2.88/extras/CUPTI/lib64:/cm/local/apps/cuda/libs/current/lib64:/cm/shared/apps/cuda92/toolkit/9.2.88/targets/x86_64-linux/lib:/cm/shared/apps/slurm/18.08.9/lib64/slurm:/cm/shared/apps/slurm/18.08.9/lib64\n",
      "2022-11-23 09:09:53.405147: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /share/apps/rc/software/CUDA/9.2.88-GCC-7.3.0-2.30/extras/CUPTI/lib64:/share/apps/rc/software/CUDA/9.2.88-GCC-7.3.0-2.30/lib64:/share/apps/rc/software/binutils/2.30-GCCcore-7.3.0/lib:/share/apps/rc/software/GCCcore/7.3.0/lib64:/share/apps/rc/software/GCCcore/7.3.0/lib:/cm/shared/apps/cuda92/toolkit/9.2.88/extras/CUPTI/lib64:/cm/local/apps/cuda/libs/current/lib64:/cm/shared/apps/cuda92/toolkit/9.2.88/targets/x86_64-linux/lib:/cm/shared/apps/slurm/18.08.9/lib64/slurm:/cm/shared/apps/slurm/18.08.9/lib64\n",
      "2022-11-23 09:09:53.405217: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-11-23 09:09:53.406160: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "\n",
    "import torch\n",
    "from transformers import BertForMaskedLM, BertTokenizer, GPT2Tokenizer\n",
    "\n",
    "from bertram import BertramWrapper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4dc82f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(inp: str, model: BertForMaskedLM, tokenizer: BertTokenizer, k: int = 3) -> List[str]:\n",
    "    \"\"\"\n",
    "    Predict the top-k substitutes for an input text containing a single MASK token.\n",
    "    :param inp: the input text\n",
    "    :param model: a masked language model\n",
    "    :param tokenizer: the tokenizer corresponding to the model\n",
    "    :param k: the number of predictions\n",
    "    :return: the list of top-k substitutes for the MASK token\n",
    "    \"\"\"\n",
    "    kwargs = {'add_prefix_space': True} if isinstance(tokenizer, GPT2Tokenizer) else {}\n",
    "    input_ids = tokenizer.encode(inp, add_special_tokens=True, **kwargs)\n",
    "    mask_idx = input_ids.index(tokenizer.mask_token_id)\n",
    "    input_ids = torch.tensor([input_ids])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        (predictions,) = model(input_ids)\n",
    "\n",
    "    predicted_tokens = []\n",
    "    _, predicted_indices = torch.topk(predictions[0, mask_idx], k)\n",
    "\n",
    "    for predicted_index in predicted_indices:\n",
    "        predicted_token = tokenizer.convert_ids_to_tokens([predicted_index.item()])[0]\n",
    "        predicted_tokens.append(predicted_token)\n",
    "    return predicted_tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1a06d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-23 09:09:57,203 - INFO - tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/sungman/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "2022-11-23 09:09:57,269 - INFO - ngram_models - Found 712 ngrams with min count 4 and (nmin,nmax)=(3,5), first 10: ['UNK', 'PAD', 'ed<S>', 'ng<S>', 'ing', 'er<S>', 'ing<S>', 'on<S>', '<S>co', 'ion'], last 10: ['tly', 'tly<S>', 'cor', 'anc', 'ance', 'ance<S>', '<S>des', 'des', 'sio', 'sion']\n",
      "2022-11-23 09:09:57,270 - INFO - utils - Loading embeddings from ./fcm/wordEmbeddings/glove.6B.50d.txt\n",
      "2022-11-23 09:29:42,352 - INFO - utils - Done loading embeddings\n",
      "2022-11-23 09:29:42,357 - INFO - configuration_utils - loading configuration file /data/user/home/sungman/test/NLP Group Project/bertram-master/outputs/BERT_form-e10/config.json\n",
      "2022-11-23 09:29:42,358 - INFO - configuration_utils - Model config {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "2022-11-23 09:29:42,360 - INFO - modeling_utils - loading weights file /data/user/home/sungman/test/NLP Group Project/bertram-master/outputs/BERT_form-e10/pytorch_model.bin\n",
      "2022-11-23 09:29:52,422 - INFO - configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/sungman/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
      "2022-11-23 09:29:52,423 - INFO - configuration_utils - Model config {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "2022-11-23 09:29:52,524 - INFO - modeling_utils - loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /home/sungman/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "2022-11-23 09:29:55,709 - INFO - modeling_utils - Weights from pretrained model not used in BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "2022-11-23 09:29:55,824 - INFO - tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/sungman/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "2022-11-23 09:29:56,225 - INFO - tokenization_utils - Adding <BERTRAM:kumquat> to the vocabulary\n",
      "2022-11-23 09:29:56,226 - INFO - tokenization_utils - Adding <BERTRAM:resigntaion> to the vocabulary\n",
      "2022-11-23 09:29:56,226 - INFO - tokenization_utils - Assigning ['<BERTRAM:kumquat>', '<BERTRAM:resigntaion>'] to the additional_special_tokens key of the tokenizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTRAM vector for \"kumquat\": tensor([ 0.2446,  0.4382,  0.1391, -0.2657,  0.4427], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/user/home/sungman/test/NLP Group Project/bertram-master/bertram.py:486: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(embedding, requires_grad = True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: a kumquat is a [MASK]. \n",
      "\tBERT:    ['noun', 'horse', 'dog']\n",
      "\tBERTRAM: ['word', 'letter', 'term']\n",
      "\n",
      "Input: 'resigntaion' is a misspelling of '[MASK]'. \n",
      "\tBERT:    ['john', 'king', 'son']\n",
      "\tBERTRAM: ['water', 's', '[UNK]']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# load a pre-trained BERTRAM model and the corresponding BERT model\n",
    "#bert_config_fused = BertConfig.from_json_file('/data/user/home/sungman/test/NLP Group Project/bertram-master/outputs/BERT_fused_test/bertram_config.json')\n",
    "'''\n",
    "bert_config_fused = BertConfig.from_json_file('/data/user/home/sungman/test/NLP Group Project/bertram-master/outputs/BERT_form-e10/bertram_config.json')\n",
    "\n",
    "bert_config_fused.output_hidden_states = True\n",
    "bert_config_fused.vocab_size = 32000\n",
    "\n",
    "fused_model = BertForSequenceClassification(bert_config_fused)\n",
    "fused_model.to(device=device)\n",
    "'''\n",
    "fused_model = BertramWrapper('/data/user/home/sungman/test/NLP Group Project/bertram-master/outputs/BERT_form-e10/', device=device)\n",
    "\n",
    "bert = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "words_with_contexts = {\n",
    "    'kumquat': ['litchi, pineapple and kumquat is planned for the greenhouse.', 'kumquat and cranberry sherbet'],\n",
    "    'resigntaion': []\n",
    "}\n",
    "\n",
    "# infer a BERTRAM vector for a single word from it's surface form and contexts\n",
    "print(f'BERTRAM vector for \"kumquat\": {fused_model.infer_vector(\"kumquat\", words_with_contexts[\"kumquat\"])[:5]}')\n",
    "\n",
    "# infer BERTRAM vectors for all words and add them to the transformer's embedding matrix\n",
    "# for each word `w`, this creates a new token `<BERTRAM:w>` that can be used like a regular word\n",
    "fused_model.add_word_vectors_to_model(words_with_contexts, tokenizer, bert)\n",
    "\n",
    "inputs_bert = [\"a kumquat is a [MASK].\", \"'resigntaion' is a misspelling of '[MASK]'.\"]\n",
    "inputs_bertram = [\"a <BERTRAM:kumquat> is a [MASK].\", \"'<BERTRAM:resigntaion>' is a misspelling of '[MASK]'.\"]\n",
    "\n",
    "for input_bert, input_bertram in zip(inputs_bert, inputs_bertram):\n",
    "    bert_predictions = predict(input_bert, bert, tokenizer)\n",
    "    bertram_predictions = predict(input_bertram, bert, tokenizer)\n",
    "    print(f'Input: {input_bert} \\n\\tBERT:    {bert_predictions}\\n\\tBERTRAM: {bertram_predictions}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc90a4a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-23 09:29:57,535 - INFO - tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/sungman/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "2022-11-23 09:29:57,588 - INFO - ngram_models - Found 712 ngrams with min count 4 and (nmin,nmax)=(3,5), first 10: ['UNK', 'PAD', 'ed<S>', 'ng<S>', 'ing', 'er<S>', 'ing<S>', 'on<S>', '<S>co', 'ion'], last 10: ['tly', 'tly<S>', 'cor', 'anc', 'ance', 'ance<S>', '<S>des', 'des', 'sio', 'sion']\n",
      "2022-11-23 09:29:57,589 - INFO - utils - Loading embeddings from ./fcm/wordEmbeddings/glove.6B.50d.txt\n",
      "2022-11-23 09:49:28,559 - INFO - utils - Done loading embeddings\n",
      "2022-11-23 09:49:28,569 - INFO - configuration_utils - loading configuration file /data/user/home/sungman/test/NLP Group Project/bertram-master/outputs/BERT_fused_test-e1/config.json\n",
      "2022-11-23 09:49:28,571 - INFO - configuration_utils - Model config {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "2022-11-23 09:49:28,573 - INFO - modeling_utils - loading weights file /data/user/home/sungman/test/NLP Group Project/bertram-master/outputs/BERT_fused_test-e1/pytorch_model.bin\n",
      "2022-11-23 09:49:32,879 - INFO - modeling_utils - Weights of Bertram not initialized from pretrained model: ['bert.embeddings.word_embeddings.weight']\n",
      "2022-11-23 09:49:32,880 - INFO - modeling_utils - Weights from pretrained model not used in Bertram: ['bert.embeddings.word_embeddings.embedding.weight']\n",
      "2022-11-23 09:49:32,882 - INFO - bertram - Reloading with do_setup=True because of missing keys: {'missing_keys': ['bert.embeddings.word_embeddings.weight'], 'unexpected_keys': ['bert.embeddings.word_embeddings.embedding.weight'], 'error_msgs': []}\n",
      "2022-11-23 09:49:32,884 - INFO - configuration_utils - loading configuration file /data/user/home/sungman/test/NLP Group Project/bertram-master/outputs/BERT_fused_test-e1/config.json\n",
      "2022-11-23 09:49:32,885 - INFO - configuration_utils - Model config {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "2022-11-23 09:49:32,885 - INFO - modeling_utils - loading weights file /data/user/home/sungman/test/NLP Group Project/bertram-master/outputs/BERT_fused_test-e1/pytorch_model.bin\n",
      "2022-11-23 09:49:35,719 - INFO - configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/sungman/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
      "2022-11-23 09:49:35,720 - INFO - configuration_utils - Model config {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "2022-11-23 09:49:35,821 - INFO - modeling_utils - loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /home/sungman/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "2022-11-23 09:49:38,945 - INFO - modeling_utils - Weights from pretrained model not used in BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "2022-11-23 09:49:39,065 - INFO - tokenization_utils - loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/sungman/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "2022-11-23 09:49:43,645 - INFO - tokenization_utils - Adding <BERTRAM:kumquat> to the vocabulary\n",
      "2022-11-23 09:49:43,646 - INFO - tokenization_utils - Adding <BERTRAM:resigntaion> to the vocabulary\n",
      "2022-11-23 09:49:43,646 - INFO - tokenization_utils - Assigning ['<BERTRAM:kumquat>', '<BERTRAM:resigntaion>'] to the additional_special_tokens key of the tokenizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "BERTRAM vector for \"kumquat\": tensor([-0.0109,  1.0750, -0.2437,  0.1992,  0.6432], device='cuda:0')\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "Input: a kumquat is a [MASK]. \n",
      "\tBERT:    ['noun', 'horse', 'dog']\n",
      "\tBERTRAM: ['word', 'name', 'letter']\n",
      "\n",
      "Input: 'resigntaion' is a misspelling of '[MASK]'. \n",
      "\tBERT:    ['john', 'king', 'son']\n",
      "\tBERTRAM: ['water', 's', '[UNK]']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fused_model = BertramWrapper('/data/user/home/sungman/test/NLP Group Project/bertram-master/outputs/BERT_fused_test-e1/', device=device)\n",
    "\n",
    "bert = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "words_with_contexts = {\n",
    "    'kumquat': ['litchi, pineapple and kumquat is planned for the greenhouse.', 'kumquat and cranberry sherbet'],\n",
    "    'resigntaion': []\n",
    "}\n",
    "\n",
    "# infer a BERTRAM vector for a single word from it's surface form and contexts\n",
    "print(f'BERTRAM vector for \"kumquat\": {fused_model.infer_vector(\"kumquat\", words_with_contexts[\"kumquat\"])[:5]}')\n",
    "\n",
    "# infer BERTRAM vectors for all words and add them to the transformer's embedding matrix\n",
    "# for each word `w`, this creates a new token `<BERTRAM:w>` that can be used like a regular word\n",
    "fused_model.add_word_vectors_to_model(words_with_contexts, tokenizer, bert)\n",
    "\n",
    "inputs_bert = [\"a kumquat is a [MASK].\", \"'resigntaion' is a misspelling of '[MASK]'.\"]\n",
    "inputs_bertram = [\"a <BERTRAM:kumquat> is a [MASK].\", \"'<BERTRAM:resigntaion>' is a misspelling of '[MASK]'.\"]\n",
    "\n",
    "for input_bert, input_bertram in zip(inputs_bert, inputs_bertram):\n",
    "    bert_predictions = predict(input_bert, bert, tokenizer)\n",
    "    bertram_predictions = predict(input_bertram, bert, tokenizer)\n",
    "    print(f'Input: {input_bert} \\n\\tBERT:    {bert_predictions}\\n\\tBERTRAM: {bertram_predictions}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcd8618",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
