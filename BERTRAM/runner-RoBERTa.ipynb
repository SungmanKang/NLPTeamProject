{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69f71aba",
   "metadata": {},
   "source": [
    "(add folders in training for context and form"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1661a549",
   "metadata": {},
   "source": [
    "## Preprocess data (only run once)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "36121bd0",
   "metadata": {},
   "source": [
    "%run fcm/preprocess.py train --input ./fcm/brown/brown.txt --output ./training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf8a07b",
   "metadata": {},
   "source": [
    "## Train the bertram context-only model (RoBERTa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7dd6b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-16 21:57:23,798 - INFO - train - Initializing new BERTRAM instance from bert-base-uncased.\n",
      "2022-11-16 21:57:23,933 - INFO - ngram_models - Found 712 ngrams with min count 4 and (nmin,nmax)=(3,5), first 10: ['UNK', 'PAD', 'ed<S>', 'ng<S>', 'ing', 'er<S>', 'ing<S>', 'on<S>', '<S>co', 'ion'], last 10: ['tly', 'tly<S>', 'cor', 'anc', 'ance', 'ance<S>', '<S>des', 'des', 'sio', 'sion']\n",
      "2022-11-16 21:57:23,935 - INFO - utils - Loading embeddings from ./fcm/wordEmbeddings/glove.840B.300d.txt\n",
      "2022-11-16 22:47:09,158 - INFO - utils - Done loading embeddings\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing Bertram: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing Bertram from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Bertram from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Bertram were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['linear.bias', 'linear.weight', 'reliability_measure.linear.bias', 'reliability_measure.linear.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/rcorkil2/.conda/envs/base_env/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "Epoch:   0%|          | 0/5 [00:00<?, ?it/s]2022-11-16 22:47:16,117 - INFO - input_processor - Processing training file 1 of 25: ./training/train.bucket16.txt\n",
      "2022-11-16 22:47:16,170 - INFO - input_processor - Done processing training file, batch size is 78\n",
      "2022-11-16 22:47:16,170 - INFO - input_processor - Processing training file 2 of 25: ./training/train.bucket12.txt\n",
      "2022-11-16 22:47:16,219 - INFO - input_processor - Done processing training file, batch size is 185\n",
      "2022-11-16 22:47:16,221 - INFO - input_processor - Processing training file 3 of 25: ./training/train.bucket9.txt\n",
      "2022-11-16 22:47:16,268 - INFO - input_processor - Done processing training file, batch size is 277\n",
      "2022-11-16 22:47:16,269 - INFO - input_processor - Processing training file 4 of 25: ./training/train.bucket19.txt\n",
      "2022-11-16 22:47:16,319 - INFO - input_processor - Done processing training file, batch size is 366\n",
      "2022-11-16 22:47:16,320 - INFO - input_processor - Processing training file 5 of 25: ./training/train.bucket18.txt\n",
      "2022-11-16 22:47:16,367 - INFO - input_processor - Done processing training file, batch size is 456\n",
      "2022-11-16 22:47:16,369 - INFO - input_processor - Processing training file 6 of 25: ./training/train.bucket6.txt\n",
      "2022-11-16 22:47:16,410 - INFO - input_processor - Done processing training file, batch size is 551\n",
      "2022-11-16 22:47:16,412 - INFO - input_processor - Processing training file 7 of 25: ./training/train.bucket5.txt\n",
      "2022-11-16 22:47:16,477 - INFO - input_processor - Done processing training file, batch size is 648\n",
      "2022-11-16 22:47:16,479 - INFO - input_processor - Processing training file 8 of 25: ./training/train.bucket10.txt\n",
      "2022-11-16 22:47:16,529 - INFO - input_processor - Done processing training file, batch size is 750\n",
      "2022-11-16 22:47:16,531 - INFO - input_processor - Processing training file 9 of 25: ./training/train.bucket15.txt\n",
      "2022-11-16 22:47:16,594 - INFO - input_processor - Done processing training file, batch size is 824\n",
      "2022-11-16 22:47:16,597 - INFO - input_processor - Processing training file 10 of 25: ./training/train.bucket21.txt\n",
      "2022-11-16 22:47:16,639 - INFO - input_processor - Done processing training file, batch size is 930\n",
      "2022-11-16 22:47:16,640 - INFO - input_processor - Processing training file 11 of 25: ./training/train.bucket11.txt\n",
      "2022-11-16 22:47:16,681 - INFO - input_processor - Done processing training file, batch size is 1008\n",
      "2022-11-16 22:47:16,683 - INFO - input_processor - Processing training file 12 of 25: ./training/train.bucket17.txt\n",
      "2022-11-16 22:47:16,738 - INFO - input_processor - Done processing training file, batch size is 1080\n",
      "2022-11-16 22:47:16,740 - INFO - input_processor - Processing training file 13 of 25: ./training/train.bucket1.txt\n",
      "2022-11-16 22:47:16,792 - INFO - input_processor - Done processing training file, batch size is 1193\n",
      "2022-11-16 22:47:16,793 - INFO - input_processor - Processing training file 14 of 25: ./training/train.bucket14.txt\n",
      "2022-11-16 22:47:16,835 - INFO - input_processor - Done processing training file, batch size is 1285\n",
      "2022-11-16 22:47:16,836 - INFO - input_processor - Processing training file 15 of 25: ./training/train.bucket22.txt\n",
      "2022-11-16 22:47:16,872 - INFO - input_processor - Done processing training file, batch size is 1373\n",
      "2022-11-16 22:47:16,872 - INFO - input_processor - Processing training file 16 of 25: ./training/train.bucket13.txt\n",
      "2022-11-16 22:47:16,907 - INFO - input_processor - Done processing training file, batch size is 1451\n",
      "2022-11-16 22:47:16,908 - INFO - input_processor - Processing training file 17 of 25: ./training/train.bucket2.txt\n",
      "2022-11-16 22:47:16,952 - INFO - input_processor - Done processing training file, batch size is 1535\n",
      "2022-11-16 22:47:16,953 - INFO - input_processor - Processing training file 18 of 25: ./training/train.bucket23.txt\n",
      "2022-11-16 22:47:16,998 - INFO - input_processor - Done processing training file, batch size is 1637\n",
      "2022-11-16 22:47:16,998 - INFO - input_processor - Processing training file 19 of 25: ./training/train.bucket4.txt\n",
      "2022-11-16 22:47:17,040 - INFO - input_processor - Done processing training file, batch size is 1738\n",
      "2022-11-16 22:47:17,041 - INFO - input_processor - Processing training file 20 of 25: ./training/train.bucket24.txt\n",
      "2022-11-16 22:47:17,093 - INFO - input_processor - Done processing training file, batch size is 1847\n",
      "2022-11-16 22:47:17,094 - INFO - input_processor - Processing training file 21 of 25: ./training/train.bucket7.txt\n",
      "2022-11-16 22:47:17,138 - INFO - input_processor - Done processing training file, batch size is 1949\n",
      "2022-11-16 22:47:17,139 - INFO - input_processor - Processing training file 22 of 25: ./training/train.bucket8.txt\n",
      "2022-11-16 22:47:17,183 - INFO - input_processor - Done processing training file, batch size is 2040\n",
      "2022-11-16 22:47:17,184 - INFO - input_processor - Processing training file 23 of 25: ./training/train.bucket0.txt\n",
      "2022-11-16 22:47:17,252 - INFO - input_processor - Done processing training file, batch size is 2119\n",
      "2022-11-16 22:47:17,253 - INFO - input_processor - Processing training file 24 of 25: ./training/train.bucket3.txt\n",
      "2022-11-16 22:47:17,304 - INFO - input_processor - Done processing training file, batch size is 2207\n",
      "2022-11-16 22:47:17,316 - INFO - input_processor - Processing training file 25 of 25: ./training/train.bucket20.txt\n",
      "2022-11-16 22:47:17,395 - INFO - input_processor - Done processing training file, batch size is 2315\n",
      "Epoch:   0%|          | 0/5 [00:04<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/data/user/home/rcorkil2/classes/NLP/bertram-master/train.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/data/user/home/rcorkil2/classes/NLP/bertram-master/train.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m                 loss = model(input_ids, segment_ids, nrs_of_contexts, mask_positions, input_mask,\n\u001b[0;32m--> 298\u001b[0;31m                              ngram_ids, ngram_lengths, target_vectors)\n\u001b[0m\u001b[1;32m    299\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mn_gpu\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m                     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# mean() to average on multi-gpu.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/base_env/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/user/home/rcorkil2/classes/NLP/bertram-master/bertram.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, token_type_ids, nr_of_contexts, mask_positions, attention_mask, ngram_ids, ngram_lengths, target_vectors)\u001b[0m\n\u001b[1;32m    269\u001b[0m             \u001b[0;31m# get only the mask vector position for each sequence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m             \u001b[0;31m# shape = sum(nr_of_contexts) x emb_dim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m             \u001b[0mmask_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_mask_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_positions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m             \u001b[0;31m# regroup the sequence_output based on given lengths\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/user/home/rcorkil2/classes/NLP/bertram-master/bertram.py\u001b[0m in \u001b[0;36m_get_mask_output\u001b[0;34m(seq, indices)\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_mask_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m         \u001b[0mindices_one_hot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m         \u001b[0mseq_masked\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseq\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mindices_one_hot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[0mmask_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_masked\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "%run train.py --model_cls 'roberta' --bert_model 'roberta-base' --output_dir ./outputs/RoBERTa_context --train_dir ./training/ --vocab ./training/train.vwc100 --emb_file ./fcm/wordEmbeddings/glove.6B.50d.txt --num_train_epochs 5 --emb_dim 768 --max_seq_length 100 --mode context --train_batch_size 32 --no_finetuning --smin 4 --smax 32\n",
    "\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d0294217",
   "metadata": {},
   "source": [
    "2022-11-16 21:57:23,798 - INFO - train - Initializing new BERTRAM instance from bert-base-uncased.\n",
    "2022-11-16 21:57:23,933 - INFO - ngram_models - Found 712 ngrams with min count 4 and (nmin,nmax)=(3,5), first 10: ['UNK', 'PAD', 'ed<S>', 'ng<S>', 'ing', 'er<S>', 'ing<S>', 'on<S>', '<S>co', 'ion'], last 10: ['tly', 'tly<S>', 'cor', 'anc', 'ance', 'ance<S>', '<S>des', 'des', 'sio', 'sion']\n",
    "2022-11-16 21:57:23,935 - INFO - utils - Loading embeddings from ./fcm/wordEmbeddings/glove.840B.300d.txt\n",
    "2022-11-16 22:47:09,158 - INFO - utils - Done loading embeddings\n",
    "Some weights of the model checkpoint at bert-base-uncased were not used when initializing Bertram: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
    "- This IS expected if you are initializing Bertram from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
    "- This IS NOT expected if you are initializing Bertram from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
    "Some weights of Bertram were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['linear.bias', 'linear.weight', 'reliability_measure.linear.bias', 'reliability_measure.linear.weight']\n",
    "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
    "/home/rcorkil2/.conda/envs/base_env/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
    "  FutureWarning,\n",
    "Epoch:   0%|          | 0/5 [00:00<?, ?it/s]2022-11-16 22:47:16,117 - INFO - input_processor - Processing training file 1 of 25: ./training/train.bucket16.txt\n",
    "2022-11-16 22:47:16,170 - INFO - input_processor - Done processing training file, batch size is 78\n",
    "2022-11-16 22:47:16,170 - INFO - input_processor - Processing training file 2 of 25: ./training/train.bucket12.txt\n",
    "2022-11-16 22:47:16,219 - INFO - input_processor - Done processing training file, batch size is 185\n",
    "2022-11-16 22:47:16,221 - INFO - input_processor - Processing training file 3 of 25: ./training/train.bucket9.txt\n",
    "2022-11-16 22:47:16,268 - INFO - input_processor - Done processing training file, batch size is 277\n",
    "2022-11-16 22:47:16,269 - INFO - input_processor - Processing training file 4 of 25: ./training/train.bucket19.txt\n",
    "2022-11-16 22:47:16,319 - INFO - input_processor - Done processing training file, batch size is 366\n",
    "2022-11-16 22:47:16,320 - INFO - input_processor - Processing training file 5 of 25: ./training/train.bucket18.txt\n",
    "2022-11-16 22:47:16,367 - INFO - input_processor - Done processing training file, batch size is 456\n",
    "2022-11-16 22:47:16,369 - INFO - input_processor - Processing training file 6 of 25: ./training/train.bucket6.txt\n",
    "2022-11-16 22:47:16,410 - INFO - input_processor - Done processing training file, batch size is 551\n",
    "2022-11-16 22:47:16,412 - INFO - input_processor - Processing training file 7 of 25: ./training/train.bucket5.txt\n",
    "2022-11-16 22:47:16,477 - INFO - input_processor - Done processing training file, batch size is 648\n",
    "2022-11-16 22:47:16,479 - INFO - input_processor - Processing training file 8 of 25: ./training/train.bucket10.txt\n",
    "2022-11-16 22:47:16,529 - INFO - input_processor - Done processing training file, batch size is 750\n",
    "2022-11-16 22:47:16,531 - INFO - input_processor - Processing training file 9 of 25: ./training/train.bucket15.txt\n",
    "2022-11-16 22:47:16,594 - INFO - input_processor - Done processing training file, batch size is 824\n",
    "2022-11-16 22:47:16,597 - INFO - input_processor - Processing training file 10 of 25: ./training/train.bucket21.txt\n",
    "2022-11-16 22:47:16,639 - INFO - input_processor - Done processing training file, batch size is 930\n",
    "2022-11-16 22:47:16,640 - INFO - input_processor - Processing training file 11 of 25: ./training/train.bucket11.txt\n",
    "2022-11-16 22:47:16,681 - INFO - input_processor - Done processing training file, batch size is 1008\n",
    "2022-11-16 22:47:16,683 - INFO - input_processor - Processing training file 12 of 25: ./training/train.bucket17.txt\n",
    "2022-11-16 22:47:16,738 - INFO - input_processor - Done processing training file, batch size is 1080\n",
    "2022-11-16 22:47:16,740 - INFO - input_processor - Processing training file 13 of 25: ./training/train.bucket1.txt\n",
    "2022-11-16 22:47:16,792 - INFO - input_processor - Done processing training file, batch size is 1193\n",
    "2022-11-16 22:47:16,793 - INFO - input_processor - Processing training file 14 of 25: ./training/train.bucket14.txt\n",
    "2022-11-16 22:47:16,835 - INFO - input_processor - Done processing training file, batch size is 1285\n",
    "2022-11-16 22:47:16,836 - INFO - input_processor - Processing training file 15 of 25: ./training/train.bucket22.txt\n",
    "2022-11-16 22:47:16,872 - INFO - input_processor - Done processing training file, batch size is 1373\n",
    "2022-11-16 22:47:16,872 - INFO - input_processor - Processing training file 16 of 25: ./training/train.bucket13.txt\n",
    "2022-11-16 22:47:16,907 - INFO - input_processor - Done processing training file, batch size is 1451\n",
    "2022-11-16 22:47:16,908 - INFO - input_processor - Processing training file 17 of 25: ./training/train.bucket2.txt\n",
    "2022-11-16 22:47:16,952 - INFO - input_processor - Done processing training file, batch size is 1535\n",
    "2022-11-16 22:47:16,953 - INFO - input_processor - Processing training file 18 of 25: ./training/train.bucket23.txt\n",
    "2022-11-16 22:47:16,998 - INFO - input_processor - Done processing training file, batch size is 1637\n",
    "2022-11-16 22:47:16,998 - INFO - input_processor - Processing training file 19 of 25: ./training/train.bucket4.txt\n",
    "2022-11-16 22:47:17,040 - INFO - input_processor - Done processing training file, batch size is 1738\n",
    "2022-11-16 22:47:17,041 - INFO - input_processor - Processing training file 20 of 25: ./training/train.bucket24.txt\n",
    "2022-11-16 22:47:17,093 - INFO - input_processor - Done processing training file, batch size is 1847\n",
    "2022-11-16 22:47:17,094 - INFO - input_processor - Processing training file 21 of 25: ./training/train.bucket7.txt\n",
    "2022-11-16 22:47:17,138 - INFO - input_processor - Done processing training file, batch size is 1949\n",
    "2022-11-16 22:47:17,139 - INFO - input_processor - Processing training file 22 of 25: ./training/train.bucket8.txt\n",
    "2022-11-16 22:47:17,183 - INFO - input_processor - Done processing training file, batch size is 2040\n",
    "2022-11-16 22:47:17,184 - INFO - input_processor - Processing training file 23 of 25: ./training/train.bucket0.txt\n",
    "2022-11-16 22:47:17,252 - INFO - input_processor - Done processing training file, batch size is 2119\n",
    "2022-11-16 22:47:17,253 - INFO - input_processor - Processing training file 24 of 25: ./training/train.bucket3.txt\n",
    "2022-11-16 22:47:17,304 - INFO - input_processor - Done processing training file, batch size is 2207\n",
    "2022-11-16 22:47:17,316 - INFO - input_processor - Processing training file 25 of 25: ./training/train.bucket20.txt\n",
    "2022-11-16 22:47:17,395 - INFO - input_processor - Done processing training file, batch size is 2315\n",
    "Epoch:   0%|          | 0/5 [00:04<?, ?it/s]\n",
    "---------------------------------------------------------------------------\n",
    "AttributeError                            Traceback (most recent call last)\n",
    "/data/user/home/rcorkil2/classes/NLP/bertram-master/train.py in <module>\n",
    "    361 \n",
    "    362 if __name__ == \"__main__\":\n",
    "--> 363     main(sys.argv[1:])\n",
    "\n",
    "/data/user/home/rcorkil2/classes/NLP/bertram-master/train.py in main(args)\n",
    "    296 \n",
    "    297                 loss = model(input_ids, segment_ids, nrs_of_contexts, mask_positions, input_mask,\n",
    "--> 298                              ngram_ids, ngram_lengths, target_vectors)\n",
    "    299                 if n_gpu > 1:\n",
    "    300                     loss = loss.mean()  # mean() to average on multi-gpu.\n",
    "\n",
    "~/.conda/envs/base_env/lib/python3.7/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)\n",
    "   1128         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n",
    "   1129                 or _global_forward_hooks or _global_forward_pre_hooks):\n",
    "-> 1130             return forward_call(*input, **kwargs)\n",
    "   1131         # Do not call functions when jit is used\n",
    "   1132         full_backward_hooks, non_full_backward_hooks = [], []\n",
    "\n",
    "/data/user/home/rcorkil2/classes/NLP/bertram-master/bertram.py in forward(self, input_ids, token_type_ids, nr_of_contexts, mask_positions, attention_mask, ngram_ids, ngram_lengths, target_vectors)\n",
    "    269             # get only the mask vector position for each sequence\n",
    "    270             # shape = sum(nr_of_contexts) x emb_dim\n",
    "--> 271             mask_output = self._get_mask_output(sequence_output, mask_positions)\n",
    "    272 \n",
    "    273             # regroup the sequence_output based on given lengths\n",
    "\n",
    "/data/user/home/rcorkil2/classes/NLP/bertram-master/bertram.py in _get_mask_output(seq, indices)\n",
    "    299     @staticmethod\n",
    "    300     def _get_mask_output(seq, indices):\n",
    "--> 301         indices_one_hot = torch.zeros([seq.shape[0], seq.shape[1]]).to(seq.device).scatter_(1, indices.unsqueeze(-1), 1)\n",
    "    302         seq_masked = seq * indices_one_hot.unsqueeze(-1)\n",
    "    303         mask_output = torch.sum(seq_masked, dim=1)\n",
    "\n",
    "AttributeError: 'str' object has no attribute 'shape'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95aa137b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "adb95f57",
   "metadata": {},
   "source": [
    "### Train the bertram form-only model (RoBERTa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e948060",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-16 23:20:10,934 - INFO - train - Initializing new BERTRAM instance from roberta-base.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2cf6230ddb44487aabd929675b6c58e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "117c9a48fecb466c9ae86d1928c0c271",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae2532472f3845bbb76e974ac5fce8a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-16 23:20:12,358 - INFO - ngram_models - Found 712 ngrams with min count 4 and (nmin,nmax)=(3,5), first 10: ['UNK', 'PAD', 'ed<S>', 'ng<S>', 'ing', 'er<S>', 'ing<S>', 'on<S>', '<S>co', 'ion'], last 10: ['tly', 'tly<S>', 'cor', 'anc', 'ance', 'ance<S>', '<S>des', 'des', 'sio', 'sion']\n",
      "2022-11-16 23:20:12,361 - INFO - utils - Loading embeddings from ./fcm/wordEmbeddings/glove.840B.300d.txt\n",
      "2022-11-17 00:24:54,914 - INFO - utils - Done loading embeddings\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "314467faafef41c38ffc4213592bf912",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff685061d1ba4cdcbb474ee7825f72d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/501M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing BertramForRoberta: ['roberta.encoder.layer.2.attention.output.LayerNorm.bias', 'roberta.encoder.layer.0.attention.self.key.weight', 'roberta.encoder.layer.6.attention.self.key.weight', 'roberta.encoder.layer.6.attention.output.dense.weight', 'roberta.encoder.layer.0.attention.self.value.bias', 'roberta.encoder.layer.1.attention.self.query.bias', 'roberta.encoder.layer.0.attention.output.LayerNorm.bias', 'roberta.encoder.layer.1.attention.output.dense.weight', 'lm_head.dense.weight', 'roberta.encoder.layer.9.attention.self.key.bias', 'roberta.encoder.layer.9.attention.output.LayerNorm.bias', 'roberta.encoder.layer.11.output.dense.weight', 'roberta.encoder.layer.10.output.LayerNorm.weight', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.bias', 'roberta.encoder.layer.10.output.dense.weight', 'roberta.encoder.layer.5.attention.self.key.weight', 'roberta.encoder.layer.2.output.dense.bias', 'roberta.encoder.layer.1.attention.self.key.weight', 'roberta.encoder.layer.5.attention.self.query.weight', 'roberta.encoder.layer.11.attention.self.key.weight', 'lm_head.dense.bias', 'roberta.encoder.layer.7.output.dense.weight', 'roberta.encoder.layer.4.attention.output.dense.weight', 'roberta.encoder.layer.9.attention.self.query.weight', 'roberta.encoder.layer.10.attention.self.key.weight', 'roberta.encoder.layer.5.attention.self.value.weight', 'roberta.encoder.layer.5.intermediate.dense.bias', 'roberta.encoder.layer.1.attention.output.LayerNorm.weight', 'roberta.encoder.layer.2.attention.self.key.bias', 'roberta.encoder.layer.2.output.LayerNorm.bias', 'roberta.encoder.layer.7.output.LayerNorm.bias', 'roberta.encoder.layer.9.attention.self.value.weight', 'roberta.encoder.layer.5.attention.self.query.bias', 'roberta.encoder.layer.2.attention.self.query.bias', 'roberta.encoder.layer.6.attention.output.LayerNorm.weight', 'roberta.encoder.layer.8.attention.output.LayerNorm.bias', 'roberta.encoder.layer.4.attention.self.key.bias', 'roberta.encoder.layer.11.output.dense.bias', 'roberta.encoder.layer.5.attention.self.key.bias', 'roberta.encoder.layer.11.attention.output.LayerNorm.bias', 'roberta.encoder.layer.5.attention.output.LayerNorm.weight', 'roberta.encoder.layer.3.attention.self.value.weight', 'roberta.encoder.layer.7.attention.output.LayerNorm.weight', 'roberta.encoder.layer.1.intermediate.dense.bias', 'roberta.encoder.layer.11.intermediate.dense.weight', 'roberta.encoder.layer.10.attention.self.query.weight', 'roberta.encoder.layer.7.attention.self.key.weight', 'roberta.encoder.layer.0.output.LayerNorm.weight', 'roberta.encoder.layer.3.intermediate.dense.bias', 'roberta.encoder.layer.6.intermediate.dense.bias', 'roberta.encoder.layer.8.attention.output.LayerNorm.weight', 'roberta.encoder.layer.8.intermediate.dense.bias', 'roberta.encoder.layer.9.intermediate.dense.bias', 'roberta.encoder.layer.5.intermediate.dense.weight', 'roberta.encoder.layer.0.attention.self.query.weight', 'roberta.encoder.layer.7.attention.self.query.bias', 'roberta.encoder.layer.10.attention.self.key.bias', 'roberta.encoder.layer.9.attention.output.LayerNorm.weight', 'roberta.encoder.layer.4.output.dense.bias', 'roberta.encoder.layer.3.attention.self.query.weight', 'roberta.encoder.layer.7.attention.self.value.weight', 'roberta.encoder.layer.8.attention.self.key.bias', 'roberta.encoder.layer.8.attention.self.query.bias', 'roberta.encoder.layer.6.intermediate.dense.weight', 'roberta.encoder.layer.1.attention.output.dense.bias', 'roberta.encoder.layer.3.attention.self.query.bias', 'roberta.encoder.layer.8.attention.self.value.weight', 'roberta.encoder.layer.6.output.dense.bias', 'roberta.encoder.layer.3.attention.output.dense.bias', 'roberta.encoder.layer.3.output.dense.weight', 'roberta.encoder.layer.3.output.LayerNorm.bias', 'roberta.encoder.layer.9.output.dense.weight', 'roberta.encoder.layer.6.output.LayerNorm.bias', 'roberta.encoder.layer.11.attention.self.value.bias', 'roberta.encoder.layer.8.intermediate.dense.weight', 'roberta.encoder.layer.0.attention.self.query.bias', 'lm_head.decoder.weight', 'roberta.encoder.layer.11.output.LayerNorm.bias', 'roberta.encoder.layer.7.attention.self.value.bias', 'roberta.encoder.layer.9.intermediate.dense.weight', 'roberta.encoder.layer.10.attention.output.dense.bias', 'roberta.encoder.layer.4.attention.output.dense.bias', 'roberta.embeddings.LayerNorm.bias', 'roberta.encoder.layer.0.attention.output.LayerNorm.weight', 'roberta.encoder.layer.11.attention.self.query.bias', 'roberta.encoder.layer.10.intermediate.dense.bias', 'roberta.encoder.layer.4.output.LayerNorm.weight', 'roberta.encoder.layer.6.attention.output.LayerNorm.bias', 'roberta.encoder.layer.9.attention.output.dense.bias', 'roberta.embeddings.LayerNorm.weight', 'roberta.encoder.layer.10.output.dense.bias', 'roberta.encoder.layer.8.output.LayerNorm.bias', 'roberta.encoder.layer.5.attention.output.LayerNorm.bias', 'roberta.encoder.layer.9.attention.output.dense.weight', 'roberta.encoder.layer.1.output.dense.bias', 'roberta.encoder.layer.8.attention.self.key.weight', 'roberta.encoder.layer.11.attention.self.key.bias', 'roberta.encoder.layer.5.attention.output.dense.bias', 'roberta.encoder.layer.2.attention.self.value.bias', 'roberta.encoder.layer.1.attention.self.value.bias', 'roberta.encoder.layer.6.attention.self.key.bias', 'roberta.encoder.layer.6.attention.self.query.weight', 'roberta.encoder.layer.9.attention.self.value.bias', 'roberta.encoder.layer.10.attention.self.query.bias', 'roberta.encoder.layer.9.attention.self.query.bias', 'roberta.embeddings.word_embeddings.weight', 'lm_head.layer_norm.weight', 'roberta.encoder.layer.6.output.LayerNorm.weight', 'roberta.encoder.layer.3.attention.output.LayerNorm.bias', 'roberta.encoder.layer.10.attention.output.LayerNorm.bias', 'roberta.encoder.layer.1.attention.output.LayerNorm.bias', 'roberta.encoder.layer.7.attention.output.dense.weight', 'roberta.encoder.layer.4.attention.self.value.bias', 'roberta.encoder.layer.9.output.dense.bias', 'roberta.encoder.layer.8.attention.output.dense.bias', 'roberta.encoder.layer.5.attention.self.value.bias', 'roberta.encoder.layer.4.intermediate.dense.bias', 'roberta.encoder.layer.7.output.dense.bias', 'roberta.encoder.layer.0.attention.output.dense.weight', 'roberta.encoder.layer.1.intermediate.dense.weight', 'roberta.encoder.layer.5.output.LayerNorm.bias', 'roberta.encoder.layer.4.attention.self.query.bias', 'roberta.encoder.layer.10.attention.output.LayerNorm.weight', 'roberta.encoder.layer.3.attention.self.value.bias', 'roberta.encoder.layer.2.output.dense.weight', 'roberta.encoder.layer.5.output.dense.bias', 'roberta.encoder.layer.7.output.LayerNorm.weight', 'roberta.encoder.layer.11.attention.output.dense.weight', 'roberta.encoder.layer.11.output.LayerNorm.weight', 'roberta.encoder.layer.0.output.LayerNorm.bias', 'roberta.encoder.layer.2.output.LayerNorm.weight', 'roberta.encoder.layer.4.intermediate.dense.weight', 'roberta.encoder.layer.2.intermediate.dense.bias', 'roberta.encoder.layer.6.attention.self.value.bias', 'roberta.encoder.layer.3.output.LayerNorm.weight', 'roberta.pooler.dense.weight', 'roberta.encoder.layer.0.intermediate.dense.bias', 'roberta.encoder.layer.2.attention.output.dense.bias', 'roberta.encoder.layer.4.attention.self.key.weight', 'roberta.encoder.layer.8.output.dense.weight', 'roberta.encoder.layer.10.attention.output.dense.weight', 'roberta.encoder.layer.1.attention.self.key.bias', 'roberta.encoder.layer.9.output.LayerNorm.weight', 'roberta.encoder.layer.5.attention.output.dense.weight', 'roberta.encoder.layer.5.output.LayerNorm.weight', 'roberta.encoder.layer.8.attention.self.query.weight', 'roberta.encoder.layer.8.attention.self.value.bias', 'roberta.encoder.layer.4.attention.self.query.weight', 'roberta.encoder.layer.3.intermediate.dense.weight', 'roberta.embeddings.position_embeddings.weight', 'roberta.encoder.layer.2.attention.output.LayerNorm.weight', 'roberta.encoder.layer.11.intermediate.dense.bias', 'roberta.encoder.layer.1.output.LayerNorm.bias', 'roberta.encoder.layer.0.attention.output.dense.bias', 'roberta.encoder.layer.1.attention.self.query.weight', 'roberta.encoder.layer.9.attention.self.key.weight', 'roberta.encoder.layer.4.attention.output.LayerNorm.bias', 'roberta.encoder.layer.2.attention.self.query.weight', 'roberta.encoder.layer.1.attention.self.value.weight', 'roberta.embeddings.token_type_embeddings.weight', 'roberta.encoder.layer.5.output.dense.weight', 'roberta.encoder.layer.2.attention.self.key.weight', 'roberta.encoder.layer.11.attention.output.dense.bias', 'roberta.encoder.layer.6.attention.output.dense.bias', 'roberta.encoder.layer.10.output.LayerNorm.bias', 'roberta.encoder.layer.2.intermediate.dense.weight', 'roberta.encoder.layer.10.attention.self.value.bias', 'roberta.encoder.layer.4.attention.self.value.weight', 'roberta.encoder.layer.7.attention.self.query.weight', 'roberta.encoder.layer.7.attention.output.LayerNorm.bias', 'roberta.encoder.layer.3.output.dense.bias', 'roberta.encoder.layer.2.attention.self.value.weight', 'roberta.encoder.layer.7.intermediate.dense.bias', 'roberta.encoder.layer.8.output.LayerNorm.weight', 'roberta.encoder.layer.7.attention.output.dense.bias', 'roberta.encoder.layer.10.intermediate.dense.weight', 'roberta.encoder.layer.0.output.dense.weight', 'roberta.encoder.layer.6.attention.self.value.weight', 'roberta.encoder.layer.11.attention.output.LayerNorm.weight', 'roberta.encoder.layer.9.output.LayerNorm.bias', 'roberta.encoder.layer.8.output.dense.bias', 'roberta.encoder.layer.3.attention.output.dense.weight', 'roberta.encoder.layer.3.attention.self.key.weight', 'roberta.encoder.layer.1.output.dense.weight', 'roberta.encoder.layer.1.output.LayerNorm.weight', 'roberta.encoder.layer.4.output.LayerNorm.bias', 'roberta.encoder.layer.11.attention.self.value.weight', 'roberta.encoder.layer.6.output.dense.weight', 'roberta.encoder.layer.6.attention.self.query.bias', 'roberta.encoder.layer.7.intermediate.dense.weight', 'roberta.encoder.layer.4.attention.output.LayerNorm.weight', 'roberta.encoder.layer.2.attention.output.dense.weight', 'roberta.encoder.layer.4.output.dense.weight', 'roberta.encoder.layer.8.attention.output.dense.weight', 'roberta.encoder.layer.0.output.dense.bias', 'roberta.encoder.layer.3.attention.self.key.bias', 'roberta.encoder.layer.11.attention.self.query.weight', 'roberta.encoder.layer.0.attention.self.key.bias', 'roberta.encoder.layer.0.intermediate.dense.weight', 'roberta.encoder.layer.10.attention.self.value.weight', 'roberta.encoder.layer.0.attention.self.value.weight', 'roberta.encoder.layer.3.attention.output.LayerNorm.weight', 'roberta.encoder.layer.7.attention.self.key.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing BertramForRoberta from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertramForRoberta from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertramForRoberta were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.ngram_processor.ngram_embeddings.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'BertramForRoberta' object has no attribute 'transformer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/data/user/home/rcorkil2/classes/NLP/bertram-master/train.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/data/user/home/rcorkil2/classes/NLP/bertram-master/train.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0minput_processor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/user/home/rcorkil2/classes/NLP/bertram-master/bertram.py\u001b[0m in \u001b[0;36msetup\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0;34m\"\"\"Initialize the BERTRAM model and put a wrapper around the underlying transformer's embedding layer\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0mform_and_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequires_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbertram_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mrequires_form\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbertram_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOverwriteableEmbedding\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mform_and_context\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m             \u001b[0mword_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOverwriteableEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_embeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/base_env/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1206\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1207\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0;32m-> 1208\u001b[0;31m             type(self).__name__, name))\n\u001b[0m\u001b[1;32m   1209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Module'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'BertramForRoberta' object has no attribute 'transformer'"
     ]
    }
   ],
   "source": [
    "%run train.py \\\n",
    "   --model_cls 'roberta' \\\n",
    "   --bert_model 'roberta-base' \\\n",
    "   --output_dir ./outputs/RoBERTa_form \\\n",
    "   --train_dir ./training/ \\\n",
    "   --vocab ./training/train.vwc100 \\\n",
    "   --emb_file ./fcm/wordEmbeddings/glove.6B.50d.txt \\\n",
    "   --num_train_epochs 20 \\\n",
    "   --emb_dim 768 \\\n",
    "   --train_batch_size 32 \\\n",
    "   --smin 1 \\\n",
    "   --smax 1 \\\n",
    "   --max_seq_length 10 \\\n",
    "   --mode form \\\n",
    "   --learning_rate 0.01 \\\n",
    "   --dropout 0.1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64d988f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ef3eb907",
   "metadata": {},
   "source": [
    "### Combine models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f569342",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run fuse_models.py \\\n",
    "   --form_model ./outputs/BERT_form \\ #MAKE THIS\n",
    "   --context_model ./outputs/BERT_context \\ #MAKE THIS\n",
    "   --mode $MODE \\ # ADD or REPLACE --> CHOOSE\n",
    "   --output ./outputs/BERT_fused #MAKE THIS"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
