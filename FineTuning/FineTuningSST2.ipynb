{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1904f91e",
   "metadata": {},
   "source": [
    "## Fine Tuning Transformers on SST2\n",
    "https://www.assemblyai.com/blog/fine-tuning-transformers-for-nlp/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c06e8d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libraries needed\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "#PyTorch dataset class\n",
    "class  SST_Dataset(Dataset):\n",
    "\t#Name: \t\t__init__\n",
    "\t#Purpose: \tinit function to load the dataset\n",
    "\t#Inputs: \tdataset -> dataset\n",
    "\t#Outputs: \tnone\n",
    "\tdef  __init__(self, dataset):\n",
    "\t\tself.dataset = dataset\n",
    "\t\treturn\n",
    "\n",
    "\t#Name: \t\t__len__\n",
    "\t#Purpose: \tget the length of the dataset\n",
    "\t#Inputs: \tnone\n",
    "\t#Outputs: \tlength -> length of the dataset\n",
    "\tdef  __len__(self):\n",
    "\t\treturn  len(self.dataset)\n",
    "\n",
    "\t#Name: \t\t__getitem__\n",
    "\t#Purpose: \tget a random text segment and its label from the dataset\n",
    "\t#Inputs: \tidx -> index of the random text segment to load\n",
    "\t#Outputs: \ttext -> text segment\n",
    "\t# \t\t\tlabel -> sentiment score\n",
    "\tdef  __getitem__(self, idx):\n",
    "\t\ttext =  self.dataset[idx]['sentence']\n",
    "\t\tlabel = torch.zeros(2)\n",
    "\t\tlabel[round(self.dataset[idx]['label'])] =  1\n",
    "\t\treturn text, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff780b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Name: \t\tget_gpu\n",
    "#Purpose: \tchecks if a GPU device is avaliable\n",
    "#Input: \tnone\n",
    "#Output: \tGPU -> GPU device if applicable, none if not\n",
    "def  get_gpu():\n",
    "\t#Check if a GPU is avaliable and if so return it\n",
    "\tGPU  =  None\n",
    "\tif torch.cuda.is_available():\n",
    "\t\tprint(\"Using GPU\")\n",
    "\t\tGPU  = torch.device(\"cuda\")\n",
    "\telse:\n",
    "\t\tprint(\"No GPU device avaliable! Using CPU\")\n",
    "\treturn  GPU\n",
    "\n",
    "#Name: \t\ttransfer_device\n",
    "#Purpose: \ttransfers model / data to the GPU devie if present\n",
    "#Inputs: \tGPU -> GPU device if applicable, none if not\n",
    "# \t\t \tdata -> data to transfer\n",
    "#Output: \tdata -> data that has been transferred if applicable\n",
    "def  transfer_device(GPU, data):\n",
    "\tif(GPU  !=  None):\n",
    "\t\tdata = data.to(GPU)\n",
    "\treturn data\n",
    "\n",
    "#Name: \t\tcount_correct\n",
    "#Purpose: \tcount the number of correct model predictions in a batch\n",
    "#Inputs: \tpredictions -> model predictions\n",
    "#\t\t \ttargets -> target labels\n",
    "#Outputs: \tcorrect -> number of correct model predictions\n",
    "def  count_correct(predictions, targets):\n",
    "\t#Create variables to store the number of correct predictions along with the index of the prediction in the batch\n",
    "\tcorrect =  0\n",
    "\tindex =  0\n",
    "  \n",
    "\t#Loop across all predictions in the batch and count the number correct\n",
    "\twhile(index <  len(predictions)):\n",
    "\t\t#Convert the prediction and target to lists\n",
    "\t\tprediction =  list(predictions[index])\n",
    "\t\ttarget =  list(targets[index])\n",
    "  \n",
    "\t\t#Get the max index indicating the truth value from the prediction and target\n",
    "\t\tprediction_index = prediction.index(max(prediction))\n",
    "\t\ttarget_index = target.index(max(target))\n",
    "  \n",
    "\t\t#If the max indices are the same increment correct\n",
    "\t\tif(prediction_index == target_index):\n",
    "\t\t\tcorrect +=  1\n",
    "\t\tindex +=  1\n",
    "\treturn correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d46beb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Name: \t\tbinary_cross_entropy\n",
    "#Purpose: \tdefines binary cross entropy loss function\n",
    "#Inputs: \tpredictions -> model predictions\n",
    "# \t\t\ttargets -> target labels\n",
    "#Outputs: \tloss -> loss value\n",
    "def  binary_cross_entropy(predictions, targets):\n",
    "\tloss =  -(targets * torch.log(predictions) + (1  - targets) * torch.log(1  - predictions))\n",
    "\tloss = torch.mean(loss)\n",
    "\treturn loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a28694cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F \n",
    "\n",
    "#Name: \t\ttrain_model\n",
    "#Purpose: \ttrain the model while evaluating its performance\n",
    "#Inputs: \tGPU -> GPU device to train / evaluate on\n",
    "# \t\t\ttrain_dataloader -> training set dataloader\n",
    "# \t\t\tdev_dataloader -> development set dataloader\n",
    "# \t\t\ttokenizer -> text tokenizer for model\n",
    "# \t\t\tmodel -> model to train / evaluate\n",
    "# \t\t\toptimizer -> optimizer to use to update model parameters\n",
    "# \t\t\tcriterion -> criterion to use to compute loss values\n",
    "#Outputs: \tmodel -> model after training\n",
    "def  train_model(GPU, train_dataloader, dev_dataloader, tokenizer, model, optimizer, criterion):\n",
    "\t#Evaluate the performance of the model before training\n",
    "\tvalid_loss, valid_accuracy = evaluate(GPU, dev_dataloader, tokenizer, model, criterion)\n",
    "\tprint(\"Pre-training validation loss: \"+str(valid_loss)+\" --- Accuracy: \"+str(valid_accuracy))\n",
    "\tprint()\n",
    "\n",
    "\t#Train the model across 3 epochs and evaluate its performance\n",
    "\tfor epoch in  range(3):\n",
    "\t\tmodel, train_loss, train_accuracy = train(GPU, train_dataloader, tokenizer, model, optimizer, criterion)\n",
    "\t\tvalid_loss, valid_accuracy = evaluate(GPU, dev_dataloader, tokenizer, model, criterion)\n",
    "\n",
    "\t\t#Print performance stats\n",
    "\t\tprint(\" \", end=\"\\r\")\n",
    "\t\tprint(\"Epoch: \"+str(epoch+1))\n",
    "\t\tprint(\"Training loss: \"+str(train_loss)+\" --- Accuracy: \"+str(train_accuracy))\n",
    "\t\tprint(\"Validation loss: \"+str(valid_loss)+\" --- Accuracy: \"+str(valid_accuracy))\n",
    "\t\tprint()\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b3cfcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Name: \t\ttrain\n",
    "#Purpose: \ttrain the model over 1 epoch\n",
    "#Inputs: \tGPU -> GPU device to train on\n",
    "# \t\t\tdataloader -> dataloader\n",
    "# \t\t\ttokenizer -> text tokenizer for model\n",
    "# \t\t\tmodel -> model to train\n",
    "# \t\t\toptimizer -> optimizer to use to update model parameters\n",
    "# \t\t\tcriterion -> criterion to use to compute loss values\n",
    "#Outputs: \tmodel -> model after training over the epoch\n",
    "# \t\t\taverage_loss -> average loss over the epoch\n",
    "# \t\t\taccuracy -> accuracy over the epoch\n",
    "def  train(GPU, dataloader, tokenizer, model, optimizer, criterion):\n",
    "\t#Place the network in training mode, create a variable to store the total loss, and create a variable to store the total number of correct predictions\n",
    "\tmodel.train()\n",
    "\ttotal_loss =  0\n",
    "\ttotal_correct =  0\n",
    "  \n",
    "\t#Loop through all batches in the dataloader\n",
    "\tfor batch_number, (texts, labels) in  enumerate(dataloader):\n",
    "\t\t#Tokenize the text segments, get the model predictions, compute the loss, and add the loss to the total loss\n",
    "\t\ttokenized_segments = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\t\ttokenized_segments_input_ids, tokenized_segments_attention_mask = tokenized_segments.input_ids, tokenized_segments.attention_mask\n",
    "\t\tmodel_predictions = F.softmax(model(input_ids=transfer_device(GPU, tokenized_segments_input_ids), attention_mask=transfer_device(GPU, tokenized_segments_attention_mask))['logits'], dim=1)\n",
    "\t\tloss = criterion(model_predictions, transfer_device(GPU, labels))\n",
    "\t\ttotal_loss += loss.item()\n",
    "  \n",
    "\t\t#Count the number of correct predictions by the model in the batch and add this to the total correct\n",
    "\t\tcorrect = count_correct(model_predictions.cpu().detach().numpy(), labels.numpy())\n",
    "\t\ttotal_correct += correct\n",
    "  \n",
    "\t\t#Zero the optimizer, compute the gradients, and update the model parameters\n",
    "\t\toptimizer.zero_grad()\n",
    "\t\tloss.backward()\n",
    "\t\toptimizer.step()\n",
    "\t\tprint(\"Training batch index: \"+str(batch_number)+\"/\"+str(len(dataloader))+  \" ( \"+str(batch_number/len(dataloader)*100)+\"% )\", end='\\r')\n",
    "  \n",
    "\t#Compute the average loss and accuracy across the epoch\n",
    "\taverage_loss = total_loss /  len(dataloader)\n",
    "\taccuracy = total_correct / dataloader.dataset.__len__()\n",
    "\treturn model, average_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0d7b3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Name: \t\tevaluate\n",
    "#Purpose: \tevaluate the model over 1 epoch\n",
    "#Inputs: \tGPU -> GPU device to evaluate on\n",
    "# \t\t\tdataloader -> dataloader\n",
    "# \t\t\ttokenizer -> text tokenizer for model\n",
    "# \t\t\tmodel -> model to evaluate\n",
    "# \t\t\tcriterion -> criterion to use to compute loss values\n",
    "#Outputs: \taverage_loss -> average loss over the epoch\n",
    "# \t\t\taccuracy -> accuracy over the epoch\n",
    "def  evaluate(GPU, dataloader, tokenizer, model, criterion):\n",
    "\t#Place the network in evaluation mode, create a variable to store the total loss, and create a variable to store the total number of correct predictions\n",
    "\tmodel.eval()\n",
    "\ttotal_loss =  0\n",
    "\ttotal_correct =  0\n",
    "  \n",
    "\t#Loop through all batches in the dataloader\n",
    "\tfor batch_number, (texts, labels) in  enumerate(dataloader):\n",
    "\t\t#Tokenize the text segments, get the model predictions, compute the loss, and add the loss to the total loss\n",
    "\t\ttokenized_segments = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\t\ttokenized_segments_input_ids, tokenized_segments_attention_mask = tokenized_segments.input_ids, tokenized_segments.attention_mask\n",
    "\t\tmodel_predictions = F.softmax(model(input_ids=transfer_device(GPU, tokenized_segments_input_ids), attention_mask=transfer_device(GPU, tokenized_segments_attention_mask))['logits'], dim=1)\n",
    "\t\tloss = criterion(model_predictions, transfer_device(GPU, labels))\n",
    "\t\ttotal_loss += loss.item()\n",
    "  \n",
    "\t\t#Count the number of correct predictions by the model in the batch and add this to the total correct\n",
    "\t\tcorrect = count_correct(model_predictions.cpu().detach().numpy(), labels.numpy())\n",
    "\t\ttotal_correct += correct\n",
    "\t\tprint(\"Evaluation batch index: \"+str(batch_number)+\"/\"+str(len(dataloader))+  \" ( \"+str(batch_number/len(dataloader)*100)+\"% )\", end='\\r')\n",
    "  \n",
    "\t#Compute the average loss and accuracy across the epoch\n",
    "\taverage_loss = total_loss /  len(dataloader)\n",
    "\taccuracy = total_correct / dataloader.dataset.__len__()\n",
    "\treturn average_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ed3bd40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-16 13:09:34.209281: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-16 13:09:37.916037: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /share/apps/rc/software/CUDA/9.2.88-GCC-7.3.0-2.30/extras/CUPTI/lib64:/share/apps/rc/software/CUDA/9.2.88-GCC-7.3.0-2.30/lib64:/share/apps/rc/software/binutils/2.30-GCCcore-7.3.0/lib:/share/apps/rc/software/GCCcore/7.3.0/lib64:/share/apps/rc/software/GCCcore/7.3.0/lib:/cm/shared/apps/cuda92/toolkit/9.2.88/extras/CUPTI/lib64:/cm/local/apps/cuda/libs/current/lib64:/cm/shared/apps/cuda92/toolkit/9.2.88/targets/x86_64-linux/lib:/cm/shared/apps/slurm/18.08.9/lib64/slurm:/cm/shared/apps/slurm/18.08.9/lib64\n",
      "2022-11-16 13:09:37.916092: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-11-16 13:09:38.857897: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-11-16 13:09:52.282336: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /share/apps/rc/software/CUDA/9.2.88-GCC-7.3.0-2.30/extras/CUPTI/lib64:/share/apps/rc/software/CUDA/9.2.88-GCC-7.3.0-2.30/lib64:/share/apps/rc/software/binutils/2.30-GCCcore-7.3.0/lib:/share/apps/rc/software/GCCcore/7.3.0/lib64:/share/apps/rc/software/GCCcore/7.3.0/lib:/cm/shared/apps/cuda92/toolkit/9.2.88/extras/CUPTI/lib64:/cm/local/apps/cuda/libs/current/lib64:/cm/shared/apps/cuda92/toolkit/9.2.88/targets/x86_64-linux/lib:/cm/shared/apps/slurm/18.08.9/lib64/slurm:/cm/shared/apps/slurm/18.08.9/lib64\n",
      "2022-11-16 13:09:52.282548: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /share/apps/rc/software/CUDA/9.2.88-GCC-7.3.0-2.30/extras/CUPTI/lib64:/share/apps/rc/software/CUDA/9.2.88-GCC-7.3.0-2.30/lib64:/share/apps/rc/software/binutils/2.30-GCCcore-7.3.0/lib:/share/apps/rc/software/GCCcore/7.3.0/lib64:/share/apps/rc/software/GCCcore/7.3.0/lib:/cm/shared/apps/cuda92/toolkit/9.2.88/extras/CUPTI/lib64:/cm/local/apps/cuda/libs/current/lib64:/cm/shared/apps/cuda92/toolkit/9.2.88/targets/x86_64-linux/lib:/cm/shared/apps/slurm/18.08.9/lib64/slurm:/cm/shared/apps/slurm/18.08.9/lib64\n",
      "2022-11-16 13:09:52.282561: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset sst (/home/rcorkil2/.cache/huggingface/datasets/sst/default/1.0.0/b8a7889ef01c5d3ae8c379b84cc4080f8aad3ac2bc538701cbe0ac6416fb76ff)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97edb862146f42fd8759c58b018cf3ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rcorkil2/.conda/envs/base_env/lib/python3.7/site-packages/torch/utils/data/dataloader.py:566: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/rcorkil2/.conda/envs/base_env/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-training validation loss: 0.6963417189461845 --- Accuracy: 0.49227974568574023\n",
      "\n",
      "Epoch: 1on batch index: 34/35 ( 97.14285714285714% ))\n",
      "Training loss: 0.44068055050203414 --- Accuracy: 0.7954119850187266\n",
      "Validation loss: 0.35872606933116913 --- Accuracy: 0.8356039963669392\n",
      "\n",
      "Epoch: 2on batch index: 34/35 ( 97.14285714285714% ))\n",
      "Training loss: 0.27489311289921237 --- Accuracy: 0.8934925093632958\n",
      "Validation loss: 0.3748814071927752 --- Accuracy: 0.8501362397820164\n",
      "\n",
      "Epoch: 3on batch index: 34/35 ( 97.14285714285714% ))\n",
      "Training loss: 0.1621928099500999 --- Accuracy: 0.9400749063670412\n",
      "Validation loss: 0.4955283475773675 --- Accuracy: 0.8501362397820164\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "\n",
    "#Get the GPU device if it exists, load the SST-2 dataset, and create PyTorch datasets and dataloaders for the training and validation sets\n",
    "GPU  = get_gpu()\n",
    "sst2_dataset = load_dataset(\"sst\", \"default\")\n",
    "train_dataset = SST_Dataset(sst2_dataset['train'])\n",
    "valid_dataset = SST_Dataset(sst2_dataset['validation'])\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "  \n",
    "#Create the tokenizer, model, optimizer, and criterion\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = transfer_device(GPU, DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased'))\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "criterion = binary_cross_entropy\n",
    "  \n",
    "#Train and save the model\n",
    "model = train_model(GPU, train_dataloader, valid_dataloader, tokenizer, model, optimizer, criterion)\n",
    "smodel = str(model)\n",
    "torch.save({'tokenizer': tokenizer, 'model_state_dict': model.state_dict()}, smodel[:10]+\".pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19cbc43d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset sst (/home/rcorkil2/.cache/huggingface/datasets/sst/default/1.0.0/b8a7889ef01c5d3ae8c379b84cc4080f8aad3ac2bc538701cbe0ac6416fb76ff)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c92dda3d381a420583f410606e0049da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-training validation loss: 0.8877063035964966 --- Accuracy: 0.4950045413260672\n",
      "\n",
      "Epoch: 1on batch index: 34/35 ( 97.14285714285714% ))\n",
      "Training loss: 0.37751751239603376 --- Accuracy: 0.8383661048689138\n",
      "Validation loss: 0.32351926096848077 --- Accuracy: 0.8810172570390554\n",
      "\n",
      "Epoch: 2on batch index: 34/35 ( 97.14285714285714% ))\n",
      "Training loss: 0.2250990467739016 --- Accuracy: 0.9197097378277154\n",
      "Validation loss: 0.31253974565437864 --- Accuracy: 0.8819255222524978\n",
      "\n",
      "Epoch: 3on batch index: 34/35 ( 97.14285714285714% ))\n",
      "Training loss: 0.12709649103564605 --- Accuracy: 0.959503745318352\n",
      "Validation loss: 0.3854422716157777 --- Accuracy: 0.8773841961852861\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "#Get the GPU device if it exists, load the SST-2 dataset, and create PyTorch datasets and dataloaders for the training and validation sets\n",
    "GPU  = get_gpu()\n",
    "sst2_dataset = load_dataset(\"sst\", \"default\")\n",
    "train_dataset = SST_Dataset(sst2_dataset['train'])\n",
    "valid_dataset = SST_Dataset(sst2_dataset['validation'])\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "  \n",
    "#Create the tokenizer, model, optimizer, and criterion\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n",
    "model = transfer_device(GPU, BertForSequenceClassification.from_pretrained('bert-large-uncased'))\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "criterion = binary_cross_entropy\n",
    "  \n",
    "#Train and save the model\n",
    "model = train_model(GPU, train_dataloader, valid_dataloader, tokenizer, model, optimizer, criterion)\n",
    "smodel = str(model)\n",
    "torch.save({'tokenizer': tokenizer, 'model_state_dict': model.state_dict()}, smodel[:10]+\".pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e527a724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset sst (/home/rcorkil2/.cache/huggingface/datasets/sst/default/1.0.0/b8a7889ef01c5d3ae8c379b84cc4080f8aad3ac2bc538701cbe0ac6416fb76ff)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfdd5b91904f4c96b4c3353b5124b487",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['pooler.dense.weight', 'classifier.bias', 'classifier.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/rcorkil2/.conda/envs/base_env/lib/python3.7/site-packages/transformers/models/deberta/modeling_deberta.py:745: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  p2c_att = torch.matmul(key_layer, torch.tensor(pos_query_layer.transpose(-1, -2), dtype=key_layer.dtype))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-training validation loss: 0.7031167081424168 --- Accuracy: 0.49318801089918257\n",
      "\n",
      "Epoch: 1on batch index: 34/35 ( 97.14285714285714% ))\n",
      "Training loss: 0.4580510814538163 --- Accuracy: 0.7814840823970037\n",
      "Validation loss: 0.2872488098485129 --- Accuracy: 0.8864668483197093\n",
      "\n",
      "Epoch: 2on batch index: 34/35 ( 97.14285714285714% ))\n",
      "Training loss: 0.24913639965910145 --- Accuracy: 0.9039091760299626\n",
      "Validation loss: 0.28509851566382816 --- Accuracy: 0.8910081743869209\n",
      "\n",
      "Epoch: 3on batch index: 34/35 ( 97.14285714285714% ))\n",
      "Training loss: 0.15288463678587688 --- Accuracy: 0.9441713483146067\n",
      "Validation loss: 0.3488712762083326 --- Accuracy: 0.8837420526793823\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import DebertaConfig, DebertaModel, DebertaTokenizer, DebertaForSequenceClassification\n",
    "\n",
    "#Get the GPU device if it exists, load the SST-2 dataset, and create PyTorch datasets and dataloaders for the training and validation sets\n",
    "GPU  = get_gpu()\n",
    "sst2_dataset = load_dataset(\"sst\", \"default\")\n",
    "train_dataset = SST_Dataset(sst2_dataset['train'])\n",
    "valid_dataset = SST_Dataset(sst2_dataset['validation'])\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "  \n",
    "#Create the tokenizer, model, optimizer, and criterion\n",
    "tokenizer = DebertaTokenizer.from_pretrained(\"microsoft/deberta-base\")\n",
    "model = transfer_device(GPU, DebertaForSequenceClassification.from_pretrained(\"microsoft/deberta-base\"))\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "criterion = binary_cross_entropy\n",
    "  \n",
    "#Train and save the model\n",
    "model = train_model(GPU, train_dataloader, valid_dataloader, tokenizer, model, optimizer, criterion)\n",
    "smodel = str(model)\n",
    "torch.save({'tokenizer': tokenizer, 'model_state_dict': model.state_dict()}, smodel[:10]+\".pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4fd262",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
